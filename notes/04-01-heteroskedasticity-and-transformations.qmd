---
title: "Heteroskedasticity and Variance Stabilizing Transformations"
date: today
---

```{r}
#| echo: false
source("../assets/notes-setup.R")
```




In this set of notes, we will use data from Statistics Canada's *Survey of Labour and Income Dynamics* (SLID) available in **slid.csv** to explain variation in the hourly wage rate of employed citizens in Ontario.

<br />


# Data Exploration

As with any potential regression analysis, we will begin by importing the data and examining the scatterplot of each predictor with the outcome. These plots suggest that each of the predictors is related to the outcome.

```{r}
# Load libraries
library(broom)
library(car)
library(corrr)
library(patchwork)
library(tidyverse)

# Import data
slid = read_csv("https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/slid.csv")

# View data
slid
```



Here we examine the marginal density plots for the outcome and each continuous predictor, along with the scatterplots showing the relationship between each predictor and the outcome.

```{r}
#| label: fig-explore
#| fig-cap: "TOP: Density plots for the hourly wage, age, and education level attributes. BOTTOM: Scatterplot of hourly wage versus each predictor (age, education level, and male). The fitted regression line is also displayed in each plot."
#| fig-alt: "TOP: Density plots for the hourly wage, age, and education level attributes. BOTTOM: Scatterplot of hourly wage versus each predictor (age, education level, and male). The fitted regression line is also displayed in each plot."
#| fig-width: 12
#| fig-height: 8
#| out-width: '100%'


d1 = ggplot(data = slid, aes(x = wages)) +
  geom_density() +
  theme_bw() +
  xlab("Hourly wage rate")

d2 = ggplot(data = slid, aes(x = age)) +
  geom_density() +
  theme_bw() +
  xlab("Age (in years)")

d3 = ggplot(data = slid, aes(x = education)) +
  geom_density() +
  theme_bw() +
  xlab("Education (in years)")


p1 = ggplot(data = slid, aes(x = age, y = wages)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("Age (in years)") +
  ylab("Hourly wage rate")

p2 = ggplot(data = slid, aes(x = age, y = education)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("Education (in years)") +
  ylab("Hourly wage rate")


p3 = ggplot(data = slid, aes(x = male, y = education)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme_bw() +
  xlab("Male") +
  ylab("Hourly wage rate")

(d1 | d2 | d3) / (p1 | p2 | p3)
```

Based on what we see in these plots, the outcome (hourly wage) looks to be right-skewed. A skewed outcome may or may not be problematic. (It often leads to violations of the conditional normality or homoskedasticity assumption, although we cannot confirm until after we fit the model and examine the residuals.) The relationships between hourly wage and each of the three potential predictors seem linear. The plot with the age predictor, however, foreshadows that we might violate the homoskedasticity assumption (the variance of hourly wages seems to grow for higher ages), but we will withhold judgment until after we fit our multi-predictor model.

<br />


# Fitting a Multi-Predictor Model

Next, we fit a model regressing wages on the three predictors simultaneously and examine the residual plots. Because we will be looking at residual plots for many different fitted models, we will write and then use a function that creates these plots.

```{r}
# Function to create residual plots
residual_plots = function(object){
  # Get residuals and fitted values
  aug_lm = broom::augment(object, se_fit = TRUE)

  # Create residual plot
  p1 = ggplot(data = aug_lm, aes(x =.resid)) +
    educate::stat_density_confidence(model = "normal") +
    geom_density() +
    theme_light() +
    xlab("Residuals") +
    ylab("Probability Density")

  # Create residual plot
  p2 = ggplot(data = aug_lm, aes(x =.fitted, y = .resid)) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    geom_point(alpha = 0.1) +
    geom_smooth(method = "lm", se = TRUE) +
    geom_smooth(method = "loess", se = FALSE, n = 50, span = 0.67) +
    theme_light() +
    xlab("Fitted values") +
    ylab("Residuals")


  return(p1 | p2)
}
```

<br />


:::fyi
**FYI**

Note that the `{educate}` package is not available from CRAN, and only available via GitHub. To install this package use the `install_github()` function from the `{remotes}` package to install it. The full syntax is: `remotes::install_github("zief0002/educate")`.
:::

<br />



Now we can use our new function to examine the residual plots from the main-effects model.


```{r}
#| label: fig-resid
#| fig-cap: "Residual plots for the model that includes the main effects of age, education level, and sex."
#| fig-alt: "Residual plots for the model that includes the main effects of age, education level, and sex."
#| fig-width: 8
#| fig-height: 4
#| out-width: '100%'

# Fit model
lm.1 = lm(wages ~ 1 + age + education + male, data = slid)

# Examine residual plots
residual_plots(lm.1)
```


Examining the residual plots:

- The linearity assumption may be violated; the loess line suggests some nonlinearity (maybe due to omitted interaction/polynomial terms)
- The normality assumption may be violated; the upper end of the distribution deviates from what would be expected from a normal distribution in the QQ-plot.
- The homoskedasticity assumption is likely violated; the plot of studentized residuals versus the fitted values shows severe fanning; the variation in residuals seems to increase for higher fitted values.


Because of the nonlinearity, we might consider including interaction terms. The most obvious interaction is that between age and education level, as it seems like the effect of age on hourly wage might be moderated by education level. (Remember, do NOT include interactions unless they make theoretical sense!) Below we fit this model, still controlling for sex, and examine the residuals.

```{r}
#| label: fig-resid-int
#| fig-cap: "Residual plots for the model that includes an interaction effect between age and education level."
#| fig-alt: "Residual plots for the model that includes an interaction effect between age and education level."
#| fig-width: 8
#| fig-height: 4
#| out-width: '80%'

# Fit model
lm.2 = lm(wages ~ 1 + age + education + male + age:education, data = slid)

# Examine residual plots
residual_plots(lm.2)
```

Including the age by education interaction term (`age:education`) seems to alleviate the nonlinearity issue, but the residual plots indicate there still may be violations of the normality and homoskedasticity assumptions. Violating normality is less problematic here since, given our sample size, the Central Limit Theorem will ensure that the inferences are still approximately valid. Violating homoskedasticity, on the other hand, is more problematic.

<br />


## Violating Homoskedasticity

Violating the distributional assumption of homoskedasticity results in:

- Incorrect computation of the sampling variances and covariances; and because of this
- The OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).

This means that the SEs (and resulting *t*- and *p*-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes).


<br />


# Heteroskedasticity: What is it and How do we Deal with It?

Recall that the variance--covariance matrix for the residuals under the asssumption of homoskedasticity was:

$$
\boldsymbol{\sigma^2}(\boldsymbol{\epsilon}) =  \begin{bmatrix}\sigma^2_{\epsilon} & 0 & 0 & \ldots & 0 \\ 0 & \sigma^2_{\epsilon} & 0 & \ldots & 0\\ 0 & 0 & \sigma^2_{\epsilon} & \ldots & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \ldots & \sigma^2_{\epsilon}\end{bmatrix}
$$

Homoskedasticity implies that the variance for each residual was identical, namely $\sigma^2_{\epsilon}$. Since the variance estimate for each residual was the same, we could estimate a single value for these variances, the residual variance, and use that to obtain the sampling variances and covariances for the coefficients:

$$
\boldsymbol{\sigma^2_B} = \sigma^2_{\epsilon} (\mathbf{X}^{\prime}\mathbf{X})^{-1}
$$


Heteroskedasticy implies that the residual variances are not constant. We can represent the variance--covariance matrix of the residuals under heteroskedasticity as:

$$
\boldsymbol{\sigma^2}(\boldsymbol{\epsilon}) =  \begin{bmatrix}\sigma^2_{1} & 0 & 0 & \ldots & 0 \\ 0 & \sigma^2_{2} & 0 & \ldots & 0\\ 0 & 0 & \sigma^2_{3} & \ldots & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \ldots & \sigma^2_{n}\end{bmatrix}
$$

In this matrix, each residual has a potentially different variance. Now, there is more than one residual variance, and estimating these variance becomes more complicated, as does estimating the sampling variances and covariances of the regression coefficients.

There are at least three primary methods for dealing with heteroskedasticity: (1) transform the *y*-values using a variance stabilizing transformation; (2) fit the model using weighted least squares rather than OLS; or (3) adjust the SEs and covariances to account for the non-constant variances. We will examine each of these in turn.

<br />


# Variance Stabilizing Transformations

The idea behind using a variance stabilizing transformation on the outcome (**y**) is that the transformed *y*-values will be homoskedastic. If so, we can fit the OLS regression model using the transformed *y*-values; the inferences will be valid; and, if necessary, we can back-transform for better interpretations. There are several transformations that can be applied to **y** that might stabilize the variances. Two common transformations are:

- Log-transformation; $\ln(Y)_i$
- Square-root transformation; $\sqrt{Y_i}$


:::fyi
**FYI**

Prior to applying these transformations, you may need to add a constant value to each *y*-value so that all $y_i>0$ (log-transformation) or all $y_i \geq 0$ (square-root transformation).
:::


Both of these transformations are *power transformations*. Power transformations have the mathematical form:

$$
y^*_i = y_i^{p}
$$

where $y^*_i$ is the transformed *y*-value, $y_i$ is the original *y*-value, and *p* is an integer. The following are all power transformations of **y**:

$$
\begin{split}
& ~\vdots \\[0.5em]
& Y^4 \\[0.5em]
& Y^3 \\[0.5em]
& Y^2 \\[1.5em]
& Y^1 = Y \\[1.5em]
& Y^{0.5} = \sqrt{Y} \\[0.5em]
& Y^0 \equiv \ln(Y) \\[0.5em]
& Y^{-1} = \frac{1}{Y} \\[0.5em]
& Y^{-2} = \frac{1}{Y^2} \\[0.5em]
& ~\vdots
\end{split}
$$

Powers such that $p<1$ are referred to as downward transformations, and those with $p>1$ are referred to as upward transformations. Both the log-transformation and square-root transformation are downward transformations of **y**. Here we will fit the main effects model using the square-root transformation and the log-transformation of the hourly wage values.

```{r}
# Create transformed y-values
slid = slid |>
  mutate(
    sqrt_wages = sqrt(wages),
    ln_wages = log(wages)
  )

# Fit models
lm_sqrt = lm(sqrt_wages ~ 1 + age + education + male, data = slid)
lm_ln = lm(ln_wages ~ 1 + age + education + male, data = slid)
```


The plots below show the residuals based on fitting a model with each of these transformations applied to the `wages` data.


```{r}
#| label: fig-resid-power
#| fig-cap: "TOP: Residual plots for the main effects model that used a square root transformation on **y**. BOTTOM: Residual plots for the main effects model that used a logarithmic transformation on **y**."
#| fig-alt: "TOP: Residual plots for the main effects model that used a square root transformation on **y**. BOTTOM: Residual plots for the main effects model that used a logarithmic transformation on **y**."
#| fig-width: 8
#| fig-height: 8
#| out-width: '100%'

# Examine residual plots
residual_plots(lm_sqrt) / residual_plots(lm_ln)
```

Both of these residual plots seem to show less heterogeneity than the residuals from the model with untransformed wages. However, neither transformation, $p=0$ nor $p=0.5$, seems to have "fixed" the problem completely.

<br />


## Box-Cox Transformation

Is there a power transformation that would better "fix" the heteroskedasticity? In their seminal paper, @Box:1964 proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:

$$
Y^{(\lambda)}_i = \beta_0 + \beta_1(X1_{i}) + \beta_2(X2_{i}) + \ldots + \beta_k(Xk_{i}) + \epsilon_i
$$

where the errors are independent and $\mathcal{N}(0,\sigma^2_{\epsilon})$, and

$$
Y^{(\lambda)}_i = \begin{cases}
   \frac{Y_i^{\lambda}-1}{\lambda} & \text{for } \lambda \neq 0 \\[1em]
   \ln(Y_i)       & \text{for } \lambda = 0
  \end{cases}
$$

This transformation is only defined for positive values of *Y*.

The `powerTransform()` function from the `{car}` library can be used to determine the optimal value of $\lambda$.

```{r}
# Find optimal power transformation using Box-Cox
powerTransform(lm.1)
```

The output from the `powerTransform()` function gives the optimal power for the transformation of **y**, namely $\lambda = 0.086$. To actually implement the power transformation we use the transform *Y* based on the Box-Cox algorithm presented earlier.

```{r}
slid = slid |>
  mutate(
    bc_wages = (wages ^ 0.086 - 1) / 0.086
  )

# Fit models
lm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)
```

The residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.

```{r}
#| label: fig-resid-boxcox
#| fig-cap: "Residual plots for the main effects model that used a Box-Cox transformation on *Y* with $\\lambda=0.086$."
#| fig-alt: "Residual plots for the main effects model that used a Box-Cox transformation on *Y* with $\\lambda=0.086$."
#| fig-width: 8
#| fig-height: 4
#| out-width: '100%'

# Examine residual plots
residual_plots(lm_bc)
```

One problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficient-level output:

```{r}
# Coeffifient-level output
tidy(lm_bc, conf.int = TRUE)
```

The age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed *Y*, controlling for differences in education and sex. But what does a 0.227-unit difference in transformed *Y* mean when we translate that back to wages?

<br />


## Profile Plot for Different Transformations

Most of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when $\lambda=0$. This is the log-transformation which is directly interpretable. Since the optimal $\lambda$ value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation ($\lambda=0$). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.


To evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a *profile plot* of the log-likelihood. The `boxCox()` function creates a profile plot of the log-likelihood for a defined sequence of $\lambda$ values. Here we will plot the profile of the log-likelihood for $-2 \leq \lambda \leq 2$.


```{r}
#| label: fig-likelihood-profile
#| fig-cap: "Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value."
#| fig-alt: "Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value."

# Plot of the log-likelihood for a given model versus a sequence of lambda values
boxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))
```

The profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of $\lambda$ values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the `boxCox()` function, we may need to zoom in to determine these limits by tweaking the sequence of $\lambda$ values in the `boxCox()` function.

```{r}
#| label: fig-lik-prof2
#| fig-cap: "Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values."
#| fig-alt: "Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values."

# Zoom in on confidence limits
boxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))
```


It looks as though $.03 \leq \lambda \leq 0.14$ all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the $\lambda$ value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use $\lambda=0.086$ versus $\lambda=0$. The only way to evaluate this is to fit the models and check the residuals.


<br />





