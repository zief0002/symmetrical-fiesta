[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Repository for EPSY 8264",
    "section": "",
    "text": "Syllabus\nTextbook: Matrix Algebra for Educational Scientists\nCSV Data Files\nData Dictionaries\nR Scripts"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Repository for EPSY 8264",
    "section": "",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#notes",
    "href": "index.html#notes",
    "title": "Repository for EPSY 8264",
    "section": "Notes",
    "text": "Notes\n\n\nMathematical and Computational Foundations\n\nSummation, Expectation, Variance, Covariance, and Correlation\nOLS Estimators and Their Properties\nAssumptions for OLS Regression and the Gauss-Markov Theorem\nStatistical Inference for the Regression Model\nA Regression Example in Practice\nRegression From Summary Measures\n\nPath Analysis\n\nIntroduction to Path Analysis\nPath Analysis Using R\n\nRegression Diagnostics\n\nRegression Diagnostics\n\nTools for Dealing with Heteroskedasticity\n\nHeteroskedasticity and Variance Stabilizing Transformations\nWeighted Least Squares (WLS) and Sandwich Estimation\n\nDiagnosing Collinearity and Tools for Dealing With It\n\nDiagnosing Collinearity\nPrincipal Components Analysis via Spectral Decomposition\nPrincipal Components Analysis via Singular Value Decomposition\nBiased Estimation: Ridge Regression\n\nModel Selection\n\nAutomated Methods for Model Selection\nCross-Validation Methods for Model Selection"
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Repository for EPSY 8264",
    "section": "Assignments",
    "text": "Assignments\n\n\nAssignment 1: Matrix Algebra for Linear Regression\nAssignment 2: Path Analysis \nAssignment 3: Using WLS to Model Data with Outliers\nAssignment 4: Collinearity and Dimension Reduction\nAssignment 5: Ridge Regression\nAssignment 6: Cross Validation"
  },
  {
    "objectID": "index.html#worksheets",
    "href": "index.html#worksheets",
    "title": "Repository for EPSY 8264",
    "section": "Worksheets",
    "text": "Worksheets\n\n\nIntroduction to Matrix Algebra\nModel Selection"
  },
  {
    "objectID": "worksheets/01-introduction-to-matrix-algebra.html",
    "href": "worksheets/01-introduction-to-matrix-algebra.html",
    "title": "Introduction to Matrix Algebra",
    "section": "",
    "text": "Directions\nComplete the problems on this worksheet with your small group. You may want to refer to the Matrix Algebra for Educational Scientists text.\nYou will likely be learning (or re-encountering) many new mathematical terms. It is a good idea to note and define all the vocabulary/terms that you encounter as you work through this worksheet. You may want to do this individually or create a shared document that you can all contribute to.\n\n\n\nProblems\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & -2\\\\5 & 1\\end{bmatrix} \\quad \\mathbf{B} = \\begin{bmatrix}3 & -1\\\\-1 & 2\\end{bmatrix}\\quad \\mathbf{C} = \\begin{bmatrix}1 & 2 & 3\\\\0 & 1 & 2\\end{bmatrix}\n\\]\nMake sure everyone in your group can solve each of these problem by hand and using R.\n\nWhat are the dimensions of A? C?\nIs C a square matrix? Explain.\nFind the trace of A.\nFind the determinant of A.\nAdd A and B\nFind the transpose of C.\nBy referring to the dimensions, can you compute AC? How about CA?\nCompute AC.\nCompute BI\nCreate a \\(3\\times3\\) diagonal matrix whose trace is 10.\nHow do you know that B has an inverse? Explain.\nCompute \\(\\mathbf{B}^{-1}\\)\nCreate a \\(3\\times3\\) matrix that has rank 2. Verify this using R.\nCreate a \\(3\\times3\\) matrix that is symmetric and is not I.\nSolve the system of linear equations using algebra (e.g., substitution, elimination) and then solve them using matrix methods (with R). To do this you will need to read the Systems of Equations chapter in Matrix Algebra for Educational Scientists.\n\n\\[\n\\begin{split}\nx + y + z &= 2 \\\\\n6x - 4y + 5z &= 31 \\\\\n5x + 2y + 2z &= 13\n\\end{split}\n\\]"
  },
  {
    "objectID": "worksheets/02-model-selection.html",
    "href": "worksheets/02-model-selection.html",
    "title": "Model Selection (In-Class Activity)",
    "section": "",
    "text": "The data in states-2019.csv include statistics collected from Wikipedia, the 2019 American Community Survey, and the National Centers for Environmental Information.\n\nYour goal is to use the data (and everything you’ve learned so far in your coursework) to create a model to predict variation in life expectancy. Keep track of the process you use to create this model, including:\n\nWhich predictors should be included in the model?\n\nWhat is the criteria/evidence you are using to make these decisions?\n\nWhen in the process do you identify problematic observations?\n\nDo you remove those problematic observations or not?\nWhat criteria/evidence are you using to make these decisions?\n\nWhen in the process do you examine the model for collinearity?\n\nWhat is the criteria/evidence you are using to make this decision?\nWhat (if anything) will you do to fix this?\n\nWhen in the process do you examine the tenability of assumptions?\n\nAlso pay attention to when in the process you are making decisions based on sample evidence (graphs/statistics) versus when those decisions are being made using statistical inference (hypothesis tests, confidence intervals). Your group will be asked to report back to the class on the process, criteria, and evidence you used."
  },
  {
    "objectID": "notes/01-ols-regression.html",
    "href": "notes/01-ols-regression.html",
    "title": "OLS Regression Using Matrices and Its Properties",
    "section": "",
    "text": "Here are links to several PDF handouts:\n\n\nSummation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this.\n\n\nThe handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest."
  },
  {
    "objectID": "index.html#handouts",
    "href": "index.html#handouts",
    "title": "Repository for EPSY 8264",
    "section": "",
    "text": "Here are links to several PDF handouts. The handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest.\n\n\nSummation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this."
  },
  {
    "objectID": "notes/02-regression-from-summary-measures.html",
    "href": "notes/02-regression-from-summary-measures.html",
    "title": "Regression from Summary Measures",
    "section": "",
    "text": "When we have raw data, we can fit a regression model using the lm() function and obtain model- and coefficient-level summaries using the glance() and tidy() functions respectively. However, there are times we might want to compute regression coefficients and standard errors, but are not given the raw data. This is common for example in articles, which often report summaries rather than providing raw data. In this set of notes, we will examine how we can fit a regression model to summaries of the data, rather than to the raw data itself.\nSo long as we have certain statistical information we can compute the regression coefficients and standard errors without having the raw data. To do this, we need the sample size, means, standard deviations, and correlations between variables. For example, consider the following summary table:\nTable 1: Correlation matrix for five attributes measured on n = 1,000 students. Means (standard deviations) for each attribute are provided on the main diagonal.\n\n\n\n\n\n\n\nCorrelation matrix for five attributes measured on *n* = 1,000 students. Means (standard deviations) for each attribute are provided on the main diagonal.\n\n\nMeasure\n1.\n2.\n3.\n4.\n5.\n\n\n\n\n1. Achievement\n50 (10)\n---\n---\n---\n---\n\n\n2. Ability\n0.737\n100 (15)\n---\n---\n---\n\n\n3. Motivation\n0.255\n0.205\n50 (10)\n---\n---\n\n\n4. Previous Coursework\n0.615\n0.498\n0.375\n4 (2)\n---\n\n\n5. Family Background\n0.417\n0.417\n0.190\n0.372\n0 (1)\nSay we wanted to use this information to fit a regression model to predict variation in achievement using the other four predictors. Mathematically,\n\\[\n\\begin{split}\n\\mathrm{Achievement}_i = &\\beta_0 + \\beta_1(\\mathrm{Ability}_i) + \\beta_2(\\mathrm{Motivation}_i) + \\\\\n&\\beta_3(\\mathrm{Coursework~}_i) + \\beta_4(\\mathrm{Family~Background}_i) + \\epsilon_i\n\\end{split}\n\\]\nTo do this we are going to create the correlation matrix, the vector of attribute means, and the vector of attribute standard deviations.\n# Create correlation matrix\ncorrs = matrix(\n  data = c(\n    1.000, 0.737, 0.255, 0.615, 0.417,\n    0.737, 1.000, 0.205, 0.498, 0.417,\n    0.255, 0.205, 1.000, 0.375, 0.190,\n    0.615, 0.498, 0.375, 1.000, 0.372,\n    0.417, 0.417, 0.190, 0.372, 1.000\n  ),\n  nrow = 5\n)\n\n# View correlation matrix\ncorrs\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 1.000 0.737 0.255 0.615 0.417\n[2,] 0.737 1.000 0.205 0.498 0.417\n[3,] 0.255 0.205 1.000 0.375 0.190\n[4,] 0.615 0.498 0.375 1.000 0.372\n[5,] 0.417 0.417 0.190 0.372 1.000\n\n# Create mean vector\nmeans = c(50, 100, 50, 4, 0)\n\n# Create sd vector\nsds = c(10, 15, 10, 2, 1)\n\n# Set sample size\nn = 1000"
  },
  {
    "objectID": "notes/02-regression-from-summary-measures.html#going-from-the-correlations-to-covariances",
    "href": "notes/02-regression-from-summary-measures.html#going-from-the-correlations-to-covariances",
    "title": "Regression from Summary Measures",
    "section": "Going From the Correlations to Covariances",
    "text": "Going From the Correlations to Covariances\nThe summary measures we have give correlations, not covariances. However, it is quite easy to convert a correlation matrix to a covariance matrix. From the chapter Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices in Matrix Algebra for Educational Scientists, know that:\n\\[\n\\mathbf{R} = \\mathbf{S} \\boldsymbol\\Sigma \\mathbf{S}\n\\]\nwhere R is the correlation matrix, \\(\\boldsymbol\\Sigma\\) is the covariance matrix, and S is a diagonal scaling matrix with diagonal elements equal to the reciprocal of the standard deviations of each of the variables in the covariance matrix. Re-arranging this:\n\\[\n\\boldsymbol\\Sigma = \\mathbf{S}^{-1} \\mathbf{R} \\mathbf{S}^{-1}\n\\]\nRecall that the inverse of a diagonal matrix is another diagonal matrix where the diagonal elements are the resciprocals of the original. Thus the diagonal elements of the inverse of our scaling matrix will be the standard deviations of the variables. Using this formula, we can now convert the given correlation matrix to a covariance matrix.\n\n# Compute the scaling matrix S^(-1)\nS_inv = diag(sds)\nS_inv\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    0    0    0    0\n[2,]    0   15    0    0    0\n[3,]    0    0   10    0    0\n[4,]    0    0    0    2    0\n[5,]    0    0    0    0    1\n\n# Compute covariance matrix\ncovs = S_inv %*% corrs %*% S_inv\ncovs\n\n       [,1]    [,2]   [,3]   [,4]  [,5]\n[1,] 100.00 110.550  25.50 12.300 4.170\n[2,] 110.55 225.000  30.75 14.940 6.255\n[3,]  25.50  30.750 100.00  7.500 1.900\n[4,]  12.30  14.940   7.50  4.000 0.744\n[5,]   4.17   6.255   1.90  0.744 1.000\n\n\nAdding in the row and column names:\n\n\n\n\nTable 2: Variance-covariance matrix for five attributes measured on n = 1,000 students. The blue elements compose Cov(X, y) and the red elements constitute Cov(X, X).\n\n\n\n\n\n\n\nVariance-covariance matrix for five attributes measured on *n* = 1,000 students. The blue elements compose Cov(X, y) and the red elements constitute Cov(X, X).\n\n\nAchievement\nAbility\nMotivation\nPrevious Coursework\nFamily Background\n\n\n\n\n100.00\n110.550\n25.50\n12.300\n4.170\n\n\n110.55\n225.000\n30.75\n14.940\n6.255\n\n\n25.50\n30.750\n100.00\n7.500\n1.900\n\n\n12.30\n14.940\n7.50\n4.000\n0.744\n\n\n4.17\n6.255\n1.90\n0.744\n1.000\n\n\n\n\n\n\n\n\n\n\n\nRemember, the diagonal elements in the covariance matrix are variances of each variable and the off-diagonal elements are the covariances between two variables. For example, the first diagonal element is 100, which is the variance of the achievement variable. The remaining elements in the first column indicate the covariances between achievement and ability, achievement and motivation, achievement and previous coursework, and achievement and family background."
  },
  {
    "objectID": "notes/02-regression-from-summary-measures.html#finding-the-predictor-coefficients",
    "href": "notes/02-regression-from-summary-measures.html#finding-the-predictor-coefficients",
    "title": "Regression from Summary Measures",
    "section": "Finding the Predictor Coefficients",
    "text": "Finding the Predictor Coefficients\nTo compute the coefficients for the predictors, we need to obtain two things:\n\nCov(X, X): The variance-covariance matrix of the predictors, and\nCov(X, y): The vector that contains the covariances between the outcome and each predictor. (Note: This vector does not include the variance of the outcome, only the covariances with the predictors.)\n\nIn our example, the elements in the first column (or row) of the variance-covariance matrix other than the variance of the outcome are the values in the vector Cov(X, y). The elements in the other rows/columns make up the elements of the Cov(X, X) matrix. In Table 2 the blue elements compose Cov(X, y) and the red elements constitute Cov(X, X). We can create this vector and matrix using indexing.\n\n# Create Cov(X, y)\ncov_xy = covs[-1 , 1]\ncov_xy\n\n[1] 110.55  25.50  12.30   4.17\n\n# Create Cov(X, X)\ncov_xx = covs[-1 , -1]\ncov_xx\n\n        [,1]   [,2]   [,3]  [,4]\n[1,] 225.000  30.75 14.940 6.255\n[2,]  30.750 100.00  7.500 1.900\n[3,]  14.940   7.50  4.000 0.744\n[4,]   6.255   1.90  0.744 1.000\n\n\nThen we can use these to compute the predictor coefficients.\n\n# Compute predictor coefficients\nb = solve(cov_xx) %*% cov_xy\nb\n\n           [,1]\n[1,] 0.36737330\n[2,] 0.01257721\n[3,] 1.55001342\n[4,] 0.69497334\n\n\nThus the coefficient estimates are:\n\n\\(\\hat\\beta_{\\mathrm{Ability}} = 0.367\\)\n\\(\\hat\\beta_{\\mathrm{Motivation}} = 0.013\\)\n\\(\\hat\\beta_{\\mathrm{Previous~Coursework}} = 1.550\\)\n\\(\\hat\\beta_{\\mathrm{Family~Background}} = 0.695\\)"
  },
  {
    "objectID": "notes/02-regression-from-summary-measures.html#computing-the-intercept",
    "href": "notes/02-regression-from-summary-measures.html#computing-the-intercept",
    "title": "Regression from Summary Measures",
    "section": "Computing the Intercept",
    "text": "Computing the Intercept\nTo compute the intercept, we can use the following:\n\\[\n\\hat\\beta_0 = \\bar{y} - \\bar{\\mathbf{x}}^\\intercal\\mathbf{b}\n\\]\nwhere \\(\\bar{y}\\) is the mean of the outcome, \\(\\bar{\\mathbf{x}}\\) is the vector of predictor means, and b is the associated vector of predictor coefficients.\nIn our example,\n\n# Compute intercept\nb_0 = means[1] - t(means[2:5]) %*% b\nb_0\n\n         [,1]\n[1,] 6.433756\n\n\nThus the fitted equation for the unstandardized model is:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Achievement}_i} = &6.434 + 0.367(\\mathrm{Ability}_i) + 0.013(\\mathrm{Motivation}_i) + \\\\\n&1.550(\\mathrm{Coursework~}_i) + 0.695(\\mathrm{Family~Background}_i)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/02-regression-from-summary-measures.html#compute-mathbfxintercalmathbfx-matrix",
    "href": "notes/02-regression-from-summary-measures.html#compute-mathbfxintercalmathbfx-matrix",
    "title": "Regression from Summary Measures",
    "section": "Compute \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix",
    "text": "Compute \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix\nThe elements in the first row and column of the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix are functions of the sample size and means of the predictor variables. Namely the first element in the first row and column is n and the remaining elements in the that row and column are created as \\(n\\mathbf{M_x}\\), where n is the sample size, and \\(\\mathbf{M_x}\\) is the vector of predictor means. The remaining elements constitute a submatrix defined as:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}} = (n-1)\\mathrm{Cov}(X,X) + n(\\mathbf{M_x})(\\mathbf{M_x}^\\intercal)\n\\]\nwhere n is the sample size, Cov(X,X) is the variance covariance matrix ofthe predictors, and \\(\\mathbf{M_x}\\) is again the vector of predictor means. We cn then create the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix as:\n\\[\n\\require{color}\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{X} &= \\begin{bmatrix}{\\color[rgb]{0.044147,0.363972,0.636955}n} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\mathbf{M_x}}\\\\{\\color[rgb]{0.044147,0.363972,0.636955}n\\mathbf{M_x}} & {\\color[rgb]{0.9058824, 0.1137255, 0.2117647}(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}}\\end{bmatrix}\n\\\\[2ex]\n&= \\begin{bmatrix}{\\color[rgb]{0.044147,0.363972,0.636955}n} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X1}} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X2}} &  {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X3}} & {\\color[rgb]{0.044147,0.363972,0.636955}\\ldots} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{Xk}}\\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X1}} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X2}} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X3}} & & & {\\color[rgb]{0.9058824, 0.1137255, 0.2117647}(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}} & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}\\vdots} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{Xk}} & & & & &   \\end{bmatrix}\n\\end{split}\n\\]\nTo create this matrix using R, we will:\n\nCompute the submatrix \\((\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}\\).\nBind the \\(n\\mathbf{M_x}\\) vector to the top of the submatrix.\nBind the vector that contains n and \\(n\\mathbf{M_x}\\) to the left of the resulting matrix from Step 2.\n\nThe resulting matrix is the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. For our example,\n\n# Step 1: Create submatrix\nsub_mat = 999 * cov_xx + 1000 * means[2:5] %*% t(means[2:5])\nsub_mat\n\n             [,1]      [,2]       [,3]     [,4]\n[1,] 10224775.000 5030719.2 414925.060 6248.745\n[2,]  5030719.250 2599900.0 207492.500 1898.100\n[3,]   414925.060  207492.5  19996.000  743.256\n[4,]     6248.745    1898.1    743.256  999.000\n\n# Step 2: Bind n(M_x) to top of submatrix\nmat_2 = rbind(1000 * means[2:5], sub_mat)\nmat_2\n\n             [,1]      [,2]       [,3]     [,4]\n[1,]   100000.000   50000.0   4000.000    0.000\n[2,] 10224775.000 5030719.2 414925.060 6248.745\n[3,]  5030719.250 2599900.0 207492.500 1898.100\n[4,]   414925.060  207492.5  19996.000  743.256\n[5,]     6248.745    1898.1    743.256  999.000\n\n# Step 3: Bind vector to left of Step 2 matrix\nXtX = cbind(c(1000, 1000 * means[2:5]), mat_2)\nXtX\n\n       [,1]         [,2]      [,3]       [,4]     [,5]\n[1,]   1000   100000.000   50000.0   4000.000    0.000\n[2,] 100000 10224775.000 5030719.2 414925.060 6248.745\n[3,]  50000  5030719.250 2599900.0 207492.500 1898.100\n[4,]   4000   414925.060  207492.5  19996.000  743.256\n[5,]      0     6248.745    1898.1    743.256  999.000\n\n\nWe can then scale this matrix using our previous estimate of the residual variance to obtain the variance-covariance matrix for the coefficients and compute the coefficient standard errors.\n\n# Compute var-cov matrix of b\ncov_b = s2_e * solve(XtX)\ncov_b\n\n            [,1]            [,2]            [,3]         [,4]          [,5]\n[1,]  2.86183238 -0.021078176856 -0.018522600965  0.052341868  0.1280945885\n[2,] -0.02107818  0.000240314903 -0.000001964506 -0.000713772 -0.0009683908\n[3,] -0.01852260 -0.000001964506  0.000435428791 -0.000763097 -0.0002472826\n[4,]  0.05234187 -0.000713772031 -0.000763096999  0.014297546 -0.0047228461\n[5,]  0.12809459 -0.000968390764 -0.000247282552 -0.004722846  0.0473303248\n\n# Compute SEs\nse = sqrt(diag(cov_b))\nse\n\n[1] 1.69169512 0.01550209 0.02086693 0.11957235 0.21755534\n\n\nThese are the standard errors for the intercept and each predictor. Here I add the coefficients and standard errors to a data frame and use them to compute t-values, associated p-values, and confidence intervals.\n\n# Load library\nlibrary(tidyverse)\n\n# Create regression table\ndata.frame(\n  Predictor = c(\"Intercept\", \"Ability\", \"Motivation\", \"Previous Coursework\", \"Family Background\"),\n  B = c(b_0, b),\n  SE = se\n) |&gt;\n  mutate(\n    t = round(B /SE, 3),\n    p = round(2 * pt(-abs(t), df = 995), 5),\n    CI = paste0(\"(\", round(B + qt(.025, df = 995)*SE, 3), \", \", round(B + qt(.975, df = 995)*SE, 3), \")\")\n  )\n\n            Predictor          B         SE      t       p              CI\n1           Intercept 6.43375597 1.69169512  3.803 0.00015  (3.114, 9.753)\n2             Ability 0.36737330 0.01550209 23.698 0.00000  (0.337, 0.398)\n3          Motivation 0.01257721 0.02086693  0.603 0.54665 (-0.028, 0.054)\n4 Previous Coursework 1.55001342 0.11957235 12.963 0.00000  (1.315, 1.785)\n5   Family Background 0.69497334 0.21755534  3.194 0.00145  (0.268, 1.122)"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html",
    "href": "files/01-some-mathematics-relevant-to-regression.html",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "",
    "text": "Assume the \\(X\\) and \\(Y\\) are random variables and \\(c\\) is a constant, such that:\n\\[\n\\begin{split}\nX &= \\{x_1, x_2, x_3, \\ldots, x_n\\} \\\\\nY &= \\{y_1, y_2, y_3, \\ldots, y_n\\} \\\\\nc &= \\{c_1, c_2, c_3, \\ldots, c_n\\} \\qquad \\mathrm{where~} c_1= c_2= c_3= \\ldots= c_n\\ \\\\\n\\end{split}\n\\]\nThe mean of these random (and constant) variables is denoted as the expected value, namely, \\(\\mathbb{E}(X)\\), \\(\\mathbb{E}(Y)\\), and \\(\\mathbb{E}(c)\\)."
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#formula-for-variance",
    "href": "files/01-some-mathematics-relevant-to-regression.html#formula-for-variance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Variance",
    "text": "Formula for Variance\nOne very useful measure that we will work with a lot in the course is the variance. Here are several formulas to compute the variance of a random variable, \\(X\\). We denote the variance of \\(X\\) using \\(\\sigma^2_X\\) or \\(\\mathrm{Var}(X)\\). The most common formula for variance is:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)^2}{n}\n\\]\nWe can also compute variance as an expected value of the squared mean deviations:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nLastly, it can sometimes be helpful to express the variance as the difference between the expected value of \\(X^2\\) and the squared expected value of \\(X\\):\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}(X^2) -  \\big[\\mathbb{E}(X)\\big]^2\n\\]\nLastly, we note that the standard deviation is the square root of the variance:\n\\[\n\\sigma_X = \\sqrt{\\sigma^2_X} = \\sqrt{\\mathrm{Var}(X)} = \\mathrm{SD}(X)\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#formula-for-covariance",
    "href": "files/01-some-mathematics-relevant-to-regression.html#formula-for-covariance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Covariance",
    "text": "Formula for Covariance\nAnother useful measure that we will be working with in the course is the covariance. We denote the covariance between \\(X\\) and \\(Y\\) using \\(\\sigma_{XY}\\) or \\(\\mathrm{Cov}(X,Y)\\). The most common formula for covariance is:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)\\bigg(Y_i - \\mathbb{E}(Y)\\bigg)}{n}\n\\]\nThe covariance can also be expressed as an expectation:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}\\bigg(\\big[X - \\mathbb{E}(X)\\big]\\big[Y - \\mathbb{E}(Y)\\big]\\bigg)\n\\]\nLastly, we can also express the covariance as a difference of expectations.\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#formula-for-correlation-coefficient",
    "href": "files/01-some-mathematics-relevant-to-regression.html#formula-for-correlation-coefficient",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Correlation Coefficient",
    "text": "Formula for Correlation Coefficient\nThe correlation coefficient is a standardized covariance value. We denote the correlation between \\(X\\) and \\(Y\\) using \\(\\rho_{XY}\\) or \\(\\mathrm{Cor}(X,Y)\\). The most common formula for correlation is:\n\\[\n\\rho_{XY} = \\mathrm{Cor}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}}\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-sums",
    "href": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-sums",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Sums",
    "text": "Rules for Working with Sums\nThe sum of \\(X\\) is defined as,\n\\[\n\\sum_{i=1}^n X_i = x_1 + x_2 + x_3 + \\ldots + x_n\n\\]\nTo keep the notation simpler, we will just denote this as \\(\\sum X\\).\nRule 1: When a summation is itself a sum or difference, the summation sign may be distributed among the separate terms of the sum. That is:\n\\[\n\\sum(X + Y) = \\sum X + \\sum Y\n\\]\nRule 2: The sum of a constant, \\(c\\), is \\(n\\) times the value of the constant.\n\\[\n\\sum(c) = nc\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-expectations-means",
    "href": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-expectations-means",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Expectations (Means)",
    "text": "Rules for Working with Expectations (Means)\nThe expectation (mean) of \\(X\\) is defined as,\n\\[\n\\mathbb{E}(X) = \\frac{\\sum_{i=1}^n X_i}{n}\n\\]\nAgain, to keep the notation simpler, we will just denote this as \\(\\mathbb{E}(X) = \\frac{\\sum X}{n}\\).\nRule 1: The expectation of a constant, \\(c\\), is the constant.\n\\[\n\\mathbb{E}(c) = c\n\\]\nRule 2: Adding a constant value, \\(c\\), to each term in a random variable, \\(X\\), increases the expected value (or mean) of \\(X\\) by the constant.\n\\[\n\\mathbb{E}(X + c) = \\mathbb{E}(X) + c\n\\]\nRule 3: Multiplying a random variable, \\(X\\), by a constant value, \\(c\\), multiplies the expected value (or mean) of \\(X\\) by that constant.\n\\[\n\\mathbb{E}(cX) = c\\bigg(\\mathbb{E}(X)\\bigg)\n\\]\nRule 4: The expected value (or mean) of the sum of two random variables, \\(X\\) and \\(Y\\) is the sum of the expected values (or means). This is also known as the additive law of expectation.\n\\[\n\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-variances",
    "href": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-variances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Variances",
    "text": "Rules for Working with Variances\nRule 1: The variance of a constant, \\(c\\), is zero.\n\\[\n\\mathrm{Var}(c) = 0\n\\]\nRule 2: Adding a constant value, \\(c\\), to a random variable, \\(X\\) does not change the variance of \\(X\\).\n\\[\n\\mathrm{Var}(X+c) = \\mathrm{Var}(X)\n\\]\nRule 3: Multiplying a random variable, \\(X\\) by a constant, \\(c\\) increases the variance of \\(X\\) by the square of the constant.\n\\[\n\\mathrm{Var}(cX) = c^2 \\times \\mathrm{Var}(X)\n\\]\nRule 4: The variance of the sum of two random variables, \\(X\\) and \\(Y\\) is equal to the sum of their variances and the covariance between them.\n\\[\n\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y)\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-covariances",
    "href": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-covariances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Covariances",
    "text": "Rules for Working with Covariances\nRule 1: The covariance of two constants, \\(c\\) and \\(k\\), is zero.\n\\[\n\\mathrm{Cov}(c,k) = 0\n\\]\nRule 2: The covariance of two independent random variables is zero.\n\\[\n\\mathrm{Cov}(X,Y) = 0\n\\]\nRule 3: The covariance is a combinative.\n\\[\n\\mathrm{Cov}(X,Y) = \\mathrm{Cov}(Y,X)\n\\]\nRule 4: The covariance of a random variable, \\(X\\), with a constant, \\(c\\) is zero.\n\\[\n\\mathrm{Cov}(X,c) = 0\n\\]\nRule 5: Adding a constant to either or both random variables does not change their covariances.\n\\[\n\\mathrm{Cov}(X+c,Y+k) = \\mathrm{Cov}(X,Y)\n\\]\nRule 6: Multiplying a random variable by a constant multiplies the covariance by that constant.\n\\[\n\\mathrm{Cov}(cX,kY) = c \\times k \\times \\mathrm{Cov}(X,Y)\n\\]\nRule 7: The additive law of covariance holds that the covariance of a random variable with a sum of random variables is just the sum of the covariances with each of the random variables.\n\\[\n\\mathrm{Cov}(X+Y, Z) = \\mathrm{Cov}(X,Z) + \\mathrm{Cov}(Y,Z)\n\\]\nRule 8: The covariance of a variable with itself is the variance of the random variable.\n\\[\n\\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n\\]"
  },
  {
    "objectID": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-correlation-coefficients",
    "href": "files/01-some-mathematics-relevant-to-regression.html#rules-for-working-with-correlation-coefficients",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Correlation Coefficients",
    "text": "Rules for Working with Correlation Coefficients\nRule 1: Adding a constant to a random variable does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(X+c, Y+k) = \\mathrm{Cor}(X, Y)\n\\]\nRule 2: Multiplying a random variable by a constant does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(cX, dY) = \\mathrm{Cor}(X, Y)\n\\]\nRule 3: Because the square root of the variance is always positive, the correlation coefficient can be negative only when the covariance is negative. This implies that:\n\\[\n-1 \\leq \\mathrm{Cor}(X, Y) \\leq 1\n\\]"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Repository for EPSY 8264",
    "section": "",
    "text": "Syllabus\nTextbook: Matrix Algebra for Educational Scientists\nCSV Data Files\nData Dictionaries\nR Scripts"
  },
  {
    "objectID": "files/03-properties-of-regression.html",
    "href": "files/03-properties-of-regression.html",
    "title": "OLS Estimators and Their Properties",
    "section": "",
    "text": "We have previously defined the population regression model (using scalar algebra) as:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nwhere the outcome (y) is assumed to be statistically and linearly related to the predictor (x) andthe error term, \\(\\epsilon\\), is a random variable.\nRecall that the least squares estimators can be analytically computed as:\n\\[\n\\begin{split}\nb_1 &= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\\\[1em]\nb_0 &= \\bar{y} - b_1(\\bar{x}) \\\\[1em]\n\\end{split}\n\\]"
  },
  {
    "objectID": "files/03-properties-of-regression.html#estimating-the-regression-coefficients",
    "href": "files/03-properties-of-regression.html#estimating-the-regression-coefficients",
    "title": "OLS Estimators and Their Properties",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nIn a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the y and x values). The estimates of the regression parameters are denoted using the roman letters \\(b_0\\) and \\(b_1\\) and the vector of these sample estimates are denoted as b. Similarly the sample residuals are denoted as e. (It is common to refer to the population errors as “errors” and the sample estimates as “residuals”.) Thus, the sample equivalent of the model is:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE). Using scalar algebra, the SSE can be expressed as: \\(\\mathrm{SSE}=\\sum e^2_i\\). The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE}&=\\mathbf{e}^\\intercal\\mathbf{e} \\\\\n&= \\begin{bmatrix}e_1 & e_2 & e_3 & \\ldots & e_n \\end{bmatrix}\\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\n\\end{split}\n\\]\nRe-arranging the sample regression equation, we can express the residual vector e as \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). The SSE can then be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y} − \\mathbf{Xb})^\\intercal(\\mathbf{y} − \\mathbf{Xb})\n\\] This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^\\intercal\\mathbf{y} − \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y}-\\mathbf{y} ^\\intercal\\mathbf{Xb} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} − 2\\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b:\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, @Fox:2009 gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThis expression is referred to as the set of Normal Equations. Note that the \\((\\mathbf{X}^\\intercal\\mathbf{X})\\) matrix has two important properties:\n\nIt is square; and\nIt is symmetric.\n\nTo solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{Ib} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\end{split}\n\\]\nAs long as \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\) exists, the vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals."
  },
  {
    "objectID": "files/03-properties-of-regression.html#extending-the-model",
    "href": "files/03-properties-of-regression.html#extending-the-model",
    "title": "OLS Estimators and Their Properties",
    "section": "Extending the Model",
    "text": "Extending the Model\nUsing matrix algebra to compute the OLS regression coefficients gives us the same values as using the analytic formulas. So why use matrix algebra? The simple reason is that we can use the same matrix algebra computation of b regardless of how many predictors we include in the model (it is extensible). The analytic formulas change and become quite difficult to manipulate. For example, consider an example where we want to estimate the coefficients for a model that includes two main effects (\\(x_1\\) and \\(x_2\\)) and an interaction between these effects. The population model written in scalar algebra is:\n\\[\ny_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\beta_3(x_{1i})(x_{2i}) + \\epsilon_i\n\\]\nIf we express this using matrix notation, we get:\n\\[\n\\begin{split}\n\\mathbf{y} &= \\mathbf{Xb} + \\mathbf{e}\n\\end{split}\n\\]\nAdding predictors expands the size of the design matrix and the length of the \\(\\boldsymbol\\beta\\) matrix, but the compact notation \\(\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\\) is exactly the same, so estimating the values in the b vector for multiple regression models is identical to doing so for the simple regression model!"
  },
  {
    "objectID": "files/03-properties-of-regression.html#properties-of-the-ols-estimators",
    "href": "files/03-properties-of-regression.html#properties-of-the-ols-estimators",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Estimators",
    "text": "Properties of the OLS Estimators\nOne property of the OLS estimators (in simple or multiple regression) is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. (Note: We derive these properties for the simple regression model, but they also can be extended for the multiple regression model.) Remember these estimators are based on the normal equations:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= \\mathbf{X}^\\intercal(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^\\intercal\\mathbf{Xb}+\\mathbf{X}^\\intercal\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^\\intercal\\mathbf{e} = \\mathbf{0}\n\\]\nLet’s examine this:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n \\end{bmatrix} \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} e_1 + e_2 + e_3 + \\ldots + e_n\\\\ X_1e_1 + X_2e_2 + X_3e_3 + \\ldots + X_ne_n \\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column k in the design matrix, that \\(X_ke_k = 0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and e is zero indicating that the two vectors are independent. This is our first property:\nP.1: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on."
  },
  {
    "objectID": "files/03-properties-of-regression.html#properties-of-the-ols-regressors",
    "href": "files/03-properties-of-regression.html#properties-of-the-ols-regressors",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Regressors",
    "text": "Properties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nP.2: The sum of the sample residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^\\intercal\\mathbf{e}\\) matrix is \\(e_1+e_2+e_3+\\ldots+e_n = \\sum e_i\\), which is equal to zero since \\(\\mathbf{X}^\\intercal\\mathbf{e}=\\mathbf{0}\\).\nP.3: The mean of the sample residuals is zero.\nSince the mean of the residuals is computed as \\(\\bar{\\mathbf{e}}=\\frac{\\sum e_i}{n}\\), and the sum (numerator) is zero, then the mean is also zero.\nP.4: The regression line passes through the point \\((\\bar{X}, \\bar{Y})\\).\nRemember that \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). This means that:\n\\[\n\\begin{split}\n\\sum \\mathbf{e} &= \\sum (\\mathbf{y}-\\mathbf{Xb})\\\\[2ex]\n&= \\sum\\mathbf{y} - \\sum (\\mathbf{Xb}) \\\\[2ex]\n&= \\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})\n\\end{split}\n\\]\nIf we divide this expression by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum \\mathbf{e}}{n} &= \\frac{\\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n&= \\frac{\\sum\\mathbf{y}}{n} - \\frac{\\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n\\bar{\\mathbf{e}} &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\]\nBut, the mean of the residuals is zero, so:\n\\[\n\\begin{split}\n0 &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\] That is, the predicted y-value when the mean of X is used as a predictor is the mean of Y. In other words, the point \\((\\bar{X}, \\bar{Y})\\) is on the regression line.\nP.5: The predicted y-values are uncorrelated with the sample residuals.\nSince \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\) then \\(\\hat{\\mathbf{y}}^\\intercal=(\\mathbf{Xb})^\\intercal\\). If we post-multiply both sides of this expression by the residual vector e, we get:\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} &= (\\mathbf{Xb})^\\intercal \\mathbf{e} \\\\[2ex]\n&= \\mathbf{b}^\\intercal \\mathbf{X}^\\intercal \\mathbf{e}\n\\end{split}\n\\]\nSince \\(\\mathbf{X}^\\intercal \\mathbf{e}=\\mathbf{0}\\), then \\(\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} =0\\). This implies that \\(\\hat{\\mathbf{y}}\\) and e are uncorrelated.\nP.6: The mean of the predicted y-values is equal to the mean of the observed y-values.\nWe can make use of the fact that \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\). Taking the sum of both sides of the expression and dividing by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum\\mathbf{y}}{n} &= \\frac{\\sum(\\hat{\\mathbf{y}} + \\mathbf{e})}{n} \\\\[2ex]\n&= \\frac{\\sum\\hat{\\mathbf{y}}}{n} + \\frac{\\sum\\mathbf{e}}{n} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}} + \\bar{\\mathbf{e}} \\\\[2ex]\n&= \\bar{\\hat{\\mathbf{y}}} + 0 \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}}\n\\end{split}\n\\]\n\nIMPORTANT\nThese properties will always be true. They do not rely on any distributional assumptions of the residuals. Furthermore, these properties do not tell us anything about how “good” the coefficient estimates (b) are. Nor do these properties allow us to make inferences about the true parameters (\\(\\boldsymbol\\beta\\))."
  },
  {
    "objectID": "files/03-properties-of-regression.html#references",
    "href": "files/03-properties-of-regression.html#references",
    "title": "OLS Estimators and Their Properties",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html",
    "href": "notes/01-02-properties-of-regression.html",
    "title": "OLS Estimators and Their Properties",
    "section": "",
    "text": "We have previously defined the population regression model (using scalar algebra) as:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nwhere the outcome (y) is assumed to be statistically and linearly related to the predictor (x) andthe error term, \\(\\epsilon\\), is a random variable.\nRecall that the least squares estimators can be analytically computed as:\n\\[\n\\begin{split}\nb_1 &= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\\\[1em]\nb_0 &= \\bar{y} - b_1(\\bar{x}) \\\\[1em]\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html#estimating-the-regression-coefficients",
    "href": "notes/01-02-properties-of-regression.html#estimating-the-regression-coefficients",
    "title": "OLS Estimators and Their Properties",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nIn a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the y and x values). The estimates of the regression parameters are denoted using the roman letters \\(b_0\\) and \\(b_1\\) and the vector of these sample estimates are denoted as b. Similarly the sample residuals are denoted as e. (It is common to refer to the population errors as “errors” and the sample estimates as “residuals”.) Thus, the sample equivalent of the model is:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE). Using scalar algebra, the SSE can be expressed as: \\(\\mathrm{SSE}=\\sum e^2_i\\). The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE}&=\\mathbf{e}^\\intercal\\mathbf{e} \\\\\n&= \\begin{bmatrix}e_1 & e_2 & e_3 & \\ldots & e_n \\end{bmatrix}\\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\n\\end{split}\n\\]\nRe-arranging the sample regression equation, we can express the residual vector e as \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). The SSE can then be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y} − \\mathbf{Xb})^\\intercal(\\mathbf{y} − \\mathbf{Xb})\n\\] This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^\\intercal\\mathbf{y} − \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y}-\\mathbf{y} ^\\intercal\\mathbf{Xb} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} − 2\\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b:\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, @Fox:2009 gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThis expression is referred to as the set of Normal Equations. Note that the \\((\\mathbf{X}^\\intercal\\mathbf{X})\\) matrix has two important properties:\n\nIt is square; and\nIt is symmetric.\n\nTo solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{Ib} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\end{split}\n\\]\nAs long as \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\) exists, the vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals."
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html#extending-the-model",
    "href": "notes/01-02-properties-of-regression.html#extending-the-model",
    "title": "OLS Estimators and Their Properties",
    "section": "Extending the Model",
    "text": "Extending the Model\nUsing matrix algebra to compute the OLS regression coefficients gives us the same values as using the analytic formulas. So why use matrix algebra? The simple reason is that we can use the same matrix algebra computation of b regardless of how many predictors we include in the model (it is extensible). The analytic formulas change and become quite difficult to manipulate. For example, consider an example where we want to estimate the coefficients for a model that includes two main effects (\\(x_1\\) and \\(x_2\\)) and an interaction between these effects. The population model written in scalar algebra is:\n\\[\ny_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\beta_3(x_{1i})(x_{2i}) + \\epsilon_i\n\\]\nIf we express this using matrix notation, we get:\n\\[\n\\begin{split}\n\\mathbf{y} &= \\mathbf{Xb} + \\mathbf{e}\n\\end{split}\n\\]\nAdding predictors expands the size of the design matrix and the length of the \\(\\boldsymbol\\beta\\) matrix, but the compact notation \\(\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\\) is exactly the same, so estimating the values in the b vector for multiple regression models is identical to doing so for the simple regression model!"
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html#properties-of-the-ols-estimators",
    "href": "notes/01-02-properties-of-regression.html#properties-of-the-ols-estimators",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Estimators",
    "text": "Properties of the OLS Estimators\nOne property of the OLS estimators (in simple or multiple regression) is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. (Note: We derive these properties for the simple regression model, but they also can be extended for the multiple regression model.) Remember these estimators are based on the normal equations:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= \\mathbf{X}^\\intercal(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^\\intercal\\mathbf{Xb}+\\mathbf{X}^\\intercal\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^\\intercal\\mathbf{e} = \\mathbf{0}\n\\]\nLet’s examine this:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n \\end{bmatrix} \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} e_1 + e_2 + e_3 + \\ldots + e_n\\\\ X_1e_1 + X_2e_2 + X_3e_3 + \\ldots + X_ne_n \\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column k in the design matrix, that \\(X_ke_k = 0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and e is zero indicating that the two vectors are independent. This is our first property:\nP.1: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on."
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html#properties-of-the-ols-regressors",
    "href": "notes/01-02-properties-of-regression.html#properties-of-the-ols-regressors",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Regressors",
    "text": "Properties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nP.2: The sum of the sample residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^\\intercal\\mathbf{e}\\) matrix is \\(e_1+e_2+e_3+\\ldots+e_n = \\sum e_i\\), which is equal to zero since \\(\\mathbf{X}^\\intercal\\mathbf{e}=\\mathbf{0}\\).\nP.3: The mean of the sample residuals is zero.\nSince the mean of the residuals is computed as \\(\\bar{\\mathbf{e}}=\\frac{\\sum e_i}{n}\\), and the sum (numerator) is zero, then the mean is also zero.\nP.4: The regression line passes through the point \\((\\bar{X}, \\bar{Y})\\).\nRemember that \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). This means that:\n\\[\n\\begin{split}\n\\sum \\mathbf{e} &= \\sum (\\mathbf{y}-\\mathbf{Xb})\\\\[2ex]\n&= \\sum\\mathbf{y} - \\sum (\\mathbf{Xb}) \\\\[2ex]\n&= \\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})\n\\end{split}\n\\]\nIf we divide this expression by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum \\mathbf{e}}{n} &= \\frac{\\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n&= \\frac{\\sum\\mathbf{y}}{n} - \\frac{\\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n\\bar{\\mathbf{e}} &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\]\nBut, the mean of the residuals is zero, so:\n\\[\n\\begin{split}\n0 &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\] That is, the predicted y-value when the mean of X is used as a predictor is the mean of Y. In other words, the point \\((\\bar{X}, \\bar{Y})\\) is on the regression line.\nP.5: The predicted y-values are uncorrelated with the sample residuals.\nSince \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\) then \\(\\hat{\\mathbf{y}}^\\intercal=(\\mathbf{Xb})^\\intercal\\). If we post-multiply both sides of this expression by the residual vector e, we get:\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} &= (\\mathbf{Xb})^\\intercal \\mathbf{e} \\\\[2ex]\n&= \\mathbf{b}^\\intercal \\mathbf{X}^\\intercal \\mathbf{e}\n\\end{split}\n\\]\nSince \\(\\mathbf{X}^\\intercal \\mathbf{e}=\\mathbf{0}\\), then \\(\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} =0\\). This implies that \\(\\hat{\\mathbf{y}}\\) and e are uncorrelated.\nP.6: The mean of the predicted y-values is equal to the mean of the observed y-values.\nWe can make use of the fact that \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\). Taking the sum of both sides of the expression and dividing by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum\\mathbf{y}}{n} &= \\frac{\\sum(\\hat{\\mathbf{y}} + \\mathbf{e})}{n} \\\\[2ex]\n&= \\frac{\\sum\\hat{\\mathbf{y}}}{n} + \\frac{\\sum\\mathbf{e}}{n} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}} + \\bar{\\mathbf{e}} \\\\[2ex]\n&= \\bar{\\hat{\\mathbf{y}}} + 0 \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}}\n\\end{split}\n\\]\n\nIMPORTANT\nThese properties will always be true. They do not rely on any distributional assumptions of the residuals. Furthermore, these properties do not tell us anything about how “good” the coefficient estimates (b) are. Nor do these properties allow us to make inferences about the true parameters (\\(\\boldsymbol\\beta\\))."
  },
  {
    "objectID": "notes/01-02-properties-of-regression.html#references",
    "href": "notes/01-02-properties-of-regression.html#references",
    "title": "OLS Estimators and Their Properties",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "files/01-02-properties-of-regression.html",
    "href": "files/01-02-properties-of-regression.html",
    "title": "OLS Estimators and Their Properties",
    "section": "",
    "text": "We have previously defined the population regression model (using scalar algebra) as:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nwhere the outcome (y) is assumed to be statistically and linearly related to the predictor (x) and the error term, \\(\\epsilon\\), is a random variable.\nRecall that the least squares estimators can be analytically computed as:\n\\[\n\\begin{split}\nb_1 &= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\\\[1em]\nb_0 &= \\bar{y} - b_1(\\bar{x}) \\\\[1em]\n\\end{split}\n\\]"
  },
  {
    "objectID": "files/01-02-properties-of-regression.html#estimating-the-regression-coefficients",
    "href": "files/01-02-properties-of-regression.html#estimating-the-regression-coefficients",
    "title": "OLS Estimators and Their Properties",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nIn a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the y and x values). The estimates of the regression parameters are denoted using the roman letters \\(b_0\\) and \\(b_1\\) and the vector of these sample estimates are denoted as b. Similarly the sample residuals are denoted as e. (It is common to refer to the population errors as “errors” and the sample estimates as “residuals”.) Thus, the sample equivalent of the model is:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE). Using scalar algebra, the SSE can be expressed as: \\(\\mathrm{SSE}=\\sum e^2_i\\). The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE}&=\\mathbf{e}^\\intercal\\mathbf{e} \\\\\n&= \\begin{bmatrix}e_1 & e_2 & e_3 & \\ldots & e_n \\end{bmatrix}\\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\n\\end{split}\n\\]\nRe-arranging the sample regression equation, we can express the residual vector e as \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). The SSE can then be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y} − \\mathbf{Xb})^\\intercal(\\mathbf{y} − \\mathbf{Xb})\n\\] This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^\\intercal\\mathbf{y} − \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y}-\\mathbf{y} ^\\intercal\\mathbf{Xb} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} − 2\\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b:\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThis expression is referred to as the set of Normal Equations. Note that the \\((\\mathbf{X}^\\intercal\\mathbf{X})\\) matrix has two important properties:\n\nIt is square; and\nIt is symmetric.\n\nTo solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{Ib} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\end{split}\n\\]\nAs long as \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\) exists, the vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals."
  },
  {
    "objectID": "files/01-02-properties-of-regression.html#extending-the-model",
    "href": "files/01-02-properties-of-regression.html#extending-the-model",
    "title": "OLS Estimators and Their Properties",
    "section": "Extending the Model",
    "text": "Extending the Model\nUsing matrix algebra to compute the OLS regression coefficients gives us the same values as using the analytic formulas. So why use matrix algebra? The simple reason is that we can use the same matrix algebra computation of b regardless of how many predictors we include in the model (it is extensible). The analytic formulas change and become quite difficult to manipulate. For example, consider an example where we want to estimate the coefficients for a model that includes two main effects (\\(x_1\\) and \\(x_2\\)) and an interaction between these effects. The population model written in scalar algebra is:\n\\[\ny_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\beta_3(x_{1i})(x_{2i}) + \\epsilon_i\n\\]\nIf we express this using matrix notation, we get:\n\\[\n\\begin{split}\n\\mathbf{y} &= \\mathbf{Xb} + \\mathbf{e}\n\\end{split}\n\\]\nAdding predictors expands the size of the design matrix and the length of the \\(\\boldsymbol\\beta\\) matrix, but the compact notation \\(\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\\) is exactly the same, so estimating the values in the b vector for multiple regression models is identical to doing so for the simple regression model!"
  },
  {
    "objectID": "files/01-02-properties-of-regression.html#properties-of-the-ols-estimators",
    "href": "files/01-02-properties-of-regression.html#properties-of-the-ols-estimators",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Estimators",
    "text": "Properties of the OLS Estimators\nOne property of the OLS estimators (in simple or multiple regression) is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. (Note: We derive these properties for the simple regression model, but they also can be extended for the multiple regression model.) Remember these estimators are based on the normal equations:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= \\mathbf{X}^\\intercal(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^\\intercal\\mathbf{Xb}+\\mathbf{X}^\\intercal\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^\\intercal\\mathbf{e} = \\mathbf{0}\n\\]\nLet’s examine this:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n \\end{bmatrix} \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} e_1 + e_2 + e_3 + \\ldots + e_n\\\\ X_1e_1 + X_2e_2 + X_3e_3 + \\ldots + X_ne_n \\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column k in the design matrix, that \\(X_ke_k = 0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and e is zero indicating that the two vectors are independent. This is our first property:\nP.1: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on."
  },
  {
    "objectID": "files/01-02-properties-of-regression.html#properties-of-the-ols-regressors",
    "href": "files/01-02-properties-of-regression.html#properties-of-the-ols-regressors",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Regressors",
    "text": "Properties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nP.2: The sum of the sample residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^\\intercal\\mathbf{e}\\) matrix is \\(e_1+e_2+e_3+\\ldots+e_n = \\sum e_i\\), which is equal to zero since \\(\\mathbf{X}^\\intercal\\mathbf{e}=\\mathbf{0}\\).\nP.3: The mean of the sample residuals is zero.\nSince the mean of the residuals is computed as \\(\\bar{\\mathbf{e}}=\\frac{\\sum e_i}{n}\\), and the sum (numerator) is zero, then the mean is also zero.\nP.4: The regression line passes through the point \\((\\bar{X}, \\bar{Y})\\).\nRemember that \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). This means that:\n\\[\n\\begin{split}\n\\sum \\mathbf{e} &= \\sum (\\mathbf{y}-\\mathbf{Xb})\\\\[2ex]\n&= \\sum\\mathbf{y} - \\sum (\\mathbf{Xb}) \\\\[2ex]\n&= \\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})\n\\end{split}\n\\]\nIf we divide this expression by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum \\mathbf{e}}{n} &= \\frac{\\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n&= \\frac{\\sum\\mathbf{y}}{n} - \\frac{\\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n\\bar{\\mathbf{e}} &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\]\nBut, the mean of the residuals is zero, so:\n\\[\n\\begin{split}\n0 &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\] That is, the predicted y-value when the mean of X is used as a predictor is the mean of Y. In other words, the point \\((\\bar{X}, \\bar{Y})\\) is on the regression line.\nP.5: The predicted y-values are uncorrelated with the sample residuals.\nSince \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\) then \\(\\hat{\\mathbf{y}}^\\intercal=(\\mathbf{Xb})^\\intercal\\). If we post-multiply both sides of this expression by the residual vector e, we get:\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} &= (\\mathbf{Xb})^\\intercal \\mathbf{e} \\\\[2ex]\n&= \\mathbf{b}^\\intercal \\mathbf{X}^\\intercal \\mathbf{e}\n\\end{split}\n\\]\nSince \\(\\mathbf{X}^\\intercal \\mathbf{e}=\\mathbf{0}\\), then \\(\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} =0\\). This implies that \\(\\hat{\\mathbf{y}}\\) and e are uncorrelated.\nP.6: The mean of the predicted y-values is equal to the mean of the observed y-values.\nWe can make use of the fact that \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\). Taking the sum of both sides of the expression and dividing by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum\\mathbf{y}}{n} &= \\frac{\\sum(\\hat{\\mathbf{y}} + \\mathbf{e})}{n} \\\\[2ex]\n&= \\frac{\\sum\\hat{\\mathbf{y}}}{n} + \\frac{\\sum\\mathbf{e}}{n} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}} + \\bar{\\mathbf{e}} \\\\[2ex]\n&= \\bar{\\hat{\\mathbf{y}}} + 0 \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}}\n\\end{split}\n\\]\n\nIMPORTANT\nThese properties will always be true. They do not rely on any distributional assumptions of the residuals. Furthermore, these properties do not tell us anything about how “good” the coefficient estimates (b) are. Nor do these properties allow us to make inferences about the true parameters (\\(\\boldsymbol\\beta\\))."
  },
  {
    "objectID": "files/01-02-properties-of-regression.html#references",
    "href": "files/01-02-properties-of-regression.html#references",
    "title": "OLS Estimators and Their Properties",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "",
    "text": "Assume the \\(X\\) and \\(Y\\) are random variables and \\(c\\) is a constant, such that:\n\\[\n\\begin{split}\nX &= \\{x_1, x_2, x_3, \\ldots, x_n\\} \\\\\nY &= \\{y_1, y_2, y_3, \\ldots, y_n\\} \\\\\nc &= \\{c_1, c_2, c_3, \\ldots, c_n\\} \\qquad \\mathrm{where~} c_1= c_2= c_3= \\ldots= c_n\\ \\\\\n\\end{split}\n\\]\nThe mean of these random (and constant) variables is denoted as the expected value, namely, \\(\\mathbb{E}(X)\\), \\(\\mathbb{E}(Y)\\), and \\(\\mathbb{E}(c)\\)."
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-variance",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-variance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Variance",
    "text": "Formula for Variance\nOne very useful measure that we will work with a lot in the course is the variance. Here are several formulas to compute the variance of a random variable, \\(X\\). We denote the variance of \\(X\\) using \\(\\sigma^2_X\\) or \\(\\mathrm{Var}(X)\\). The most common formula for variance is:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)^2}{n}\n\\]\nWe can also compute variance as an expected value of the squared mean deviations:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nLastly, it can sometimes be helpful to express the variance as the difference between the expected value of \\(X^2\\) and the squared expected value of \\(X\\):\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}(X^2) -  \\big[\\mathbb{E}(X)\\big]^2\n\\]\nLastly, we note that the standard deviation is the square root of the variance:\n\\[\n\\sigma_X = \\sqrt{\\sigma^2_X} = \\sqrt{\\mathrm{Var}(X)} = \\mathrm{SD}(X)\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-covariance",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-covariance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Covariance",
    "text": "Formula for Covariance\nAnother useful measure that we will be working with in the course is the covariance. We denote the covariance between \\(X\\) and \\(Y\\) using \\(\\sigma_{XY}\\) or \\(\\mathrm{Cov}(X,Y)\\). The most common formula for covariance is:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)\\bigg(Y_i - \\mathbb{E}(Y)\\bigg)}{n}\n\\]\nThe covariance can also be expressed as an expectation:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}\\bigg(\\big[X - \\mathbb{E}(X)\\big]\\big[Y - \\mathbb{E}(Y)\\big]\\bigg)\n\\]\nLastly, we can also express the covariance as a difference of expectations.\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-correlation-coefficient",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#formula-for-correlation-coefficient",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Correlation Coefficient",
    "text": "Formula for Correlation Coefficient\nThe correlation coefficient is a standardized covariance value. We denote the correlation between \\(X\\) and \\(Y\\) using \\(\\rho_{XY}\\) or \\(\\mathrm{Cor}(X,Y)\\). The most common formula for correlation is:\n\\[\n\\rho_{XY} = \\mathrm{Cor}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}}\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-sums",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-sums",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Sums",
    "text": "Rules for Working with Sums\nThe sum of \\(X\\) is defined as,\n\\[\n\\sum_{i=1}^n X_i = x_1 + x_2 + x_3 + \\ldots + x_n\n\\]\nTo keep the notation simpler, we will just denote this as \\(\\sum X\\).\nRule 1: When a summation is itself a sum or difference, the summation sign may be distributed among the separate terms of the sum. That is:\n\\[\n\\sum(X + Y) = \\sum X + \\sum Y\n\\]\nRule 2: The sum of a constant, \\(c\\), is \\(n\\) times the value of the constant.\n\\[\n\\sum(c) = nc\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-expectations-means",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-expectations-means",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Expectations (Means)",
    "text": "Rules for Working with Expectations (Means)\nThe expectation (mean) of \\(X\\) is defined as,\n\\[\n\\mathbb{E}(X) = \\frac{\\sum_{i=1}^n X_i}{n}\n\\]\nAgain, to keep the notation simpler, we will just denote this as \\(\\mathbb{E}(X) = \\frac{\\sum X}{n}\\).\nRule 1: The expectation of a constant, \\(c\\), is the constant.\n\\[\n\\mathbb{E}(c) = c\n\\]\nRule 2: Adding a constant value, \\(c\\), to each term in a random variable, \\(X\\), increases the expected value (or mean) of \\(X\\) by the constant.\n\\[\n\\mathbb{E}(X + c) = \\mathbb{E}(X) + c\n\\]\nRule 3: Multiplying a random variable, \\(X\\), by a constant value, \\(c\\), multiplies the expected value (or mean) of \\(X\\) by that constant.\n\\[\n\\mathbb{E}(cX) = c\\bigg(\\mathbb{E}(X)\\bigg)\n\\]\nRule 4: The expected value (or mean) of the sum of two random variables, \\(X\\) and \\(Y\\) is the sum of the expected values (or means). This is also known as the additive law of expectation.\n\\[\n\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-variances",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-variances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Variances",
    "text": "Rules for Working with Variances\nRule 1: The variance of a constant, \\(c\\), is zero.\n\\[\n\\mathrm{Var}(c) = 0\n\\]\nRule 2: Adding a constant value, \\(c\\), to a random variable, \\(X\\) does not change the variance of \\(X\\).\n\\[\n\\mathrm{Var}(X+c) = \\mathrm{Var}(X)\n\\]\nRule 3: Multiplying a random variable, \\(X\\) by a constant, \\(c\\) increases the variance of \\(X\\) by the square of the constant.\n\\[\n\\mathrm{Var}(cX) = c^2 \\times \\mathrm{Var}(X)\n\\]\nRule 4: The variance of the sum of two random variables, \\(X\\) and \\(Y\\) is equal to the sum of their variances and the covariance between them.\n\\[\n\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y)\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-covariances",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-covariances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Covariances",
    "text": "Rules for Working with Covariances\nRule 1: The covariance of two constants, \\(c\\) and \\(k\\), is zero.\n\\[\n\\mathrm{Cov}(c,k) = 0\n\\]\nRule 2: The covariance of two independent random variables is zero.\n\\[\n\\mathrm{Cov}(X,Y) = 0\n\\]\nRule 3: The covariance is a combinative.\n\\[\n\\mathrm{Cov}(X,Y) = \\mathrm{Cov}(Y,X)\n\\]\nRule 4: The covariance of a random variable, \\(X\\), with a constant, \\(c\\) is zero.\n\\[\n\\mathrm{Cov}(X,c) = 0\n\\]\nRule 5: Adding a constant to either or both random variables does not change their covariances.\n\\[\n\\mathrm{Cov}(X+c,Y+k) = \\mathrm{Cov}(X,Y)\n\\]\nRule 6: Multiplying a random variable by a constant multiplies the covariance by that constant.\n\\[\n\\mathrm{Cov}(cX,kY) = c \\times k \\times \\mathrm{Cov}(X,Y)\n\\]\nRule 7: The additive law of covariance holds that the covariance of a random variable with a sum of random variables is just the sum of the covariances with each of the random variables.\n\\[\n\\mathrm{Cov}(X+Y, Z) = \\mathrm{Cov}(X,Z) + \\mathrm{Cov}(Y,Z)\n\\]\nRule 8: The covariance of a variable with itself is the variance of the random variable.\n\\[\n\\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n\\]"
  },
  {
    "objectID": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-correlation-coefficients",
    "href": "files/01-01-some-mathematics-relevant-to-regression.html#rules-for-working-with-correlation-coefficients",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Correlation Coefficients",
    "text": "Rules for Working with Correlation Coefficients\nRule 1: Adding a constant to a random variable does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(X+c, Y+k) = \\mathrm{Cor}(X, Y)\n\\]\nRule 2: Multiplying a random variable by a constant does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(cX, dY) = \\mathrm{Cor}(X, Y)\n\\]\nRule 3: Because the square root of the variance is always positive, the correlation coefficient can be negative only when the covariance is negative. This implies that:\n\\[\n-1 \\leq \\mathrm{Cor}(X, Y) \\leq 1\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html",
    "href": "files/01-01-summation-expectation.html",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "",
    "text": "Assume the \\(X\\) and \\(Y\\) are random variables and \\(c\\) is a constant, such that:\n\\[\n\\begin{split}\nX &= \\{x_1, x_2, x_3, \\ldots, x_n\\} \\\\\nY &= \\{y_1, y_2, y_3, \\ldots, y_n\\} \\\\\nc &= \\{c_1, c_2, c_3, \\ldots, c_n\\} \\qquad \\mathrm{where~} c_1= c_2= c_3= \\ldots= c_n\\ \\\\\n\\end{split}\n\\]\nThe mean of these random (and constant) variables is denoted as the expected value, namely, \\(\\mathbb{E}(X)\\), \\(\\mathbb{E}(Y)\\), and \\(\\mathbb{E}(c)\\)."
  },
  {
    "objectID": "files/01-01-summation-expectation.html#formula-for-variance",
    "href": "files/01-01-summation-expectation.html#formula-for-variance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Variance",
    "text": "Formula for Variance\nOne very useful measure that we will work with a lot in the course is the variance. Here are several formulas to compute the variance of a random variable, \\(X\\). We denote the variance of \\(X\\) using \\(\\sigma^2_X\\) or \\(\\mathrm{Var}(X)\\). The most common formula for variance is:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)^2}{n}\n\\]\nWe can also compute variance as an expected value of the squared mean deviations:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nLastly, it can sometimes be helpful to express the variance as the difference between the expected value of \\(X^2\\) and the squared expected value of \\(X\\):\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}(X^2) -  \\big[\\mathbb{E}(X)\\big]^2\n\\]\nLastly, we note that the standard deviation is the square root of the variance:\n\\[\n\\sigma_X = \\sqrt{\\sigma^2_X} = \\sqrt{\\mathrm{Var}(X)} = \\mathrm{SD}(X)\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#formula-for-covariance",
    "href": "files/01-01-summation-expectation.html#formula-for-covariance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Covariance",
    "text": "Formula for Covariance\nAnother useful measure that we will be working with in the course is the covariance. We denote the covariance between \\(X\\) and \\(Y\\) using \\(\\sigma_{XY}\\) or \\(\\mathrm{Cov}(X,Y)\\). The most common formula for covariance is:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)\\bigg(Y_i - \\mathbb{E}(Y)\\bigg)}{n}\n\\]\nThe covariance can also be expressed as an expectation:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}\\bigg(\\big[X - \\mathbb{E}(X)\\big]\\big[Y - \\mathbb{E}(Y)\\big]\\bigg)\n\\]\nLastly, we can also express the covariance as a difference of expectations.\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#formula-for-correlation-coefficient",
    "href": "files/01-01-summation-expectation.html#formula-for-correlation-coefficient",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Correlation Coefficient",
    "text": "Formula for Correlation Coefficient\nThe correlation coefficient is a standardized covariance value. We denote the correlation between \\(X\\) and \\(Y\\) using \\(\\rho_{XY}\\) or \\(\\mathrm{Cor}(X,Y)\\). The most common formula for correlation is:\n\\[\n\\rho_{XY} = \\mathrm{Cor}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}}\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#rules-for-working-with-sums",
    "href": "files/01-01-summation-expectation.html#rules-for-working-with-sums",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Sums",
    "text": "Rules for Working with Sums\nThe sum of \\(X\\) is defined as,\n\\[\n\\sum_{i=1}^n X_i = x_1 + x_2 + x_3 + \\ldots + x_n\n\\]\nTo keep the notation simpler, we will just denote this as \\(\\sum X\\).\nRule 1: When a summation is itself a sum or difference, the summation sign may be distributed among the separate terms of the sum. That is:\n\\[\n\\sum(X + Y) = \\sum X + \\sum Y\n\\]\nRule 2: The sum of a constant, \\(c\\), is \\(n\\) times the value of the constant.\n\\[\n\\sum(c) = nc\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#rules-for-working-with-expectations-means",
    "href": "files/01-01-summation-expectation.html#rules-for-working-with-expectations-means",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Expectations (Means)",
    "text": "Rules for Working with Expectations (Means)\nThe expectation (mean) of \\(X\\) is defined as,\n\\[\n\\mathbb{E}(X) = \\frac{\\sum_{i=1}^n X_i}{n}\n\\]\nAgain, to keep the notation simpler, we will just denote this as \\(\\mathbb{E}(X) = \\frac{\\sum X}{n}\\).\nRule 1: The expectation of a constant, \\(c\\), is the constant.\n\\[\n\\mathbb{E}(c) = c\n\\]\nRule 2: Adding a constant value, \\(c\\), to each term in a random variable, \\(X\\), increases the expected value (or mean) of \\(X\\) by the constant.\n\\[\n\\mathbb{E}(X + c) = \\mathbb{E}(X) + c\n\\]\nRule 3: Multiplying a random variable, \\(X\\), by a constant value, \\(c\\), multiplies the expected value (or mean) of \\(X\\) by that constant.\n\\[\n\\mathbb{E}(cX) = c\\bigg(\\mathbb{E}(X)\\bigg)\n\\]\nRule 4: The expected value (or mean) of the sum of two random variables, \\(X\\) and \\(Y\\) is the sum of the expected values (or means). This is also known as the additive law of expectation.\n\\[\n\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#rules-for-working-with-variances",
    "href": "files/01-01-summation-expectation.html#rules-for-working-with-variances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Variances",
    "text": "Rules for Working with Variances\nRule 1: The variance of a constant, \\(c\\), is zero.\n\\[\n\\mathrm{Var}(c) = 0\n\\]\nRule 2: Adding a constant value, \\(c\\), to a random variable, \\(X\\) does not change the variance of \\(X\\).\n\\[\n\\mathrm{Var}(X+c) = \\mathrm{Var}(X)\n\\]\nRule 3: Multiplying a random variable, \\(X\\) by a constant, \\(c\\) increases the variance of \\(X\\) by the square of the constant.\n\\[\n\\mathrm{Var}(cX) = c^2 \\times \\mathrm{Var}(X)\n\\]\nRule 4: The variance of the sum of two random variables, \\(X\\) and \\(Y\\) is equal to the sum of their variances and the covariance between them.\n\\[\n\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y)\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#rules-for-working-with-covariances",
    "href": "files/01-01-summation-expectation.html#rules-for-working-with-covariances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Covariances",
    "text": "Rules for Working with Covariances\nRule 1: The covariance of two constants, \\(c\\) and \\(k\\), is zero.\n\\[\n\\mathrm{Cov}(c,k) = 0\n\\]\nRule 2: The covariance of two independent random variables is zero.\n\\[\n\\mathrm{Cov}(X,Y) = 0\n\\]\nRule 3: The covariance is a combinative.\n\\[\n\\mathrm{Cov}(X,Y) = \\mathrm{Cov}(Y,X)\n\\]\nRule 4: The covariance of a random variable, \\(X\\), with a constant, \\(c\\) is zero.\n\\[\n\\mathrm{Cov}(X,c) = 0\n\\]\nRule 5: Adding a constant to either or both random variables does not change their covariances.\n\\[\n\\mathrm{Cov}(X+c,Y+k) = \\mathrm{Cov}(X,Y)\n\\]\nRule 6: Multiplying a random variable by a constant multiplies the covariance by that constant.\n\\[\n\\mathrm{Cov}(cX,kY) = c \\times k \\times \\mathrm{Cov}(X,Y)\n\\]\nRule 7: The additive law of covariance holds that the covariance of a random variable with a sum of random variables is just the sum of the covariances with each of the random variables.\n\\[\n\\mathrm{Cov}(X+Y, Z) = \\mathrm{Cov}(X,Z) + \\mathrm{Cov}(Y,Z)\n\\]\nRule 8: The covariance of a variable with itself is the variance of the random variable.\n\\[\n\\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n\\]"
  },
  {
    "objectID": "files/01-01-summation-expectation.html#rules-for-working-with-correlation-coefficients",
    "href": "files/01-01-summation-expectation.html#rules-for-working-with-correlation-coefficients",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Correlation Coefficients",
    "text": "Rules for Working with Correlation Coefficients\nRule 1: Adding a constant to a random variable does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(X+c, Y+k) = \\mathrm{Cor}(X, Y)\n\\]\nRule 2: Multiplying a random variable by a constant does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(cX, dY) = \\mathrm{Cor}(X, Y)\n\\]\nRule 3: Because the square root of the variance is always positive, the correlation coefficient can be negative only when the covariance is negative. This implies that:\n\\[\n-1 \\leq \\mathrm{Cor}(X, Y) \\leq 1\n\\]"
  },
  {
    "objectID": "files/01-02-ols-estimators-and-their-properties.html",
    "href": "files/01-02-ols-estimators-and-their-properties.html",
    "title": "OLS Estimators and Their Properties",
    "section": "",
    "text": "We have previously defined the population regression model (using scalar algebra) as:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nwhere the outcome (y) is assumed to be statistically and linearly related to the predictor (x) and the error term, \\(\\epsilon\\), is a random variable.\nRecall that the least squares estimators can be analytically computed as:\n\\[\n\\begin{split}\nb_1 &= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\\\[1em]\nb_0 &= \\bar{y} - b_1(\\bar{x}) \\\\[1em]\n\\end{split}\n\\]"
  },
  {
    "objectID": "files/01-02-ols-estimators-and-their-properties.html#estimating-the-regression-coefficients",
    "href": "files/01-02-ols-estimators-and-their-properties.html#estimating-the-regression-coefficients",
    "title": "OLS Estimators and Their Properties",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nIn a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the y and x values). The estimates of the regression parameters are denoted using the roman letters \\(b_0\\) and \\(b_1\\) and the vector of these sample estimates are denoted as b. Similarly the sample residuals are denoted as e. (It is common to refer to the population errors as “errors” and the sample estimates as “residuals”.) Thus, the sample equivalent of the model is:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE). Using scalar algebra, the SSE can be expressed as: \\(\\mathrm{SSE}=\\sum e^2_i\\). The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE}&=\\mathbf{e}^\\intercal\\mathbf{e} \\\\\n&= \\begin{bmatrix}e_1 & e_2 & e_3 & \\ldots & e_n \\end{bmatrix}\\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\n\\end{split}\n\\]\nRe-arranging the sample regression equation, we can express the residual vector e as \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). The SSE can then be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y} − \\mathbf{Xb})^\\intercal(\\mathbf{y} − \\mathbf{Xb})\n\\] This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^\\intercal\\mathbf{y} − \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y}-\\mathbf{y} ^\\intercal\\mathbf{Xb} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} − 2\\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b:\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThis expression is referred to as the set of Normal Equations. Note that the \\((\\mathbf{X}^\\intercal\\mathbf{X})\\) matrix has two important properties:\n\nIt is square; and\nIt is symmetric.\n\nTo solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{Ib} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\end{split}\n\\]\nAs long as \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\) exists, the vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals."
  },
  {
    "objectID": "files/01-02-ols-estimators-and-their-properties.html#extending-the-model",
    "href": "files/01-02-ols-estimators-and-their-properties.html#extending-the-model",
    "title": "OLS Estimators and Their Properties",
    "section": "Extending the Model",
    "text": "Extending the Model\nUsing matrix algebra to compute the OLS regression coefficients gives us the same values as using the analytic formulas. So why use matrix algebra? The simple reason is that we can use the same matrix algebra computation of b regardless of how many predictors we include in the model (it is extensible). The analytic formulas change and become quite difficult to manipulate. For example, consider an example where we want to estimate the coefficients for a model that includes two main effects (\\(x_1\\) and \\(x_2\\)) and an interaction between these effects. The population model written in scalar algebra is:\n\\[\ny_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\beta_3(x_{1i})(x_{2i}) + \\epsilon_i\n\\]\nIf we express this using matrix notation, we get:\n\\[\n\\begin{split}\n\\mathbf{y} &= \\mathbf{Xb} + \\mathbf{e}\n\\end{split}\n\\]\nAdding predictors expands the size of the design matrix and the length of the \\(\\boldsymbol\\beta\\) matrix, but the compact notation \\(\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\\) is exactly the same, so estimating the values in the b vector for multiple regression models is identical to doing so for the simple regression model!"
  },
  {
    "objectID": "files/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-estimators",
    "href": "files/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-estimators",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Estimators",
    "text": "Properties of the OLS Estimators\nOne property of the OLS estimators (in simple or multiple regression) is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. (Note: We derive these properties for the simple regression model, but they also can be extended for the multiple regression model.) Remember these estimators are based on the normal equations:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= \\mathbf{X}^\\intercal(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^\\intercal\\mathbf{Xb}+\\mathbf{X}^\\intercal\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^\\intercal\\mathbf{e} = \\mathbf{0}\n\\]\nLet’s examine this:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n \\end{bmatrix} \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} e_1 + e_2 + e_3 + \\ldots + e_n\\\\ X_1e_1 + X_2e_2 + X_3e_3 + \\ldots + X_ne_n \\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column k in the design matrix, that \\(X_ke_k = 0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and e is zero indicating that the two vectors are independent. This is our first property:\nP.1: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on."
  },
  {
    "objectID": "files/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-regressors",
    "href": "files/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-regressors",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Regressors",
    "text": "Properties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nP.2: The sum of the sample residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^\\intercal\\mathbf{e}\\) matrix is \\(e_1+e_2+e_3+\\ldots+e_n = \\sum e_i\\), which is equal to zero since \\(\\mathbf{X}^\\intercal\\mathbf{e}=\\mathbf{0}\\).\nP.3: The mean of the sample residuals is zero.\nSince the mean of the residuals is computed as \\(\\bar{\\mathbf{e}}=\\frac{\\sum e_i}{n}\\), and the sum (numerator) is zero, then the mean is also zero.\nP.4: The regression line passes through the point \\((\\bar{X}, \\bar{Y})\\).\nRemember that \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). This means that:\n\\[\n\\begin{split}\n\\sum \\mathbf{e} &= \\sum (\\mathbf{y}-\\mathbf{Xb})\\\\[2ex]\n&= \\sum\\mathbf{y} - \\sum (\\mathbf{Xb}) \\\\[2ex]\n&= \\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})\n\\end{split}\n\\]\nIf we divide this expression by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum \\mathbf{e}}{n} &= \\frac{\\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n&= \\frac{\\sum\\mathbf{y}}{n} - \\frac{\\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n\\bar{\\mathbf{e}} &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\]\nBut, the mean of the residuals is zero, so:\n\\[\n\\begin{split}\n0 &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\] That is, the predicted y-value when the mean of X is used as a predictor is the mean of Y. In other words, the point \\((\\bar{X}, \\bar{Y})\\) is on the regression line.\nP.5: The predicted y-values are uncorrelated with the sample residuals.\nSince \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\) then \\(\\hat{\\mathbf{y}}^\\intercal=(\\mathbf{Xb})^\\intercal\\). If we post-multiply both sides of this expression by the residual vector e, we get:\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} &= (\\mathbf{Xb})^\\intercal \\mathbf{e} \\\\[2ex]\n&= \\mathbf{b}^\\intercal \\mathbf{X}^\\intercal \\mathbf{e}\n\\end{split}\n\\]\nSince \\(\\mathbf{X}^\\intercal \\mathbf{e}=\\mathbf{0}\\), then \\(\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} =0\\). This implies that \\(\\hat{\\mathbf{y}}\\) and e are uncorrelated.\nP.6: The mean of the predicted y-values is equal to the mean of the observed y-values.\nWe can make use of the fact that \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\). Taking the sum of both sides of the expression and dividing by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum\\mathbf{y}}{n} &= \\frac{\\sum(\\hat{\\mathbf{y}} + \\mathbf{e})}{n} \\\\[2ex]\n&= \\frac{\\sum\\hat{\\mathbf{y}}}{n} + \\frac{\\sum\\mathbf{e}}{n} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}} + \\bar{\\mathbf{e}} \\\\[2ex]\n&= \\bar{\\hat{\\mathbf{y}}} + 0 \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}}\n\\end{split}\n\\]\n\nIMPORTANT\nThese properties will always be true. They do not rely on any distributional assumptions of the residuals. Furthermore, these properties do not tell us anything about how “good” the coefficient estimates (b) are. Nor do these properties allow us to make inferences about the true parameters (\\(\\boldsymbol\\beta\\))."
  },
  {
    "objectID": "files/01-03-assumptions-and-gauss-markov-theorem.html",
    "href": "files/01-03-assumptions-and-gauss-markov-theorem.html",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "",
    "text": "One reason that OLS estimation is so useful is that, under a certain set of assumptions underlying the classical linear regression model, the estimators \\(b_0,b_1, b_2,\\ldots,b_k\\) have several desirable statistical properties. These properties include:"
  },
  {
    "objectID": "files/01-03-assumptions-and-gauss-markov-theorem.html#assumptions-of-the-ols-regression-model",
    "href": "files/01-03-assumptions-and-gauss-markov-theorem.html#assumptions-of-the-ols-regression-model",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "Assumptions of the OLS Regression Model",
    "text": "Assumptions of the OLS Regression Model\nAs mentioned, there are a certain set of assumptions underlying the linear regression model (both the simple and multiple regression models) for these properties to be true. These assumptions are:\n\nA.1: The model is correctly specified.\nA.2: The design matrix, X, is of full rank.\nA.3: The population errors given X have a mean of zero.\nA.4: The population errors given X are homoscedastic.\nA.5: The population errors given X are independent.\nA.6: The predictor values are fixed with finite, non-zero variance.\n\nIf these six assumptions are satisfied, then the estimators will have the properties we referred to previously. This is sometimes referred to as the weak classical regression model.\nAnother assumption that is useful is:\n\nA.7: The population errors given X are normally distributed.\n\nIf all seven assumptions are met, we refer to this as the strong classical regression model. When this assumption is met (in addition to the six other assumptions), the sampling distribution for the least squares estimators are also normally distributed; they are approximately normal under other conditions, especially with large sample sizes. This is useful for carrying out statistical inference. Furthermore, under the full set of seven assumptions, the least squares estimators are the maximum-likelihood estimators of the population coefficients.\nWe will now examine each of the assumptions underlying the linear regression model.\nA.1: The model is correctly specified.\nWhen we posit or fit the model \\(\\mathbf{y}=\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\), we are assuming that there is a linear relationship between the predictor(s) and the outcome. Furthermore, we are stating that this model is correctly specified using the set of predictors included and that deviation from this model in the observed data is all due to random sampling error.\nA.2: The design matrix, X, is of full rank.\nThis assumption indicates that there is no perfect multicollinearity in the predictor space. That is, the rows (or columns) of X are linearly independent. This is what allows us to compute \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\nA.3: The population errors given X have a mean of zero.\nThis assumption states that the mean error (in the population) at a given x-value is zero. Using our rules of expectation:\n\\[\n\\begin{split}\n\\mathbb{E}(\\mathbf{e} | \\mathbf{X}) &= \\mathbb{E}\\begin{bmatrix}e_1 | \\mathbf{X} \\\\ e_2 | \\mathbf{X}\\\\ e_3 | \\mathbf{X}\\\\ \\vdots \\\\e_n | \\mathbf{X}\\end{bmatrix}  \\\\[2ex]\n&= \\begin{bmatrix}\\mathbb{E}(e_1 | \\mathbf{X}) \\\\\\mathbb{E}( e_2 | \\mathbf{X})\\\\ \\mathbb{E}(e_3 | \\mathbf{X})\\\\ \\vdots \\\\\\mathbb{E}(e_n | \\mathbf{X})\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}0 \\\\ 0\\\\ 0\\\\ \\vdots 0\\end{bmatrix}\n\\end{split}\n\\] This assumption implies that \\(\\mathbb{E}(\\mathbf{y})=\\mathbf{X}\\boldsymbol\\beta\\).\nA.4: The population errors given X are homoscedastic.\nThis assumption indicates that residuals at a given value of X have equal variances (homoskedasticity). To show this, we make use of our rules of expectations and the fact that the variance-covariance matrix of the errors at a given X (denoted as \\(\\sum(\\epsilon|\\mathbf{X})\\)) can be defined as the expected value of \\(\\boldsymbol\\epsilon^\\intercal\\boldsymbol\\epsilon\\).\n\\[\n\\begin{split}\n\\mathbb{E}(\\epsilon|\\mathbf{X}) &= \\mathbb{E}(\\boldsymbol\\epsilon\\boldsymbol\\epsilon^\\intercal|\\mathbf{X})  \\\\[2ex]\n&= \\mathbb{E}\\bigg(\\begin{bmatrix}\\epsilon_1|\\mathbf{X} \\\\ \\epsilon_2|\\mathbf{X} \\\\ \\epsilon_3|\\mathbf{X} \\\\ \\vdots \\\\ \\epsilon_n|\\mathbf{X}\\end{bmatrix} \\begin{bmatrix} \\epsilon_1|\\mathbf{X} & \\epsilon_2|\\mathbf{X} & \\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_n|\\mathbf{X}\\end{bmatrix}\\bigg) \\\\[2ex]\n&= \\mathbb{E}\\begin{bmatrix}\\epsilon_1^2|\\mathbf{X} & \\epsilon_1\\epsilon_2|\\mathbf{X} & \\epsilon_1\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_1\\epsilon_n|\\mathbf{X} \\\\\n\\epsilon_2\\epsilon_1|\\mathbf{X} &\\epsilon_2^2|\\mathbf{X} & \\epsilon_2\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_2\\epsilon_n|\\mathbf{X} \\\\\n\\epsilon_3\\epsilon_1|\\mathbf{X} & \\epsilon_3\\epsilon_2|\\mathbf{X} & \\epsilon_3^2|\\mathbf{X} & \\ldots & \\epsilon_3\\epsilon_n|\\mathbf{X} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n\\epsilon_1|\\mathbf{X} & \\epsilon_n\\epsilon_2|\\mathbf{X} & \\epsilon_n\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_n^2|\\mathbf{X}\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\\mathbb{E}(\\epsilon_1^2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_1\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_2\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_2^2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_2\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_2\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_3\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3^2|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_3\\epsilon_n|\\mathbf{X}) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(\\epsilon_n\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_n^2|\\mathbf{X})\n\\end{bmatrix}\n\\end{split}\n\\]\nThe elements along the main diagonal are the error variances. For example \\(\\mathbb{E}(\\epsilon_i^2|\\mathbf{X})\\) is the variance of the ith residual. To show that this is the case we use the rules of expectations:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\epsilon_i|\\mathbf{X}) &= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) - \\big[\\mathbb{E}(\\epsilon_i|\\mathbf{X})\\big]^2 \\\\[2ex]\n&= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) - 0 \\\\[1ex]\n&= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X})\n\\end{split}\n\\]\nThe homoskedasticity assumption makes each variance in the matrix equal, but unknown. Because the value of the variance is unknown, we can denote it as such using the placeholder \\(\\sigma^2_\\epsilon\\); that is, \\(\\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) = \\sigma^2_\\epsilon\\). Using this to re-write our variance-covariance matrix as:\n\\[\n\\mathbb{E}(\\epsilon|\\mathbf{X}) = \\begin{bmatrix}\\sigma^2_\\epsilon & \\mathbb{E}(\\epsilon_1\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_1\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_2\\epsilon_1|\\mathbf{X}) & \\sigma^2_\\epsilon & \\mathbb{E}(\\epsilon_2\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_2\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_3\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3\\epsilon_2|\\mathbf{X}) & \\sigma^2_\\epsilon & \\ldots & \\mathbb{E}(\\epsilon_3\\epsilon_n|\\mathbf{X}) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(\\epsilon_n\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_3|\\mathbf{X}) & \\ldots & \\sigma^2_\\epsilon\n\\end{bmatrix}\n\\]\nA.5: The population errors given X are independent.\nThe off-diagonal elements in the variance-covariance matrix of the errors are the covariances between the errors. For example, \\(\\mathbb{E}(\\epsilon_i\\epsilon_j|\\mathbf{X})\\) is the covariance between the ith and jth errors. We can show this using rules of expectations:\n\\[\n\\begin{split}\n\\mathbb{E}(\\epsilon|\\mathbf{X}) &= \\begin{bmatrix}\\sigma^2_\\epsilon & 0 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2_\\epsilon & 0 & \\ldots & 0 \\\\\n0 & 0 & \\sigma^2_\\epsilon & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & \\sigma^2_\\epsilon\n\\end{bmatrix} \\\\[2ex]\n&= \\sigma^2_\\epsilon \\begin{bmatrix}1 & 0 & 0 & \\ldots & 0 \\\\\n0 & 1 & 0 & \\ldots & 0 \\\\\n0 & 0 & 1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1\n\\end{bmatrix} \\\\[2ex]\n&= \\sigma^2_\\epsilon \\mathbf{I}\n\\end{split}\n\\]\n\nNOTATION\nThe assumptions of homoskedasticity and independence can be more compactly expressed as: \\(\\mathbb{E}(\\epsilon|\\mathbf{X})=\\sigma^2_\\epsilon \\mathbf{I}\\)\n\nA.6: The predictor values are fixed with finite, non-zero variance.\nFixing the predictor values implies that the values in the X matrix are the same under repeated sampling from the same population. In most research in the social sciences, this is not the case. In those cases, we instead assume that the values in X are measured without error and that they are independent (uncorrelated) with the errors; \\(\\mathrm{Cov}(\\mathbf{X}, \\boldsymbol\\epsilon) = 0\\)."
  },
  {
    "objectID": "files/01-03-assumptions-and-gauss-markov-theorem.html#gauss-markov-theorem",
    "href": "files/01-03-assumptions-and-gauss-markov-theorem.html#gauss-markov-theorem",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\nThe Gauss–Markov Theorem is a powerful theorem that states that under the weak classical model (A.1–A.6), the least squares estimators have certain desirable properties. These properties are that the estimators are:\n\nLinear functions of the observations, \\(y_i\\);\nUnbiased estimators of the population coefficients;\nThe most efficient (smallest sampling variance) unbiased linear estimators of the population coefficients.\n\n\nFYI\nBecause of this theorem, we typically refer to the OLS coefficents as BLUE (Best Linear Unbiased Estimators).\n\nFox (2016) reminds us that the “best” in BLUE means that they have the smallest sampling variance of all the possible linear unbiased estimators. There may be a biased or non-linear estimator that produces a smaller sampling variance than the OLS estimator. It is also worth noting that if we also invoke the normality assumption (A.7), then the OLS estimators become “best” among all unbiased estimators (both linear and non-linear).\nProving this theorem is beyond the scope of the class, but an outline for this proof would entail:\n\nShow that \\(b_0, b_1, b_2, \\ldots, b_n\\) are linear functions of the observations; that is, we can express each estimator as \\(b_k = \\sum w_iy_i\\) for some \\(w_i\\).\nShow that \\(b_0, b_1, b_2, \\ldots, b_n\\) are unbiased; that \\(\\mathbb{E}(b_k)=\\beta_k\\) for each of the k estimators\nShow that for any other unbiased linear estimator for \\(b_k\\), say \\(L_k\\), that \\(\\mathrm{Var}(b_k) &lt; \\mathrm{Var}(L_k)\\).\n\nA.7: The population errors given X are normally distributed.\nA final assumption that we make about normality is not required to prove the Gauss-Markov Theorem, but it is used to carry out hypothesis testing. This assumption states that the distribution of errors (in the population) at each X is normally distributed. If we combine this with the property that the mean error given X is zero, and the homoskedasticity assumption, then:\n\\[\n\\boldsymbol\\epsilon | \\mathbf{X} \\sim \\mathcal{N}(0, ~\\sigma^2_\\epsilon\\mathbf{I})\n\\]\nThis encapsulates the assumptions we check about the probability distribution of the model errors!"
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html",
    "title": "Statistical Inference for the Regression Model",
    "section": "",
    "text": "Recall that there are a certain set of assumptions underlying the linear regression model (both the simple and multiple regression models). These assumptions are:\nAnother assumption that is useful for inference is:"
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#sampling-distribution-of-the-ols-estimators",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#sampling-distribution-of-the-ols-estimators",
    "title": "Statistical Inference for the Regression Model",
    "section": "Sampling Distribution of the OLS Estimators",
    "text": "Sampling Distribution of the OLS Estimators\nWhen X is fixed, the b vector can be written as a linear transformation of the response vector y:\n\\[\n\\begin{split}\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n&= \\mathbf{My}\n\\end{split}\n\\]\nwhere \\(\\mathbf{M} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\).\nPreviously we showed that the estimators in b are unbiased estimates of \\(\\boldsymbol\\beta\\), that is \\(\\mathbb{E}(\\mathbf{b})=\\boldsymbol\\beta\\). We can also define the variance–covariance matrix of b as:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\mathbf{b}) &= \\mathbf{M}\\mathrm{Var}(\\mathbf{y})\\mathbf{M}^\\intercal \\\\[2ex]\n&= \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]\\sigma^2_\\epsilon\\mathbf{I} \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]^\\intercal\n\\end{split}\n\\]\nRearranging this and using our rules of transposes and inverses, we get:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\mathbf{b}) &= \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]\\sigma^2_\\epsilon\\mathbf{I} \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]^\\intercal \\\\[2ex]\n&= \\sigma^2_\\epsilon (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_\\epsilon (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\end{split}\n\\]\nThis implies that the sampling variances and covariances of the estimators depend only on the predictor values and the error variance. This matrix is often referred to as V.\nNote that to derive this, we only need the assumption of linearity. The normality assumption is not used to compute the mean (expectation) nor the sampling variance, nor covariances of the estimators. However, if the errors (and hence y) are normally distributed (i.e., assumption A.7), then so are the sampling distributions of the estimators. We can express these distributions as:\n\\[\n\\mathbf{b} \\sim \\mathcal{N}\\bigg(\\boldsymbol\\beta,~ \\sigma^2_\\epsilon(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg)\n\\]"
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#inference-for-the-estimators",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#inference-for-the-estimators",
    "title": "Statistical Inference for the Regression Model",
    "section": "Inference for the Estimators",
    "text": "Inference for the Estimators\nAn individual estimator, \\(b_j\\), has a sampling distribution of:\n\\[\nb_j \\sim \\mathcal{N}\\bigg(\\beta_j,~ \\mathbf{V}_{jj}\\bigg)\n\\]\nwhere \\(\\mathbf{V}_{jj}\\) is the element in the jth row and jth column of the variance–covariance matrix for b. To test the hypothesis that:\n\\[\nH_0: \\beta_j = \\beta_j^{(0)}\n\\]\nwhere \\(\\beta_j^{(0)}\\) is some value (e.g., \\(H_0: \\beta_j = 0\\)), we compute the ratio:\n\\[\nZ_0 = \\frac{b_j - \\beta_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nwhich is unit-normal distributed; \\(\\mathcal{N}(0, 1)\\). Note the denominator is the standard deviation of the sampling distribution for \\(b_j\\).\nUnfortunately, in practice we do not know \\(\\sigma_\\epsilon\\) to compute V. Instead, we substitute in the estimate for this value from our sample, the unbiased estimator \\(s_e\\), where,\n\\[\ns_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\n\\]\nwhere n is the sample size, k is the number of predictors in the model, and \\(n − k − 1\\) is the residual degrees of freedom for the model. Thus, we get an estimate of the variance–covariance matrix using:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Var}(\\mathbf{b})} &= s^2_e (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\\\[2ex]\n&= \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\end{split}\n\\]\nThe standard error for \\(b_j\\) is then estimated using the jth diagonal element of this matrix, namely \\(\\sqrt{\\hat{v}_{jj}}\\).\n\nMATH NOTE\nThere is a theorem which says that (1) if Z is a standard normal variable and W is chi-squared distributed with \\(\\nu\\) degrees of freedom, and (2) Z and W are independent, then T, defined as:\n\\[\nT = \\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\]\nwill follow a t-distribution with \\(\\nu\\) degrees of freedom.\n\nSince it can be shown that \\((n−𝑘− 1)\\frac{s^2_e}{\\sigma^2_\\epsilon}\\) follows a chi-squared distribution with \\(n-l-1\\) degrees of freedom, and also that \\(b_j\\) and \\(s^2_e\\) are independent; it follows that:\n\\[\nT = \\frac{b_j - b_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nis t-distributed with \\(n-k-1\\) degrees of freedom. To evaluate the hypothesis that \\(H_0∶ \\beta_j = \\beta_j^{(0)}\\), we compute the test statistic:\n\\[\nt_0 = \\frac{b_j - b_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nand evaluate it within the t-distribution having \\(n-k-1\\) degrees of freedom (where n is the sample size, and k is the number of predictors in the model)."
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#confidence-intervals",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#confidence-intervals",
    "title": "Statistical Inference for the Regression Model",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA \\(1−\\alpha\\)% confidence interval can also be constructed for each coefficient using:\n\\[\n\\begin{split}\n\\mathrm{CI}_{1−\\alpha} &= b_j \\pm |t^*_{\\alpha/2}| \\times \\bigg(s_e (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg) \\\\[2ex]\n&= b_j \\pm |t^*_{\\alpha/2}| \\times \\sqrt{\\mathbf{V}_{jj}}\n\\end{split}\n\\]\nwhere \\(t^*_{\\alpha/2}\\) is the critical value demarcating the area in the \\(\\alpha/2\\) proportion of the distribution in the t-distribution with \\(n-k-1\\) degrees of freedom. For example, if we wanted to compute a 95% CI, then \\(\\alpha = 0.05\\) and \\(t^*_{.025}\\) would be the critical value that demarcates the lowest 0.025 of the t-distribution."
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#model-level-inference",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#model-level-inference",
    "title": "Statistical Inference for the Regression Model",
    "section": "Model-Level Inference",
    "text": "Model-Level Inference\nAt the model-level we are interested in testing the hypothesis \\(H_0∶ \\rho^2 = 0\\). Recall this is equivalent to testing the hypothesis that all the regression parameters (except the intercept) in the model are zero:\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3=\\ldots = \\beta_k = 0\n\\]\nThis is a specific form of the general linear hypothesis which can be expressed as:\n\\[\nH_0: \\mathbf{L}\\boldsymbol\\beta = \\mathbf{c}\n\\]\nwhere,\n\nL is a \\(q \\times (k + 1)\\) matrix referred to as the hypothesis matrix, where q is the number of parameters being tested;\n\\(\\boldsymbol\\beta\\) is a \\((k + 1) \\times 1\\) vector of parameters included in the model; and\nc is a \\(q \\times 1\\) vector of hypothesized values\n\nAs an example, if we wanted to test the model-level null hypothesis in a two predictor model (\\(y_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\epsilon_i\\)),\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\nThen the general linear hypothesis could be expressed as:\n\\[\nH_0: \\begin{bmatrix}0 & 1 & 0 \\\\0 & 0 & 1\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\end{bmatrix} = \\begin{bmatrix}0 \\\\0\\end{bmatrix}\n\\]\nThis results in:\n\\[\nH_0: \\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} = \\begin{bmatrix}0 \\\\0\\end{bmatrix}\n\\]\nWe can construct a test statistic, \\(F_0\\), as:\n\\[\nF_0 = \\frac{(\\mathbf{Lb} − \\mathbf{c})^\\intercal[\\mathbf{L}(\\mathbf{X}^\\intercal\\mathbf{X})^{−1}\\mathbf{L}^\\intercal]^{−1}(\\mathbf{Lb} − \\mathbf{c})}{q(s^2_e)}\n\\]\nThis follows an F-distribution with q and \\(n-k-1\\) degrees of freedom."
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#implications-for-applied-researchers",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#implications-for-applied-researchers",
    "title": "Statistical Inference for the Regression Model",
    "section": "Implications for Applied Researchers",
    "text": "Implications for Applied Researchers\nIf the assumptions underlying the strong classical regression model (A.1–A.7) are all valid, then the OLS estimators \\(b_0, b_1, \\ldots, b_k\\) are good estimators of the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\). They are unbiased and efficient and have accurate sampling variances and covariance (i.e., they are BLUE).\nOf course, any of the assumptions may be challenged either on a priori substantive grounds, or post hoc, via empirical examination of the sample residuals. If one (or more) of the assumptions are violated, then some of the properties may be compromised. Let’s look at violation in turn:\nViolating A.1: The model is not correctly specified.\nThis is probably the most egregious and costly violation. Violating this assumption means that the theoretical model embodied in the regression equation is wrong. If this is the case, there is no use proceeding.\nViolating A.2: The design matrix, X, is not of full rank.\nThis would mean that we cannot compute an inverse for the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. Subsequently, we could not compute any parameter estimates nor SEs.\nViolating A.3: The population errors given X do not have a mean of zero.\nThe estimator \\(b_0\\) (the intercept) will be biased when this assumption is violated. However, the other regression estimators (e.g., \\(b_0, b_1, \\ldots, b_k\\)) are still BLUE. Unfortunately, we can never test this assumption in practice since \\(\\sum e_i =0\\) always holds when using OLS.\nViolating A.4: The population errors given X are heteroscedastic.\nIf the assumption of constant variance is violated, the coefficient estimators will remain unbiased. However, the estimates for the variances/standard errors for these coefficients will be wrong. This also impacts the confidence intervals and hypothesis tests for these parameters. There are solutions to this problem, including using a different estimation method (e.g., weighted least squares) or estimating the sampling variances for these coefficients differently (e.g., sandwich estimation).\nViolating A.5: The population errors given X are not independent.\nIf the errors are correlated (not independent), the coefficient estimators will still be unbiased. Again, however, the variances/standard errors and results from the confidence intervals and hypothesis tests will be wrong. Often the hypothesis tests will indicate statistical significance much more often then it should (increased probability of a type I error). Dealing with this problem requires using models that account for the correlation (e.g., mixed-effects models) and different estimation methods (e.g., maximum likelihood).\nViolating A.6: The predictor values are not fixed.\nWhen X is fixed, the predictors are uncorrelated with the error terms. This is what leads to unbiased estimates of both the estimators and the variances/standard errors. When X is random (which is almost always the case in observational data), we need to assume that \\(\\mathrm{Cov}(\\mathbf{X}, \\boldsymbol\\epsilon) = \\mathbf{0}\\). That is we assume that the predictor values are generated by a mechanism unrelated to the errors. If that is not the case, then the OLS estimates will be biased\nViolating A.7: The population errors given X are not normally distributed.\nViolation of the normality assumption causes the least number of problems. Under non-normality the regression estimators are still BLUE, and \\(s^2_e\\) is still an unbiased estimator of \\(\\sigma^2_\\epsilon\\). However, the under non-normality, the use of the F- and t-distributions for inference is questionable, especially if the sample size is small. If the sample size is large, the sampling distributions of the coefficients are approximately normal (i.e., Central Limit Theorem) and the use of the F- and t-distributions for inference is justified."
  },
  {
    "objectID": "files/01-04-statistical-inference-for-the-regression-model.html#references",
    "href": "files/01-04-statistical-inference-for-the-regression-model.html#references",
    "title": "Statistical Inference for the Regression Model",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "files/01-05-regression-example.html",
    "href": "files/01-05-regression-example.html",
    "title": "A Regression Example",
    "section": "",
    "text": "In this document we will use the data in contraception.csv to examine whether female education level explains variation in contraceptive useage after controlling for GNI.\n# Load libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\n\n# Import data\ncontraception = read_csv(\"https://github.com/zief0002/epsy-8264/raw/master/data/contraception.csv\")\n\n# View data\ncontraception\n\n# A tibble: 97 × 5\n   country                region                 contraceptive educ_female gni  \n   &lt;chr&gt;                  &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;\n 1 Algeria                Middle East and North…            57         5.9 High \n 2 Austria                Europe and Central As…            66         8.9 High \n 3 Azerbaijan             Europe and Central As…            55        10.5 High \n 4 Bangladesh             South Asia                        62         4.6 Low  \n 5 Belgium                Europe and Central As…            67        10.5 High \n 6 Belize                 Latin America and the…            51         9.2 High \n 7 Benin                  Sub-Saharan Africa                16         2   Low  \n 8 Bolivia                Latin America and the…            67         8.4 Low  \n 9 Bosnia and Herzegovina Europe and Central As…            46         7.2 High \n10 Botswana               Sub-Saharan Africa                53         8.7 High \n# ℹ 87 more rows\n\n# IF you want to see all the variables\n#contraception |&gt; print(width = Inf)"
  },
  {
    "objectID": "files/01-05-regression-example.html#examine-the-data",
    "href": "files/01-05-regression-example.html#examine-the-data",
    "title": "A Regression Example",
    "section": "Examine the Data",
    "text": "Examine the Data\nWe need to correctly specify the model. Since we have no theory to guide us, this is done empirically by looking at the data.\n\n# Create density plot of contraception\np1 = ggplot(data = contraception, aes(x = contraceptive)) + \n   geom_density() +\n   theme_bw() +\n   labs(\n      x = \"Contraceptive useage\"\n   ) \n\n# Create density plot of female education level\np2 = ggplot(data = contraception, aes(x = educ_female)) + \n   geom_density() +\n   theme_bw() +\n   labs(\n      x = \"Female education level\"\n   ) \n\n\n# Condition the relationship on GNI\np3 = ggplot(data = contraception, aes(x = educ_female, y = contraceptive, color = gni)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) +\n   facet_wrap(~gni)\n\n# Layout plots with patchwork\n(p1 | p2) / p3\n\n\n\n\n\n\n\nFigure 1: TOP-LEFT: Density plot of contracrptive useage. TOP-RIGHT: Density plot of female education level. BOTTOM: Scatterplot of contraceptive useage versus female education level conditioned on GNI.\n\n\n\n\n\n\nShould we include main-effects only? Or an interaction?\nIs there non-linearity to account for (e.g., transformations)? Or does it look linear?"
  },
  {
    "objectID": "files/01-05-regression-example.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "href": "files/01-05-regression-example.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "title": "A Regression Example",
    "section": "Use Matrix Algebra to Compute Coefficient Estimates",
    "text": "Use Matrix Algebra to Compute Coefficient Estimates\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\n\n# Store values\nn = nrow(contraception) #Sample size\nk = 2 #Number of predictors\n\n\n# Create outcome vector\ny = contraception$contraceptive\n\n# Create dummy variable for GNI\ncontraception = contraception %&gt;%\n   mutate(\n      high_gni = if_else(gni == \"High\", 1, 0)\n      )\n\n# Create design matrix\nX = matrix(\n   data = c(rep(1, n), contraception$educ_female, contraception$high_gni),\n   ncol = 3\n)\n\n# Compute b vector\nb = solve(t(X) %*% X) %*% t(X) %*% y\nb\n\n          [,1]\n[1,] 27.021387\n[2,]  4.088735\n[3,]  1.608766\n\n\nThus the fitted regression equation is:\n\\[\n\\widehat{\\mathrm{Contraceptive~Use}}_i = 27.02 + 4.09(\\mathrm{Female~Education~Level}_i) + 1.60(\\mathrm{High~GNI}_i)\n\\]"
  },
  {
    "objectID": "files/01-05-regression-example.html#compute-residual-standard-error",
    "href": "files/01-05-regression-example.html#compute-residual-standard-error",
    "title": "A Regression Example",
    "section": "Compute Residual Standard Error",
    "text": "Compute Residual Standard Error\n\n# Compute e vector\ne = y - X %*% b\n\n# Compute s_e\ns_e = sqrt((t(e) %*% e) / (n - k - 1))\ns_e\n\n         [,1]\n[1,] 14.39792\n\n\nThus the residual standard error (a.k.a., the root mean square error; RMSE) is:\n\\[\ns_e = 14.40\n\\]"
  },
  {
    "objectID": "files/01-05-regression-example.html#compute-variancecovariance-matrix-for-the-coefficients",
    "href": "files/01-05-regression-example.html#compute-variancecovariance-matrix-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Compute Variance–Covariance Matrix for the Coefficients",
    "text": "Compute Variance–Covariance Matrix for the Coefficients\n\\[\n\\mathrm{Var}(\\mathbf{b}) = s^2_e(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\]\nwhere \\(s^2_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\\)\n\n# Compute varaince-covariance matrix of b\nV = as.numeric(s_e^2) * solve(t(X) %*% X)\nV\n\n          [,1]       [,2]      [,3]\n[1,] 12.414688 -1.8783934  4.782603\n[2,] -1.878393  0.4267136 -2.028306\n[3,]  4.782603 -2.0283060 18.197825\n\n# Compute SEs for b\nsqrt(diag(V))\n\n[1] 3.5234483 0.6532332 4.2658909\n\n\nThus\n\\[\n\\mathrm{SE}(b_0) = 3.52 \\qquad \\mathrm{SE}(b_1) = 0.65 \\qquad \\mathrm{SE}(b_2) = 4.27\n\\]"
  },
  {
    "objectID": "files/01-05-regression-example.html#coefficient-level-inference",
    "href": "files/01-05-regression-example.html#coefficient-level-inference",
    "title": "A Regression Example",
    "section": "Coefficient-Level Inference",
    "text": "Coefficient-Level Inference\nHere we will focus on the effects of female education level since it is our focal predictor. (GNI is a control.) Note this is the second effect in the b vector and in the V matrix. We will test the hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\n\n# Compute t-value\nt_0 = (b[2] - 0) / sqrt(V[2, 2])\nt_0\n\n[1] 6.259228\n\n# Evaluate t-value\ndf = n - k - 1\np = 2* (1 - pt(abs(t_0), df = df))\np\n\n[1] 0.00000001143799\n\n\nHere,\n\\[\nt(94) = 6.26,~p=0.0000000114\n\\]\nThe evidence suggests that the data are not very compatible with the hypothesis that there is no effect of female education level on contraceptive useage, after controlling for differences in GNI."
  },
  {
    "objectID": "files/01-05-regression-example.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "href": "files/01-05-regression-example.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Statistical Inference: Confidence Intervals for the Coefficients",
    "text": "Statistical Inference: Confidence Intervals for the Coefficients\nFrom the hypothesis test, we believe there is an effect of female education level on contraceptive useage, after controlling for differences in GNI. What is that effect? To answer this we will compute a 95% CI for the effect of female education.\n\n# Compute critical value\nt_star = qt(.025, df = df)\n\n# Compute CI\nb[2] - abs(t_star) * sqrt(V[2, 2])\n\n[1] 2.791725\n\nb[2] + abs(t_star) * sqrt(V[2, 2])\n\n[1] 5.385745\n\n\nThe 95% CI indicates that the population effect of female education level on contraceptive useage, after controlling for differences in GNI is between 2.79 and 5.39."
  },
  {
    "objectID": "files/01-05-regression-example.html#accessing-regression-matrices-from-lm",
    "href": "files/01-05-regression-example.html#accessing-regression-matrices-from-lm",
    "title": "A Regression Example",
    "section": "Accessing Regression Matrices from lm()",
    "text": "Accessing Regression Matrices from lm()\nThere are several built-in R functions that allow you to access different regression matrices once you have fitted a model with lm().\n\n# Access design matrix\nmodel.matrix(lm.1)\n\n   (Intercept) educ_female gniLow\n1            1         5.9      0\n2            1         8.9      0\n3            1        10.5      0\n4            1         4.6      1\n:            :          :       :\n97           1         6.7      1\nattr(,\"assign\")\n[1] 0 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gni\n[1] \"contr.treatment\"\nThe design matrix is given and information about this design matrix is also encoded. There is an attribute “assign”, an integer vector with an entry for each column in the matrix giving the term in the formula which gave rise to the column. Value 0 corresponds to the intercept (if any), and positive values to terms in the order given by the term.labels attribute of the terms structure corresponding to object. There is also an attribute called “contrasts” that identifies any factors (categorical variables) in the model and indicates how the contrast testing (comparison of the factor levels) will be carried out. Here “contr.treatment” is used. This compares each level of the factor to the baseline (which is how dummy coding works).\n\n# Access coefficient estimates\ncoef(lm.1)\n\n(Intercept) educ_female      gniLow \n  28.630153    4.088735   -1.608766 \n\n# Access variance-covariance matrix for b\nvcov(lm.1)\n\n            (Intercept) educ_female     gniLow\n(Intercept)   40.177719  -3.9066994 -22.980428\neduc_female   -3.906699   0.4267136   2.028306\ngniLow       -22.980428   2.0283060  18.197825\n\n# Access fitted values\nfitted(lm.1)\n\n       1        2        3        4        5        6        7        8 \n52.75369 65.01990 71.56187 45.82957 71.56187 66.24652 35.19886 61.36676 \n       9       10       11       12       13       14       15       16 \n58.06905 64.20215 71.97075 34.78998 36.01660 40.10534 47.87394 78.92160 \n      17       18       19       20       21       22       23       24 \n29.47463 67.88201 57.25130 35.60773 62.97553 71.56187 78.10385 60.11341 \n      25       26       27       28       29       30       31       32 \n58.88679 48.69168 51.96267 32.74562 73.19737 51.14493 69.10863 30.29238 \n      33       34       35       36       37       38       39       40 \n40.10534 48.69168 74.42399 40.10534 55.23366 46.62059 68.29089 68.69976 \n      41       42       43       44       45       46       47       48 \n74.42399 67.06427 70.33525 49.10056 74.01512 42.55858 59.70454 54.82479 \n      49       50       51       52       53       54       55       56 \n36.42548 46.64732 40.92309 66.24652 50.70932 32.74562 67.47314 61.34004 \n      57       58       59       60       61       62       63       64 \n61.74891 66.27325 61.77564 40.10534 30.29238 54.38919 36.83435 46.64732 \n      65       66       67       68       69       70       71       72 \n30.29238 44.19408 40.51421 67.88201 59.29567 63.00226 61.34004 71.15300 \n      73       74       75       76       77       78       79       80 \n39.69647 43.37633 40.92309 66.24652 35.19886 76.05948 76.87723 68.69976 \n      81       82       83       84       85       86       87       88 \n67.47314 58.47792 57.27803 67.90874 45.42070 57.25130 40.51421 49.50943 \n      89       90       91       92       93       94       95       96 \n44.60295 72.81522 80.96597 64.20215 64.20215 48.28281 31.92787 50.73605 \n      97 \n54.41591 \n\n# Access raw residuals\nresid(lm.1)\n\n          1           2           3           4           5           6 \n  4.2463087   0.9801026 -16.5618739  16.1704304  -4.5618739 -15.2465180 \n          7           8           9          10          11          12 \n-19.1988577   5.6332361 -12.0690473 -11.2021503  -2.9707474  -2.7899842 \n         13          14          15          16          17          18 \n -7.0166048  15.8946599 -13.8739373   6.0784025 -23.4746282   8.1179879 \n         19          20          21          22          23          24 \n 23.7486998 -15.6077312  15.0244703  -2.5618739   7.8961495   9.8865851 \n         25          26          27          28          29          30 \n 21.1132057  10.3083157  20.0373274   7.2543835   4.8026319 -20.1449256 \n         31          32          33          34          35          36 \n  6.8913673 -21.2923753  -6.1053401  24.3083157 -12.4239887  13.8946599 \n         37          38          39          40          41          42 \n  5.7663391   6.3794117  -3.2908856   4.3002408 -34.4239887 -15.0642650 \n         43          44          45          46          47          48 \n-15.3352533  11.8994421   5.9848849  11.4414187  -4.7045414   5.1752126 \n         49          50          51          52          53          54 \n -5.4254783   1.3526833  18.0769128 -14.2465180 -31.7093236 -16.7456165 \n         55          56          57          58          59          60 \n 18.5268614   2.6599645   5.2510909  -6.2732463  -6.7756375  30.8946599 \n         61          62          63          64          65          66 \n -3.2923753   1.6108145  16.1656482  33.3526833 -19.2923753 -16.1940755 \n         67          68          69          70          71          72 \n -6.5142136  -4.8820121   8.7043321  -9.0022581  12.6599645  -1.1530004 \n         73          74          75          76          77          78 \n 13.3035334  -2.3763284 -12.9230872  -8.2465180 -12.1988577   3.9405172 \n         79          80          81          82          83          84 \n  2.1227701 -13.6997592   3.5268614 -10.4779208   8.7219714 -38.9087405 \n         85          86          87          88          89          90 \n -7.4206961  20.7486998 -20.5142136  13.4905686  -2.6029490  -7.8152229 \n         91          92          93          94          95          96 \n  3.0340348  15.7978497  10.7978497  27.7171892   2.0721306  -1.7360520 \n         97 \n 12.5840862 \n\n\n\nPROTIP\nThe tidy() and augment() functions from the {broom} package also give the coefficients, residuals, and fitted values. The difference is in the type of out put you get. For example the residuals from resid() are outputted as a vector, whereas the residuals from augment() are outputted in a data frame/tibble. In R certain output types are better than other in different situations (e.g., {ggplot2} functions require data frames)."
  },
  {
    "objectID": "notes/01-01-summation-expectation.html",
    "href": "notes/01-01-summation-expectation.html",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "",
    "text": "Assume the \\(X\\) and \\(Y\\) are random variables and \\(c\\) is a constant, such that:\n\\[\n\\begin{split}\nX &= \\{x_1, x_2, x_3, \\ldots, x_n\\} \\\\\nY &= \\{y_1, y_2, y_3, \\ldots, y_n\\} \\\\\nc &= \\{c_1, c_2, c_3, \\ldots, c_n\\} \\qquad \\mathrm{where~} c_1= c_2= c_3= \\ldots= c_n\\ \\\\\n\\end{split}\n\\]\nThe mean of these random (and constant) variables is denoted as the expected value, namely, \\(\\mathbb{E}(X)\\), \\(\\mathbb{E}(Y)\\), and \\(\\mathbb{E}(c)\\)."
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#formula-for-variance",
    "href": "notes/01-01-summation-expectation.html#formula-for-variance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Variance",
    "text": "Formula for Variance\nOne very useful measure that we will work with a lot in the course is the variance. Here are several formulas to compute the variance of a random variable, \\(X\\). We denote the variance of \\(X\\) using \\(\\sigma^2_X\\) or \\(\\mathrm{Var}(X)\\). The most common formula for variance is:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)^2}{n}\n\\]\nWe can also compute variance as an expected value of the squared mean deviations:\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nLastly, it can sometimes be helpful to express the variance as the difference between the expected value of \\(X^2\\) and the squared expected value of \\(X\\):\n\\[\n\\sigma^2_X = \\mathrm{Var}(X) = \\mathbb{E}(X^2) -  \\big[\\mathbb{E}(X)\\big]^2\n\\]\nLastly, we note that the standard deviation is the square root of the variance:\n\\[\n\\sigma_X = \\sqrt{\\sigma^2_X} = \\sqrt{\\mathrm{Var}(X)} = \\mathrm{SD}(X)\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#formula-for-covariance",
    "href": "notes/01-01-summation-expectation.html#formula-for-covariance",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Covariance",
    "text": "Formula for Covariance\nAnother useful measure that we will be working with in the course is the covariance. We denote the covariance between \\(X\\) and \\(Y\\) using \\(\\sigma_{XY}\\) or \\(\\mathrm{Cov}(X,Y)\\). The most common formula for covariance is:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\frac{\\sum_{i=1}^n\\bigg(X_i - \\mathbb{E}(X)\\bigg)\\bigg(Y_i - \\mathbb{E}(Y)\\bigg)}{n}\n\\]\nThe covariance can also be expressed as an expectation:\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}\\bigg(\\big[X - \\mathbb{E}(X)\\big]\\big[Y - \\mathbb{E}(Y)\\big]\\bigg)\n\\]\nLastly, we can also express the covariance as a difference of expectations.\n\\[\n\\sigma_{XY} = \\mathrm{Cov}(X,Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#formula-for-correlation-coefficient",
    "href": "notes/01-01-summation-expectation.html#formula-for-correlation-coefficient",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Formula for Correlation Coefficient",
    "text": "Formula for Correlation Coefficient\nThe correlation coefficient is a standardized covariance value. We denote the correlation between \\(X\\) and \\(Y\\) using \\(\\rho_{XY}\\) or \\(\\mathrm{Cor}(X,Y)\\). The most common formula for correlation is:\n\\[\n\\rho_{XY} = \\mathrm{Cor}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(Y)}}\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#rules-for-working-with-sums",
    "href": "notes/01-01-summation-expectation.html#rules-for-working-with-sums",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Sums",
    "text": "Rules for Working with Sums\nThe sum of \\(X\\) is defined as,\n\\[\n\\sum_{i=1}^n X_i = x_1 + x_2 + x_3 + \\ldots + x_n\n\\]\nTo keep the notation simpler, we will just denote this as \\(\\sum X\\).\nRule 1: When a summation is itself a sum or difference, the summation sign may be distributed among the separate terms of the sum. That is:\n\\[\n\\sum(X + Y) = \\sum X + \\sum Y\n\\]\nRule 2: The sum of a constant, \\(c\\), is \\(n\\) times the value of the constant.\n\\[\n\\sum(c) = nc\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#rules-for-working-with-expectations-means",
    "href": "notes/01-01-summation-expectation.html#rules-for-working-with-expectations-means",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Expectations (Means)",
    "text": "Rules for Working with Expectations (Means)\nThe expectation (mean) of \\(X\\) is defined as,\n\\[\n\\mathbb{E}(X) = \\frac{\\sum_{i=1}^n X_i}{n}\n\\]\nAgain, to keep the notation simpler, we will just denote this as \\(\\mathbb{E}(X) = \\frac{\\sum X}{n}\\).\nRule 1: The expectation of a constant, \\(c\\), is the constant.\n\\[\n\\mathbb{E}(c) = c\n\\]\nRule 2: Adding a constant value, \\(c\\), to each term in a random variable, \\(X\\), increases the expected value (or mean) of \\(X\\) by the constant.\n\\[\n\\mathbb{E}(X + c) = \\mathbb{E}(X) + c\n\\]\nRule 3: Multiplying a random variable, \\(X\\), by a constant value, \\(c\\), multiplies the expected value (or mean) of \\(X\\) by that constant.\n\\[\n\\mathbb{E}(cX) = c\\bigg(\\mathbb{E}(X)\\bigg)\n\\]\nRule 4: The expected value (or mean) of the sum of two random variables, \\(X\\) and \\(Y\\) is the sum of the expected values (or means). This is also known as the additive law of expectation.\n\\[\n\\mathbb{E}(X + Y) = \\mathbb{E}(X) + \\mathbb{E}(Y)\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#rules-for-working-with-variances",
    "href": "notes/01-01-summation-expectation.html#rules-for-working-with-variances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Variances",
    "text": "Rules for Working with Variances\nRule 1: The variance of a constant, \\(c\\), is zero.\n\\[\n\\mathrm{Var}(c) = 0\n\\]\nRule 2: Adding a constant value, \\(c\\), to a random variable, \\(X\\) does not change the variance of \\(X\\).\n\\[\n\\mathrm{Var}(X+c) = \\mathrm{Var}(X)\n\\]\nRule 3: Multiplying a random variable, \\(X\\) by a constant, \\(c\\) increases the variance of \\(X\\) by the square of the constant.\n\\[\n\\mathrm{Var}(cX) = c^2 \\times \\mathrm{Var}(X)\n\\]\nRule 4: The variance of the sum of two random variables, \\(X\\) and \\(Y\\) is equal to the sum of their variances and the covariance between them.\n\\[\n\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\mathrm{Cov}(X,Y)\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#rules-for-working-with-covariances",
    "href": "notes/01-01-summation-expectation.html#rules-for-working-with-covariances",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Covariances",
    "text": "Rules for Working with Covariances\nRule 1: The covariance of two constants, \\(c\\) and \\(k\\), is zero.\n\\[\n\\mathrm{Cov}(c,k) = 0\n\\]\nRule 2: The covariance of two independent random variables is zero.\n\\[\n\\mathrm{Cov}(X,Y) = 0\n\\]\nRule 3: The covariance is a combinative.\n\\[\n\\mathrm{Cov}(X,Y) = \\mathrm{Cov}(Y,X)\n\\]\nRule 4: The covariance of a random variable, \\(X\\), with a constant, \\(c\\) is zero.\n\\[\n\\mathrm{Cov}(X,c) = 0\n\\]\nRule 5: Adding a constant to either or both random variables does not change their covariances.\n\\[\n\\mathrm{Cov}(X+c,Y+k) = \\mathrm{Cov}(X,Y)\n\\]\nRule 6: Multiplying a random variable by a constant multiplies the covariance by that constant.\n\\[\n\\mathrm{Cov}(cX,kY) = c \\times k \\times \\mathrm{Cov}(X,Y)\n\\]\nRule 7: The additive law of covariance holds that the covariance of a random variable with a sum of random variables is just the sum of the covariances with each of the random variables.\n\\[\n\\mathrm{Cov}(X+Y, Z) = \\mathrm{Cov}(X,Z) + \\mathrm{Cov}(Y,Z)\n\\]\nRule 8: The covariance of a variable with itself is the variance of the random variable.\n\\[\n\\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n\\]"
  },
  {
    "objectID": "notes/01-01-summation-expectation.html#rules-for-working-with-correlation-coefficients",
    "href": "notes/01-01-summation-expectation.html#rules-for-working-with-correlation-coefficients",
    "title": "Summation, Expectation, Variance, Covariance, and Correlation",
    "section": "Rules for Working with Correlation Coefficients",
    "text": "Rules for Working with Correlation Coefficients\nRule 1: Adding a constant to a random variable does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(X+c, Y+k) = \\mathrm{Cor}(X, Y)\n\\]\nRule 2: Multiplying a random variable by a constant does not change their correlation coefficient.\n\\[\n\\mathrm{Cor}(cX, dY) = \\mathrm{Cor}(X, Y)\n\\]\nRule 3: Because the square root of the variance is always positive, the correlation coefficient can be negative only when the covariance is negative. This implies that:\n\\[\n-1 \\leq \\mathrm{Cor}(X, Y) \\leq 1\n\\]"
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html",
    "href": "notes/01-02-ols-estimators-and-their-properties.html",
    "title": "OLS Estimators and Their Properties",
    "section": "",
    "text": "We have previously defined the population regression model (using scalar algebra) as:\n\\[\ny_i = \\beta_0 + \\beta_1(x_i) + \\epsilon_i\n\\]\nwhere the outcome (y) is assumed to be statistically and linearly related to the predictor (x) and the error term, \\(\\epsilon\\), is a random variable.\nRecall that the least squares estimators can be analytically computed as:\n\\[\n\\begin{split}\nb_1 &= \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} \\\\[1em]\nb_0 &= \\bar{y} - b_1(\\bar{x}) \\\\[1em]\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html#estimating-the-regression-coefficients",
    "href": "notes/01-02-ols-estimators-and-their-properties.html#estimating-the-regression-coefficients",
    "title": "OLS Estimators and Their Properties",
    "section": "Estimating the Regression Coefficients",
    "text": "Estimating the Regression Coefficients\nIn a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the y and x values). The estimates of the regression parameters are denoted using the roman letters \\(b_0\\) and \\(b_1\\) and the vector of these sample estimates are denoted as b. Similarly the sample residuals are denoted as e. (It is common to refer to the population errors as “errors” and the sample estimates as “residuals”.) Thus, the sample equivalent of the model is:\n\\[\n\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\n\\]\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE). Using scalar algebra, the SSE can be expressed as: \\(\\mathrm{SSE}=\\sum e^2_i\\). The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE}&=\\mathbf{e}^\\intercal\\mathbf{e} \\\\\n&= \\begin{bmatrix}e_1 & e_2 & e_3 & \\ldots & e_n \\end{bmatrix}\\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix}\n\\end{split}\n\\]\nRe-arranging the sample regression equation, we can express the residual vector e as \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). The SSE can then be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y} − \\mathbf{Xb})^\\intercal(\\mathbf{y} − \\mathbf{Xb})\n\\] This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^\\intercal\\mathbf{y} − \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y}-\\mathbf{y} ^\\intercal\\mathbf{Xb} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\\\\[2ex]\n&= \\mathbf{y}^\\intercal\\mathbf{y} − 2\\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{y} + \\mathbf{b}^\\intercal\\mathbf{X}^\\intercal\\mathbf{Xb}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b:\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\nThis expression is referred to as the set of Normal Equations. Note that the \\((\\mathbf{X}^\\intercal\\mathbf{X})\\) matrix has two important properties:\n\nIt is square; and\nIt is symmetric.\n\nTo solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{Ib} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\end{split}\n\\]\nAs long as \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\) exists, the vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y})\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals."
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html#extending-the-model",
    "href": "notes/01-02-ols-estimators-and-their-properties.html#extending-the-model",
    "title": "OLS Estimators and Their Properties",
    "section": "Extending the Model",
    "text": "Extending the Model\nUsing matrix algebra to compute the OLS regression coefficients gives us the same values as using the analytic formulas. So why use matrix algebra? The simple reason is that we can use the same matrix algebra computation of b regardless of how many predictors we include in the model (it is extensible). The analytic formulas change and become quite difficult to manipulate. For example, consider an example where we want to estimate the coefficients for a model that includes two main effects (\\(x_1\\) and \\(x_2\\)) and an interaction between these effects. The population model written in scalar algebra is:\n\\[\ny_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\beta_3(x_{1i})(x_{2i}) + \\epsilon_i\n\\]\nIf we express this using matrix notation, we get:\n\\[\n\\begin{split}\n\\mathbf{y} &= \\mathbf{Xb} + \\mathbf{e}\n\\end{split}\n\\]\nAdding predictors expands the size of the design matrix and the length of the \\(\\boldsymbol\\beta\\) matrix, but the compact notation \\(\\mathbf{y} = \\mathbf{Xb} + \\mathbf{e}\\) is exactly the same, so estimating the values in the b vector for multiple regression models is identical to doing so for the simple regression model!"
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-estimators",
    "href": "notes/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-estimators",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Estimators",
    "text": "Properties of the OLS Estimators\nOne property of the OLS estimators (in simple or multiple regression) is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. (Note: We derive these properties for the simple regression model, but they also can be extended for the multiple regression model.) Remember these estimators are based on the normal equations:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} = \\mathbf{X}^\\intercal\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n(\\mathbf{X}^\\intercal\\mathbf{X})\\mathbf{b} &= \\mathbf{X}^\\intercal(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^\\intercal\\mathbf{Xb}+\\mathbf{X}^\\intercal\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^\\intercal\\mathbf{e} = \\mathbf{0}\n\\]\nLet’s examine this:\n\\[\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} 1 & 1 & 1 & \\ldots & 1\\\\ X_1 & X_2 & X_3 & \\ldots & X_n \\end{bmatrix} \\begin{bmatrix} e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix} e_1 + e_2 + e_3 + \\ldots + e_n\\\\ X_1e_1 + X_2e_2 + X_3e_3 + \\ldots + X_ne_n \\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column k in the design matrix, that \\(X_ke_k = 0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and e is zero indicating that the two vectors are independent. This is our first property:\nP.1: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on."
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-regressors",
    "href": "notes/01-02-ols-estimators-and-their-properties.html#properties-of-the-ols-regressors",
    "title": "OLS Estimators and Their Properties",
    "section": "Properties of the OLS Regressors",
    "text": "Properties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nP.2: The sum of the sample residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^\\intercal\\mathbf{e}\\) matrix is \\(e_1+e_2+e_3+\\ldots+e_n = \\sum e_i\\), which is equal to zero since \\(\\mathbf{X}^\\intercal\\mathbf{e}=\\mathbf{0}\\).\nP.3: The mean of the sample residuals is zero.\nSince the mean of the residuals is computed as \\(\\bar{\\mathbf{e}}=\\frac{\\sum e_i}{n}\\), and the sum (numerator) is zero, then the mean is also zero.\nP.4: The regression line passes through the point \\((\\bar{X}, \\bar{Y})\\).\nRemember that \\(\\mathbf{e}=\\mathbf{y} − \\mathbf{Xb}\\). This means that:\n\\[\n\\begin{split}\n\\sum \\mathbf{e} &= \\sum (\\mathbf{y}-\\mathbf{Xb})\\\\[2ex]\n&= \\sum\\mathbf{y} - \\sum (\\mathbf{Xb}) \\\\[2ex]\n&= \\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})\n\\end{split}\n\\]\nIf we divide this expression by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum \\mathbf{e}}{n} &= \\frac{\\sum\\mathbf{y} - \\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n&= \\frac{\\sum\\mathbf{y}}{n} - \\frac{\\mathbf{b} \\sum (\\mathbf{X})}{n} \\\\[2ex]\n\\bar{\\mathbf{e}} &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\]\nBut, the mean of the residuals is zero, so:\n\\[\n\\begin{split}\n0 &= \\bar{\\mathbf{y}} - \\mathbf{b}\\bar{\\mathbf{x}} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\mathbf{b}\\bar{\\mathbf{x}}\n\\end{split}\n\\] That is, the predicted y-value when the mean of X is used as a predictor is the mean of Y. In other words, the point \\((\\bar{X}, \\bar{Y})\\) is on the regression line.\nP.5: The predicted y-values are uncorrelated with the sample residuals.\nSince \\(\\hat{\\mathbf{y}}=\\mathbf{Xb}\\) then \\(\\hat{\\mathbf{y}}^\\intercal=(\\mathbf{Xb})^\\intercal\\). If we post-multiply both sides of this expression by the residual vector e, we get:\n\\[\n\\begin{split}\n\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} &= (\\mathbf{Xb})^\\intercal \\mathbf{e} \\\\[2ex]\n&= \\mathbf{b}^\\intercal \\mathbf{X}^\\intercal \\mathbf{e}\n\\end{split}\n\\]\nSince \\(\\mathbf{X}^\\intercal \\mathbf{e}=\\mathbf{0}\\), then \\(\\hat{\\mathbf{y}}^\\intercal \\mathbf{e} =0\\). This implies that \\(\\hat{\\mathbf{y}}\\) and e are uncorrelated.\nP.6: The mean of the predicted y-values is equal to the mean of the observed y-values.\nWe can make use of the fact that \\(\\mathbf{y} = \\hat{\\mathbf{y}} + \\mathbf{e}\\). Taking the sum of both sides of the expression and dividing by n, we get:\n\\[\n\\begin{split}\n\\frac{\\sum\\mathbf{y}}{n} &= \\frac{\\sum(\\hat{\\mathbf{y}} + \\mathbf{e})}{n} \\\\[2ex]\n&= \\frac{\\sum\\hat{\\mathbf{y}}}{n} + \\frac{\\sum\\mathbf{e}}{n} \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}} + \\bar{\\mathbf{e}} \\\\[2ex]\n&= \\bar{\\hat{\\mathbf{y}}} + 0 \\\\[2ex]\n\\bar{\\mathbf{y}} &= \\bar{\\hat{\\mathbf{y}}}\n\\end{split}\n\\]\n\nIMPORTANT\nThese properties will always be true. They do not rely on any distributional assumptions of the residuals. Furthermore, these properties do not tell us anything about how “good” the coefficient estimates (b) are. Nor do these properties allow us to make inferences about the true parameters (\\(\\boldsymbol\\beta\\))."
  },
  {
    "objectID": "notes/01-03-assumptions-and-gauss-markov-theorem.html",
    "href": "notes/01-03-assumptions-and-gauss-markov-theorem.html",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "",
    "text": "One reason that OLS estimation is so useful is that, under a certain set of assumptions underlying the classical linear regression model, the estimators \\(b_0,b_1, b_2,\\ldots,b_k\\) have several desirable statistical properties. These properties include:"
  },
  {
    "objectID": "notes/01-03-assumptions-and-gauss-markov-theorem.html#assumptions-of-the-ols-regression-model",
    "href": "notes/01-03-assumptions-and-gauss-markov-theorem.html#assumptions-of-the-ols-regression-model",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "Assumptions of the OLS Regression Model",
    "text": "Assumptions of the OLS Regression Model\nAs mentioned, there are a certain set of assumptions underlying the linear regression model (both the simple and multiple regression models) for these properties to be true. These assumptions are:\n\nA.1: The model is correctly specified.\nA.2: The design matrix, X, is of full rank.\nA.3: The population errors given X have a mean of zero.\nA.4: The population errors given X are homoscedastic.\nA.5: The population errors given X are independent.\nA.6: The predictor values are fixed with finite, non-zero variance.\n\nIf these six assumptions are satisfied, then the estimators will have the properties we referred to previously. This is sometimes referred to as the weak classical regression model.\nAnother assumption that is useful is:\n\nA.7: The population errors given X are normally distributed.\n\nIf all seven assumptions are met, we refer to this as the strong classical regression model. When this assumption is met (in addition to the six other assumptions), the sampling distribution for the least squares estimators are also normally distributed; they are approximately normal under other conditions, especially with large sample sizes. This is useful for carrying out statistical inference. Furthermore, under the full set of seven assumptions, the least squares estimators are the maximum-likelihood estimators of the population coefficients.\nWe will now examine each of the assumptions underlying the linear regression model.\nA.1: The model is correctly specified.\nWhen we posit or fit the model \\(\\mathbf{y}=\\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\\), we are assuming that there is a linear relationship between the predictor(s) and the outcome. Furthermore, we are stating that this model is correctly specified using the set of predictors included and that deviation from this model in the observed data is all due to random sampling error.\nA.2: The design matrix, X, is of full rank.\nThis assumption indicates that there is no perfect multicollinearity in the predictor space. That is, the rows (or columns) of X are linearly independent. This is what allows us to compute \\((\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\).\nA.3: The population errors given X have a mean of zero.\nThis assumption states that the mean error (in the population) at a given x-value is zero. Using our rules of expectation:\n\\[\n\\begin{split}\n\\mathbb{E}(\\mathbf{e} | \\mathbf{X}) &= \\mathbb{E}\\begin{bmatrix}e_1 | \\mathbf{X} \\\\ e_2 | \\mathbf{X}\\\\ e_3 | \\mathbf{X}\\\\ \\vdots \\\\e_n | \\mathbf{X}\\end{bmatrix}  \\\\[2ex]\n&= \\begin{bmatrix}\\mathbb{E}(e_1 | \\mathbf{X}) \\\\\\mathbb{E}( e_2 | \\mathbf{X})\\\\ \\mathbb{E}(e_3 | \\mathbf{X})\\\\ \\vdots \\\\\\mathbb{E}(e_n | \\mathbf{X})\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}0 \\\\ 0\\\\ 0\\\\ \\vdots 0\\end{bmatrix}\n\\end{split}\n\\] This assumption implies that \\(\\mathbb{E}(\\mathbf{y})=\\mathbf{X}\\boldsymbol\\beta\\).\nA.4: The population errors given X are homoscedastic.\nThis assumption indicates that residuals at a given value of X have equal variances (homoskedasticity). To show this, we make use of our rules of expectations and the fact that the variance-covariance matrix of the errors at a given X (denoted as \\(\\sum(\\epsilon|\\mathbf{X})\\)) can be defined as the expected value of \\(\\boldsymbol\\epsilon^\\intercal\\boldsymbol\\epsilon\\).\n\\[\n\\begin{split}\n\\mathbb{E}(\\epsilon|\\mathbf{X}) &= \\mathbb{E}(\\boldsymbol\\epsilon\\boldsymbol\\epsilon^\\intercal|\\mathbf{X})  \\\\[2ex]\n&= \\mathbb{E}\\bigg(\\begin{bmatrix}\\epsilon_1|\\mathbf{X} \\\\ \\epsilon_2|\\mathbf{X} \\\\ \\epsilon_3|\\mathbf{X} \\\\ \\vdots \\\\ \\epsilon_n|\\mathbf{X}\\end{bmatrix} \\begin{bmatrix} \\epsilon_1|\\mathbf{X} & \\epsilon_2|\\mathbf{X} & \\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_n|\\mathbf{X}\\end{bmatrix}\\bigg) \\\\[2ex]\n&= \\mathbb{E}\\begin{bmatrix}\\epsilon_1^2|\\mathbf{X} & \\epsilon_1\\epsilon_2|\\mathbf{X} & \\epsilon_1\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_1\\epsilon_n|\\mathbf{X} \\\\\n\\epsilon_2\\epsilon_1|\\mathbf{X} &\\epsilon_2^2|\\mathbf{X} & \\epsilon_2\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_2\\epsilon_n|\\mathbf{X} \\\\\n\\epsilon_3\\epsilon_1|\\mathbf{X} & \\epsilon_3\\epsilon_2|\\mathbf{X} & \\epsilon_3^2|\\mathbf{X} & \\ldots & \\epsilon_3\\epsilon_n|\\mathbf{X} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\epsilon_n\\epsilon_1|\\mathbf{X} & \\epsilon_n\\epsilon_2|\\mathbf{X} & \\epsilon_n\\epsilon_3|\\mathbf{X} & \\ldots & \\epsilon_n^2|\\mathbf{X}\n\\end{bmatrix} \\\\[2ex]\n&= \\begin{bmatrix}\\mathbb{E}(\\epsilon_1^2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_1\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_2\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_2^2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_2\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_2\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_3\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3^2|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_3\\epsilon_n|\\mathbf{X}) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(\\epsilon_n\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_n^2|\\mathbf{X})\n\\end{bmatrix}\n\\end{split}\n\\]\nThe elements along the main diagonal are the error variances. For example \\(\\mathbb{E}(\\epsilon_i^2|\\mathbf{X})\\) is the variance of the ith residual. To show that this is the case we use the rules of expectations:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\epsilon_i|\\mathbf{X}) &= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) - \\big[\\mathbb{E}(\\epsilon_i|\\mathbf{X})\\big]^2 \\\\[2ex]\n&= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) - 0 \\\\[1ex]\n&= \\mathbb{E}(\\epsilon_i^2|\\mathbf{X})\n\\end{split}\n\\]\nThe homoskedasticity assumption makes each variance in the matrix equal, but unknown. Because the value of the variance is unknown, we can denote it as such using the placeholder \\(\\sigma^2_\\epsilon\\); that is, \\(\\mathbb{E}(\\epsilon_i^2|\\mathbf{X}) = \\sigma^2_\\epsilon\\). Using this to re-write our variance-covariance matrix as:\n\\[\n\\mathbb{E}(\\epsilon|\\mathbf{X}) = \\begin{bmatrix}\\sigma^2_\\epsilon & \\mathbb{E}(\\epsilon_1\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_1\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_1\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_2\\epsilon_1|\\mathbf{X}) & \\sigma^2_\\epsilon & \\mathbb{E}(\\epsilon_2\\epsilon_3|\\mathbf{X}) & \\ldots & \\mathbb{E}(\\epsilon_2\\epsilon_n|\\mathbf{X}) \\\\\n\\mathbb{E}(\\epsilon_3\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_3\\epsilon_2|\\mathbf{X}) & \\sigma^2_\\epsilon & \\ldots & \\mathbb{E}(\\epsilon_3\\epsilon_n|\\mathbf{X}) \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}(\\epsilon_n\\epsilon_1|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_2|\\mathbf{X}) & \\mathbb{E}(\\epsilon_n\\epsilon_3|\\mathbf{X}) & \\ldots & \\sigma^2_\\epsilon\n\\end{bmatrix}\n\\]\nA.5: The population errors given X are independent.\nThe off-diagonal elements in the variance-covariance matrix of the errors are the covariances between the errors. For example, \\(\\mathbb{E}(\\epsilon_i\\epsilon_j|\\mathbf{X})\\) is the covariance between the ith and jth errors. We can show this using rules of expectations:\n\\[\n\\begin{split}\n\\mathbb{E}(\\epsilon|\\mathbf{X}) &= \\begin{bmatrix}\\sigma^2_\\epsilon & 0 & 0 & \\ldots & 0 \\\\\n0 & \\sigma^2_\\epsilon & 0 & \\ldots & 0 \\\\\n0 & 0 & \\sigma^2_\\epsilon & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & \\sigma^2_\\epsilon\n\\end{bmatrix} \\\\[2ex]\n&= \\sigma^2_\\epsilon \\begin{bmatrix}1 & 0 & 0 & \\ldots & 0 \\\\\n0 & 1 & 0 & \\ldots & 0 \\\\\n0 & 0 & 1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & 1\n\\end{bmatrix} \\\\[2ex]\n&= \\sigma^2_\\epsilon \\mathbf{I}\n\\end{split}\n\\]\n\nNOTATION\nThe assumptions of homoskedasticity and independence can be more compactly expressed as: \\(\\mathbb{E}(\\epsilon|\\mathbf{X})=\\sigma^2_\\epsilon \\mathbf{I}\\)\n\nA.6: The predictor values are fixed with finite, non-zero variance.\nFixing the predictor values implies that the values in the X matrix are the same under repeated sampling from the same population. In most research in the social sciences, this is not the case. In those cases, we instead assume that the values in X are measured without error and that they are independent (uncorrelated) with the errors; \\(\\mathrm{Cov}(\\mathbf{X}, \\boldsymbol\\epsilon) = 0\\)."
  },
  {
    "objectID": "notes/01-03-assumptions-and-gauss-markov-theorem.html#gauss-markov-theorem",
    "href": "notes/01-03-assumptions-and-gauss-markov-theorem.html#gauss-markov-theorem",
    "title": "Assumptions for OLS Regression and the Gauss-Markov Theorem",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\nThe Gauss–Markov Theorem is a powerful theorem that states that under the weak classical model (A.1–A.6), the least squares estimators have certain desirable properties. These properties are that the estimators are:\n\nLinear functions of the observations, \\(y_i\\);\nUnbiased estimators of the population coefficients;\nThe most efficient (smallest sampling variance) unbiased linear estimators of the population coefficients.\n\n\nFYI\nBecause of this theorem, we typically refer to the OLS coefficents as BLUE (Best Linear Unbiased Estimators).\n\nFox (2016) reminds us that the “best” in BLUE means that they have the smallest sampling variance of all the possible linear unbiased estimators. There may be a biased or non-linear estimator that produces a smaller sampling variance than the OLS estimator. It is also worth noting that if we also invoke the normality assumption (A.7), then the OLS estimators become “best” among all unbiased estimators (both linear and non-linear).\nProving this theorem is beyond the scope of the class, but an outline for this proof would entail:\n\nShow that \\(b_0, b_1, b_2, \\ldots, b_n\\) are linear functions of the observations; that is, we can express each estimator as \\(b_k = \\sum w_iy_i\\) for some \\(w_i\\).\nShow that \\(b_0, b_1, b_2, \\ldots, b_n\\) are unbiased; that \\(\\mathbb{E}(b_k)=\\beta_k\\) for each of the k estimators\nShow that for any other unbiased linear estimator for \\(b_k\\), say \\(L_k\\), that \\(\\mathrm{Var}(b_k) &lt; \\mathrm{Var}(L_k)\\).\n\nA.7: The population errors given X are normally distributed.\nA final assumption that we make about normality is not required to prove the Gauss-Markov Theorem, but it is used to carry out hypothesis testing. This assumption states that the distribution of errors (in the population) at each X is normally distributed. If we combine this with the property that the mean error given X is zero, and the homoskedasticity assumption, then:\n\\[\n\\boldsymbol\\epsilon | \\mathbf{X} \\sim \\mathcal{N}(0, ~\\sigma^2_\\epsilon\\mathbf{I})\n\\]\nThis encapsulates the assumptions we check about the probability distribution of the model errors!"
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html",
    "title": "Statistical Inference for the Regression Model",
    "section": "",
    "text": "Recall that there are a certain set of assumptions underlying the linear regression model (both the simple and multiple regression models). These assumptions are:\nAnother assumption that is useful for inference is:"
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html#sampling-distribution-of-the-ols-estimators",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html#sampling-distribution-of-the-ols-estimators",
    "title": "Statistical Inference for the Regression Model",
    "section": "Sampling Distribution of the OLS Estimators",
    "text": "Sampling Distribution of the OLS Estimators\nWhen X is fixed, the b vector can be written as a linear transformation of the response vector y:\n\\[\n\\begin{split}\n\\mathbf{b} &= (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}(\\mathbf{X}^\\intercal\\mathbf{y}) \\\\[2ex]\n&= \\mathbf{My}\n\\end{split}\n\\]\nwhere \\(\\mathbf{M} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\).\nPreviously we showed that the estimators in b are unbiased estimates of \\(\\boldsymbol\\beta\\), that is \\(\\mathbb{E}(\\mathbf{b})=\\boldsymbol\\beta\\). We can also define the variance–covariance matrix of b as:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\mathbf{b}) &= \\mathbf{M}\\mathrm{Var}(\\mathbf{y})\\mathbf{M}^\\intercal \\\\[2ex]\n&= \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]\\sigma^2_\\epsilon\\mathbf{I} \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]^\\intercal\n\\end{split}\n\\]\nRearranging this and using our rules of transposes and inverses, we get:\n\\[\n\\begin{split}\n\\mathrm{Var}(\\mathbf{b}) &= \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]\\sigma^2_\\epsilon\\mathbf{I} \\big[(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\big]^\\intercal \\\\[2ex]\n&= \\sigma^2_\\epsilon (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\mathbf{X}^\\intercal\\mathbf{X} (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_\\epsilon (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\end{split}\n\\]\nThis implies that the sampling variances and covariances of the estimators depend only on the predictor values and the error variance. This matrix is often referred to as V.\nNote that to derive this, we only need the assumption of linearity. The normality assumption is not used to compute the mean (expectation) nor the sampling variance, nor covariances of the estimators. However, if the errors (and hence y) are normally distributed (i.e., assumption A.7), then so are the sampling distributions of the estimators. We can express these distributions as:\n\\[\n\\mathbf{b} \\sim \\mathcal{N}\\bigg(\\boldsymbol\\beta,~ \\sigma^2_\\epsilon(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg)\n\\]"
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html#inference-for-the-estimators",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html#inference-for-the-estimators",
    "title": "Statistical Inference for the Regression Model",
    "section": "Inference for the Estimators",
    "text": "Inference for the Estimators\nAn individual estimator, \\(b_j\\), has a sampling distribution of:\n\\[\nb_j \\sim \\mathcal{N}\\bigg(\\beta_j,~ \\mathbf{V}_{jj}\\bigg)\n\\]\nwhere \\(\\mathbf{V}_{jj}\\) is the element in the jth row and jth column of the variance–covariance matrix for b. To test the hypothesis that:\n\\[\nH_0: \\beta_j = \\beta_j^{(0)}\n\\]\nwhere \\(\\beta_j^{(0)}\\) is some value (e.g., \\(H_0: \\beta_j = 0\\)), we compute the ratio:\n\\[\nZ_0 = \\frac{b_j - \\beta_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nwhich is unit-normal distributed; \\(\\mathcal{N}(0, 1)\\). Note the denominator is the standard deviation of the sampling distribution for \\(b_j\\).\nUnfortunately, in practice we do not know \\(\\sigma_\\epsilon\\) to compute V. Instead, we substitute in the estimate for this value from our sample, the unbiased estimator \\(s_e\\), where,\n\\[\ns_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\n\\]\nwhere n is the sample size, k is the number of predictors in the model, and \\(n − k − 1\\) is the residual degrees of freedom for the model. Thus, we get an estimate of the variance–covariance matrix using:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Var}(\\mathbf{b})} &= s^2_e (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\\\[2ex]\n&= \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\end{split}\n\\]\nThe standard error for \\(b_j\\) is then estimated using the jth diagonal element of this matrix, namely \\(\\sqrt{\\hat{v}_{jj}}\\).\n\nMATH NOTE\nThere is a theorem which says that (1) if Z is a standard normal variable and W is chi-squared distributed with \\(\\nu\\) degrees of freedom, and (2) Z and W are independent, then T, defined as:\n\\[\nT = \\frac{Z}{\\sqrt{\\frac{W}{\\nu}}}\n\\]\nwill follow a t-distribution with \\(\\nu\\) degrees of freedom.\n\nSince it can be shown that \\((n−𝑘− 1)\\frac{s^2_e}{\\sigma^2_\\epsilon}\\) follows a chi-squared distribution with \\(n-l-1\\) degrees of freedom, and also that \\(b_j\\) and \\(s^2_e\\) are independent; it follows that:\n\\[\nT = \\frac{b_j - b_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nis t-distributed with \\(n-k-1\\) degrees of freedom. To evaluate the hypothesis that \\(H_0∶ \\beta_j = \\beta_j^{(0)}\\), we compute the test statistic:\n\\[\nt_0 = \\frac{b_j - b_j^{(0)}}{\\sqrt{\\mathbf{V}_{jj}}}\n\\]\nand evaluate it within the t-distribution having \\(n-k-1\\) degrees of freedom (where n is the sample size, and k is the number of predictors in the model)."
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html#confidence-intervals",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html#confidence-intervals",
    "title": "Statistical Inference for the Regression Model",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA \\(1−\\alpha\\)% confidence interval can also be constructed for each coefficient using:\n\\[\n\\begin{split}\n\\mathrm{CI}_{1−\\alpha} &= b_j \\pm |t^*_{\\alpha/2}| \\times \\bigg(s_e (\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\\bigg) \\\\[2ex]\n&= b_j \\pm |t^*_{\\alpha/2}| \\times \\sqrt{\\mathbf{V}_{jj}}\n\\end{split}\n\\]\nwhere \\(t^*_{\\alpha/2}\\) is the critical value demarcating the area in the \\(\\alpha/2\\) proportion of the distribution in the t-distribution with \\(n-k-1\\) degrees of freedom. For example, if we wanted to compute a 95% CI, then \\(\\alpha = 0.05\\) and \\(t^*_{.025}\\) would be the critical value that demarcates the lowest 0.025 of the t-distribution."
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html#model-level-inference",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html#model-level-inference",
    "title": "Statistical Inference for the Regression Model",
    "section": "Model-Level Inference",
    "text": "Model-Level Inference\nAt the model-level we are interested in testing the hypothesis \\(H_0∶ \\rho^2 = 0\\). Recall this is equivalent to testing the hypothesis that all the regression parameters (except the intercept) in the model are zero:\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3=\\ldots = \\beta_k = 0\n\\]\nThis is a specific form of the general linear hypothesis which can be expressed as:\n\\[\nH_0: \\mathbf{L}\\boldsymbol\\beta = \\mathbf{c}\n\\]\nwhere,\n\nL is a \\(q \\times (k + 1)\\) matrix referred to as the hypothesis matrix, where q is the number of parameters being tested;\n\\(\\boldsymbol\\beta\\) is a \\((k + 1) \\times 1\\) vector of parameters included in the model; and\nc is a \\(q \\times 1\\) vector of hypothesized values\n\nAs an example, if we wanted to test the model-level null hypothesis in a two predictor model (\\(y_i = \\beta_0 + \\beta_1(x_{1i}) + \\beta_2(x_{2i}) + \\epsilon_i\\)),\n\\[\nH_0: \\beta_1 = \\beta_2 = 0\n\\]\nThen the general linear hypothesis could be expressed as:\n\\[\nH_0: \\begin{bmatrix}0 & 1 & 0 \\\\0 & 0 & 1\\end{bmatrix}\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\\end{bmatrix} = \\begin{bmatrix}0 \\\\0\\end{bmatrix}\n\\]\nThis results in:\n\\[\nH_0: \\begin{bmatrix}\\beta_1 \\\\ \\beta_2\\end{bmatrix} = \\begin{bmatrix}0 \\\\0\\end{bmatrix}\n\\]\nWe can construct a test statistic, \\(F_0\\), as:\n\\[\nF_0 = \\frac{(\\mathbf{Lb} − \\mathbf{c})^\\intercal[\\mathbf{L}(\\mathbf{X}^\\intercal\\mathbf{X})^{−1}\\mathbf{L}^\\intercal]^{−1}(\\mathbf{Lb} − \\mathbf{c})}{q(s^2_e)}\n\\]\nThis follows an F-distribution with q and \\(n-k-1\\) degrees of freedom."
  },
  {
    "objectID": "notes/01-04-statistical-inference-for-the-regression-model.html#implications-for-applied-researchers",
    "href": "notes/01-04-statistical-inference-for-the-regression-model.html#implications-for-applied-researchers",
    "title": "Statistical Inference for the Regression Model",
    "section": "Implications for Applied Researchers",
    "text": "Implications for Applied Researchers\nIf the assumptions underlying the strong classical regression model (A.1–A.7) are all valid, then the OLS estimators \\(b_0, b_1, \\ldots, b_k\\) are good estimators of the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\). They are unbiased and efficient and have accurate sampling variances and covariance (i.e., they are BLUE).\nOf course, any of the assumptions may be challenged either on a priori substantive grounds, or post hoc, via empirical examination of the sample residuals. If one (or more) of the assumptions are violated, then some of the properties may be compromised. Let’s look at violation in turn:\nViolating A.1: The model is not correctly specified.\nThis is probably the most egregious and costly violation. Violating this assumption means that the theoretical model embodied in the regression equation is wrong. If this is the case, there is no use proceeding.\nViolating A.2: The design matrix, X, is not of full rank.\nThis would mean that we cannot compute an inverse for the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. Subsequently, we could not compute any parameter estimates nor SEs.\nViolating A.3: The population errors given X do not have a mean of zero.\nThe estimator \\(b_0\\) (the intercept) will be biased when this assumption is violated. However, the other regression estimators (e.g., \\(b_0, b_1, \\ldots, b_k\\)) are still BLUE. Unfortunately, we can never test this assumption in practice since \\(\\sum e_i =0\\) always holds when using OLS.\nViolating A.4: The population errors given X are heteroscedastic.\nIf the assumption of constant variance is violated, the coefficient estimators will remain unbiased. However, the estimates for the variances/standard errors for these coefficients will be wrong. This also impacts the confidence intervals and hypothesis tests for these parameters. There are solutions to this problem, including using a different estimation method (e.g., weighted least squares) or estimating the sampling variances for these coefficients differently (e.g., sandwich estimation).\nViolating A.5: The population errors given X are not independent.\nIf the errors are correlated (not independent), the coefficient estimators will still be unbiased. Again, however, the variances/standard errors and results from the confidence intervals and hypothesis tests will be wrong. Often the hypothesis tests will indicate statistical significance much more often then it should (increased probability of a type I error). Dealing with this problem requires using models that account for the correlation (e.g., mixed-effects models) and different estimation methods (e.g., maximum likelihood).\nViolating A.6: The predictor values are not fixed.\nWhen X is fixed, the predictors are uncorrelated with the error terms. This is what leads to unbiased estimates of both the estimators and the variances/standard errors. When X is random (which is almost always the case in observational data), we need to assume that \\(\\mathrm{Cov}(\\mathbf{X}, \\boldsymbol\\epsilon) = \\mathbf{0}\\). That is we assume that the predictor values are generated by a mechanism unrelated to the errors. If that is not the case, then the OLS estimates will be biased\nViolating A.7: The population errors given X are not normally distributed.\nViolation of the normality assumption causes the least number of problems. Under non-normality the regression estimators are still BLUE, and \\(s^2_e\\) is still an unbiased estimator of \\(\\sigma^2_\\epsilon\\). However, the under non-normality, the use of the F- and t-distributions for inference is questionable, especially if the sample size is small. If the sample size is large, the sampling distributions of the coefficients are approximately normal (i.e., Central Limit Theorem) and the use of the F- and t-distributions for inference is justified."
  },
  {
    "objectID": "notes/01-05-regression-example.html",
    "href": "notes/01-05-regression-example.html",
    "title": "A Regression Example",
    "section": "",
    "text": "In this document we will use the data in contraception.csv to examine whether female education level explains variation in contraceptive useage after controlling for GNI.\n# Load libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\n\n# Import data\ncontraception = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/contraception.csv\")\n\n# View data\ncontraception\n\n# A tibble: 97 × 5\n   country                region                 contraceptive educ_female gni  \n   &lt;chr&gt;                  &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;\n 1 Algeria                Middle East and North…            57         5.9 High \n 2 Austria                Europe and Central As…            66         8.9 High \n 3 Azerbaijan             Europe and Central As…            55        10.5 High \n 4 Bangladesh             South Asia                        62         4.6 Low  \n 5 Belgium                Europe and Central As…            67        10.5 High \n 6 Belize                 Latin America and the…            51         9.2 High \n 7 Benin                  Sub-Saharan Africa                16         2   Low  \n 8 Bolivia                Latin America and the…            67         8.4 Low  \n 9 Bosnia and Herzegovina Europe and Central As…            46         7.2 High \n10 Botswana               Sub-Saharan Africa                53         8.7 High \n# ℹ 87 more rows\n\n# IF you want to see all the variables\n#contraception |&gt; print(width = Inf)"
  },
  {
    "objectID": "notes/01-05-regression-example.html#examine-the-data",
    "href": "notes/01-05-regression-example.html#examine-the-data",
    "title": "A Regression Example",
    "section": "Examine the Data",
    "text": "Examine the Data\nWe need to correctly specify the model. Since we have no theory to guide us, this is done empirically by looking at the data.\n\n# Create density plot of contraception\np1 = ggplot(data = contraception, aes(x = contraceptive)) + \n   geom_density() +\n   theme_bw() +\n   labs(\n      x = \"Contraceptive useage\"\n   ) \n\n# Create density plot of female education level\np2 = ggplot(data = contraception, aes(x = educ_female)) + \n   geom_density() +\n   theme_bw() +\n   labs(\n      x = \"Female education level\"\n   ) \n\n\n# Condition the relationship on GNI\np3 = ggplot(data = contraception, aes(x = educ_female, y = contraceptive, color = gni)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) +\n   facet_wrap(~gni)\n\n# Layout plots with patchwork\n(p1 | p2) / p3\n\n\n\n\n\n\n\nFigure 1: TOP-LEFT: Density plot of contracrptive useage. TOP-RIGHT: Density plot of female education level. BOTTOM: Scatterplot of contraceptive useage versus female education level conditioned on GNI.\n\n\n\n\n\n\nShould we include main-effects only? Or an interaction?\nIs there non-linearity to account for (e.g., transformations)? Or does it look linear?"
  },
  {
    "objectID": "notes/01-05-regression-example.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "href": "notes/01-05-regression-example.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "title": "A Regression Example",
    "section": "Use Matrix Algebra to Compute Coefficient Estimates",
    "text": "Use Matrix Algebra to Compute Coefficient Estimates\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\n\n# Store values\nn = nrow(contraception) #Sample size\nk = 2 #Number of predictors\n\n\n# Create outcome vector\ny = contraception$contraceptive\n\n# Create dummy variable for GNI\ncontraception = contraception %&gt;%\n   mutate(\n      high_gni = if_else(gni == \"High\", 1, 0)\n      )\n\n# Create design matrix\nX = matrix(\n   data = c(rep(1, n), contraception$educ_female, contraception$high_gni),\n   ncol = 3\n)\n\n# Compute b vector\nb = solve(t(X) %*% X) %*% t(X) %*% y\nb\n\n          [,1]\n[1,] 27.021387\n[2,]  4.088735\n[3,]  1.608766\n\n\nThus the fitted regression equation is:\n\\[\n\\widehat{\\mathrm{Contraceptive~Use}}_i = 27.02 + 4.09(\\mathrm{Female~Education~Level}_i) + 1.60(\\mathrm{High~GNI}_i)\n\\]"
  },
  {
    "objectID": "notes/01-05-regression-example.html#compute-residual-standard-error",
    "href": "notes/01-05-regression-example.html#compute-residual-standard-error",
    "title": "A Regression Example",
    "section": "Compute Residual Standard Error",
    "text": "Compute Residual Standard Error\n\n# Compute e vector\ne = y - X %*% b\n\n# Compute s_e\ns_e = sqrt((t(e) %*% e) / (n - k - 1))\ns_e\n\n         [,1]\n[1,] 14.39792\n\n\nThus the residual standard error (a.k.a., the root mean square error; RMSE) is:\n\\[\ns_e = 14.40\n\\]"
  },
  {
    "objectID": "notes/01-05-regression-example.html#compute-variancecovariance-matrix-for-the-coefficients",
    "href": "notes/01-05-regression-example.html#compute-variancecovariance-matrix-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Compute Variance–Covariance Matrix for the Coefficients",
    "text": "Compute Variance–Covariance Matrix for the Coefficients\n\\[\n\\mathrm{Var}(\\mathbf{b}) = s^2_e(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\]\nwhere \\(s^2_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\\)\n\n# Compute varaince-covariance matrix of b\nV = as.numeric(s_e^2) * solve(t(X) %*% X)\nV\n\n          [,1]       [,2]      [,3]\n[1,] 12.414688 -1.8783934  4.782603\n[2,] -1.878393  0.4267136 -2.028306\n[3,]  4.782603 -2.0283060 18.197825\n\n# Compute SEs for b\nsqrt(diag(V))\n\n[1] 3.5234483 0.6532332 4.2658909\n\n\nThus\n\\[\n\\mathrm{SE}(b_0) = 3.52 \\qquad \\mathrm{SE}(b_1) = 0.65 \\qquad \\mathrm{SE}(b_2) = 4.27\n\\]"
  },
  {
    "objectID": "notes/01-05-regression-example.html#coefficient-level-inference",
    "href": "notes/01-05-regression-example.html#coefficient-level-inference",
    "title": "A Regression Example",
    "section": "Coefficient-Level Inference",
    "text": "Coefficient-Level Inference\nHere we will focus on the effects of female education level since it is our focal predictor. (GNI is a control.) Note this is the second effect in the b vector and in the V matrix. We will test the hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\n\n# Compute t-value\nt_0 = (b[2] - 0) / sqrt(V[2, 2])\nt_0\n\n[1] 6.259228\n\n# Evaluate t-value\ndf = n - k - 1\np = 2* (1 - pt(abs(t_0), df = df))\np\n\n[1] 0.00000001143799\n\n\nHere,\n\\[\nt(94) = 6.26,~p=0.0000000114\n\\]\nThe evidence suggests that the data are not very compatible with the hypothesis that there is no effect of female education level on contraceptive useage, after controlling for differences in GNI."
  },
  {
    "objectID": "notes/01-05-regression-example.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "href": "notes/01-05-regression-example.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Statistical Inference: Confidence Intervals for the Coefficients",
    "text": "Statistical Inference: Confidence Intervals for the Coefficients\nFrom the hypothesis test, we believe there is an effect of female education level on contraceptive useage, after controlling for differences in GNI. What is that effect? To answer this we will compute a 95% CI for the effect of female education.\n\n# Compute critical value\nt_star = qt(.025, df = df)\n\n# Compute CI\nb[2] - abs(t_star) * sqrt(V[2, 2])\n\n[1] 2.791725\n\nb[2] + abs(t_star) * sqrt(V[2, 2])\n\n[1] 5.385745\n\n\nThe 95% CI indicates that the population effect of female education level on contraceptive useage, after controlling for differences in GNI is between 2.79 and 5.39."
  },
  {
    "objectID": "notes/01-05-regression-example.html#accessing-regression-matrices-from-lm",
    "href": "notes/01-05-regression-example.html#accessing-regression-matrices-from-lm",
    "title": "A Regression Example",
    "section": "Accessing Regression Matrices from lm()",
    "text": "Accessing Regression Matrices from lm()\nThere are several built-in R functions that allow you to access different regression matrices once you have fitted a model with lm().\n\n# Access design matrix\nmodel.matrix(lm.1)\n\n   (Intercept) educ_female gniLow\n1            1         5.9      0\n2            1         8.9      0\n3            1        10.5      0\n4            1         4.6      1\n:            :          :       :\n97           1         6.7      1\nattr(,\"assign\")\n[1] 0 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gni\n[1] \"contr.treatment\"\nThe design matrix is given and information about this design matrix is also encoded. There is an attribute “assign”, an integer vector with an entry for each column in the matrix giving the term in the formula which gave rise to the column. Value 0 corresponds to the intercept (if any), and positive values to terms in the order given by the term.labels attribute of the terms structure corresponding to object. There is also an attribute called “contrasts” that identifies any factors (categorical variables) in the model and indicates how the contrast testing (comparison of the factor levels) will be carried out. Here “contr.treatment” is used. This compares each level of the factor to the baseline (which is how dummy coding works).\n\n# Access coefficient estimates\ncoef(lm.1)\n\n(Intercept) educ_female      gniLow \n  28.630153    4.088735   -1.608766 \n\n# Access variance-covariance matrix for b\nvcov(lm.1)\n\n            (Intercept) educ_female     gniLow\n(Intercept)   40.177719  -3.9066994 -22.980428\neduc_female   -3.906699   0.4267136   2.028306\ngniLow       -22.980428   2.0283060  18.197825\n\n# Access fitted values\nfitted(lm.1)\n\n       1        2        3        4        5        6        7        8 \n52.75369 65.01990 71.56187 45.82957 71.56187 66.24652 35.19886 61.36676 \n       9       10       11       12       13       14       15       16 \n58.06905 64.20215 71.97075 34.78998 36.01660 40.10534 47.87394 78.92160 \n      17       18       19       20       21       22       23       24 \n29.47463 67.88201 57.25130 35.60773 62.97553 71.56187 78.10385 60.11341 \n      25       26       27       28       29       30       31       32 \n58.88679 48.69168 51.96267 32.74562 73.19737 51.14493 69.10863 30.29238 \n      33       34       35       36       37       38       39       40 \n40.10534 48.69168 74.42399 40.10534 55.23366 46.62059 68.29089 68.69976 \n      41       42       43       44       45       46       47       48 \n74.42399 67.06427 70.33525 49.10056 74.01512 42.55858 59.70454 54.82479 \n      49       50       51       52       53       54       55       56 \n36.42548 46.64732 40.92309 66.24652 50.70932 32.74562 67.47314 61.34004 \n      57       58       59       60       61       62       63       64 \n61.74891 66.27325 61.77564 40.10534 30.29238 54.38919 36.83435 46.64732 \n      65       66       67       68       69       70       71       72 \n30.29238 44.19408 40.51421 67.88201 59.29567 63.00226 61.34004 71.15300 \n      73       74       75       76       77       78       79       80 \n39.69647 43.37633 40.92309 66.24652 35.19886 76.05948 76.87723 68.69976 \n      81       82       83       84       85       86       87       88 \n67.47314 58.47792 57.27803 67.90874 45.42070 57.25130 40.51421 49.50943 \n      89       90       91       92       93       94       95       96 \n44.60295 72.81522 80.96597 64.20215 64.20215 48.28281 31.92787 50.73605 \n      97 \n54.41591 \n\n# Access raw residuals\nresid(lm.1)\n\n          1           2           3           4           5           6 \n  4.2463087   0.9801026 -16.5618739  16.1704304  -4.5618739 -15.2465180 \n          7           8           9          10          11          12 \n-19.1988577   5.6332361 -12.0690473 -11.2021503  -2.9707474  -2.7899842 \n         13          14          15          16          17          18 \n -7.0166048  15.8946599 -13.8739373   6.0784025 -23.4746282   8.1179879 \n         19          20          21          22          23          24 \n 23.7486998 -15.6077312  15.0244703  -2.5618739   7.8961495   9.8865851 \n         25          26          27          28          29          30 \n 21.1132057  10.3083157  20.0373274   7.2543835   4.8026319 -20.1449256 \n         31          32          33          34          35          36 \n  6.8913673 -21.2923753  -6.1053401  24.3083157 -12.4239887  13.8946599 \n         37          38          39          40          41          42 \n  5.7663391   6.3794117  -3.2908856   4.3002408 -34.4239887 -15.0642650 \n         43          44          45          46          47          48 \n-15.3352533  11.8994421   5.9848849  11.4414187  -4.7045414   5.1752126 \n         49          50          51          52          53          54 \n -5.4254783   1.3526833  18.0769128 -14.2465180 -31.7093236 -16.7456165 \n         55          56          57          58          59          60 \n 18.5268614   2.6599645   5.2510909  -6.2732463  -6.7756375  30.8946599 \n         61          62          63          64          65          66 \n -3.2923753   1.6108145  16.1656482  33.3526833 -19.2923753 -16.1940755 \n         67          68          69          70          71          72 \n -6.5142136  -4.8820121   8.7043321  -9.0022581  12.6599645  -1.1530004 \n         73          74          75          76          77          78 \n 13.3035334  -2.3763284 -12.9230872  -8.2465180 -12.1988577   3.9405172 \n         79          80          81          82          83          84 \n  2.1227701 -13.6997592   3.5268614 -10.4779208   8.7219714 -38.9087405 \n         85          86          87          88          89          90 \n -7.4206961  20.7486998 -20.5142136  13.4905686  -2.6029490  -7.8152229 \n         91          92          93          94          95          96 \n  3.0340348  15.7978497  10.7978497  27.7171892   2.0721306  -1.7360520 \n         97 \n 12.5840862 \n\n\n\nPROTIP\nThe tidy() and augment() functions from the {broom} package also give the coefficients, residuals, and fitted values. The difference is in the type of out put you get. For example the residuals from resid() are outputted as a vector, whereas the residuals from augment() are outputted in a data frame/tibble. In R certain output types are better than other in different situations (e.g., {ggplot2} functions require data frames)."
  },
  {
    "objectID": "notes/01-02-ols-estimators-and-their-properties.html#representing-the-population-regression-model-using-matrix-algebra",
    "href": "notes/01-02-ols-estimators-and-their-properties.html#representing-the-population-regression-model-using-matrix-algebra",
    "title": "OLS Estimators and Their Properties",
    "section": "Representing the Population Regression Model Using Matrix Algebra",
    "text": "Representing the Population Regression Model Using Matrix Algebra\nUsing the subject-specific subscripts \\((1, 2, 3, \\ldots, 𝑛)\\), we can write out each subject’s equation:\n\\[\n\\begin{split}\ny_1 &= \\beta_0 + \\beta_1(x_1) + \\epsilon_1\\\\\ny_2 &= \\beta_0 + \\beta_1(x_2) + \\epsilon_2\\\\\ny_3 &= \\beta_0 + \\beta_1(x_3) + \\epsilon_3\\\\\n\\vdots~~&~~~~~~~\\vdots~~~~~~~~~\\vdots~~~~~~~~~~~\\vdots\\\\\ny_n &= \\beta_0 + \\beta_1(x_n) + \\epsilon_n\\\\\n\\end{split}\n\\]\nThese can be arranged into a set of vectors and matrices, namely,\n\\[\n\\begin{split}\n\\begin{bmatrix}y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_n\\end{bmatrix} &= \\begin{bmatrix}1 & x_1 \\\\ 1 & x_2 \\\\ 1 & x_3 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\end{bmatrix} + \\begin{bmatrix}e_1 \\\\ e_2 \\\\ e_3 \\\\ \\vdots \\\\ e_n \\end{bmatrix} \\\\[2ex]\n\\mathbf{y} &= \\mathbf{X}\\boldsymbol\\beta + \\boldsymbol\\epsilon\n\\end{split}\n\\]\nwhere,\n\ny is an \\(n \\times 1\\) vector of observations on the outcome variable.\nX is an \\(n \\times k\\) matrix (called the design matrix) consisting of a column of ones and the observations for k independent predictors. In the simple regression example, \\(k=2\\), and the design matrix has two columns—a column of ones and a column of observations for the predictor x.\n\\(\\boldsymbol\\beta\\) is a \\(k \\times 1\\) vector of unknown population parameters that we want to estimate. In the simple regression model, \\(\\boldsymbol\\beta\\) is a \\(2 \\times 1\\) vector consisting of \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\boldsymbol\\epsilon\\) is a \\(n \\times 1\\) vector of residuals."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#what-are-path-diagrams",
    "href": "notes/03-02-path-analysis.html#what-are-path-diagrams",
    "title": "🎶 Path Analysis",
    "section": "What are Path Diagrams?",
    "text": "What are Path Diagrams?\nPath diagrams are visual depictions of the hypothesized relationships between a set of variables. In this set of notes, we will introduce some of the ideas and concepts related to path diagrams, and the estimation of effects depicted in a path diagram (i.e., path analysis).\n\n[R Script File]"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#example",
    "href": "notes/03-02-path-analysis.html#example",
    "title": "🎶 Path Analysis",
    "section": "Example",
    "text": "Example\nTo help us in this endeavor, we will consider three potential student-level variables for our path model:\n\nAchievement\nAcademic Ability\nMotivation"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#simulating-data",
    "href": "notes/03-02-path-analysis.html#simulating-data",
    "title": "🎶 Path Analysis",
    "section": "Simulating Data",
    "text": "Simulating Data\n\n# Load Libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(corrr)\n\n# Create correlation matrix\ncorrs = matrix(\n  data = c(\n    1.000, 0.737, 0.255,\n    0.737, 1.000, 0.205,\n    0.255, 0.205, 1.000\n  ),\n  nrow = 3\n)\n\n# Create mean vector\nmeans = c(0, 0, 0) \n\n# Set sample size\nn = 1000"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#simulating-data-cntd.",
    "href": "notes/03-02-path-analysis.html#simulating-data-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Simulating Data (cntd.)",
    "text": "Simulating Data (cntd.)\n\n# Make simulation reproducible\nset.seed(1)\n\n\n# Simulate the data and convert to data frame\nsim_dat &lt;- data.frame(MASS::mvrnorm(n = 1000, mu = means, Sigma = corrs, empirical = TRUE)) |&gt;\n  rename(\n    achievement = X1,\n    ability = X2, \n    motivation = X3\n  )\n\n\n# View simulated data\nhead(sim_dat)\n\n  achievement    ability motivation\n1  0.33527960 -0.1297477  1.4114639\n2  0.99197530 -0.3813547  1.2637617\n3 -0.70788322  0.6913426 -0.5557938\n4 -0.87083130 -1.3095554 -1.1570792\n5 -0.05935273 -0.2583549 -0.1541515\n6  0.38943125  1.4254429 -0.6394095"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#reading-a-path-diagram",
    "href": "notes/03-02-path-analysis.html#reading-a-path-diagram",
    "title": "🎶 Path Analysis",
    "section": "Reading a Path Diagram",
    "text": "Reading a Path Diagram\nIn a path diagram,\n\nVariables we have data on (i.e., manifest or measured variables) are represented by squares or rectangles.\nLines between the variables indicate a relationship between the two connected variables. The lines between the different variables are called paths.\n\nIf the path has an arrowhead on both sides, it indicates the two connected variables are related (i.e., correlated).\nIf the path has an arrowhead on only one side, it indicates a hypothesized causal relationship. The cause is in the direction of the arrow; that is, the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#reading-a-path-diagram-cntd.",
    "href": "notes/03-02-path-analysis.html#reading-a-path-diagram-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Reading a Path Diagram (cntd.)",
    "text": "Reading a Path Diagram (cntd.)"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#two-example-path-diagrams",
    "href": "notes/03-02-path-analysis.html#two-example-path-diagrams",
    "title": "🎶 Path Analysis",
    "section": "Two Example Path Diagrams",
    "text": "Two Example Path Diagrams\n\nIn both of the examples below, there are three manifest/measured variables (academic ability, motivation, and achievement) and three paths (p1, p2, and p3). The difference between the two is that in the right-hand diagram, there are hypothesized causal relationships, while in the left-hand diagram the relationships are not presumed to be causal."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#path-diagram-1",
    "href": "notes/03-02-path-analysis.html#path-diagram-1",
    "title": "🎶 Path Analysis",
    "section": "Path Diagram 1",
    "text": "Path Diagram 1\n\n\n\n\n\n\n\n\n\n\n\n\nThe model depicted in this path diagram posits that there are relationships between:\n\nAcademic ability and achievement (p1);\nAcademic ability and motivation (p2); and\nMotivation and achievement (p3).\n\nThe double-headed arrows on the paths indicate that the variables are related (i.e., correlated), although there is no causal direction hypothesized."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-path-coefficients-in-path-diagram-1",
    "href": "notes/03-02-path-analysis.html#estimating-path-coefficients-in-path-diagram-1",
    "title": "🎶 Path Analysis",
    "section": "Estimating Path Coefficients in Path Diagram 1",
    "text": "Estimating Path Coefficients in Path Diagram 1\n\nIn a path analysis, one goal is to estimate path coefficients. In this model, since the paths represent the relationship between two variables, the path coefficients are simply the bivariate correlations between each set of variables.\n\n# Compute correlations\nsim_dat |&gt;\n  correlate()\n\n# A tibble: 3 × 4\n  term        achievement ability motivation\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 achievement      NA       0.737      0.255\n2 ability           0.737  NA          0.205\n3 motivation        0.255   0.205     NA"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#weak-causal-ordering",
    "href": "notes/03-02-path-analysis.html#weak-causal-ordering",
    "title": "🎶 Path Analysis",
    "section": "Weak Causal Ordering",
    "text": "Weak Causal Ordering\nIn path modeling, we often assert weak causal ordering on the variables in the model. With weak causal ordering, we are not saying X causes Y, but rather that if X and Y are casually related, one of these variables (typically X) is the cause of the other.\n\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nIn this model, the paths have arrows at only one end indicating the hypothesized causal relationship. The cause is in the direction of the arrow, that is the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line.\n\nAny weak causal model should be posited prior to collecting or looking at the data! Creating this model can also help you identify the data to collect."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#path-model-2-weak-causal-ordering",
    "href": "notes/03-02-path-analysis.html#path-model-2-weak-causal-ordering",
    "title": "🎶 Path Analysis",
    "section": "Path Model 2: Weak Causal Ordering",
    "text": "Path Model 2: Weak Causal Ordering\n\n\n\n\n\n\n\n\n\n\n\n\nThe model depicted in this path diagram posits the following causal relationships:\n\nIf academic ability and achievement are causally related, then academic ability is the cause of achievement.\nIf academic ability and motivation are causally related, then academic ability is the cause of motivation.\nIf motivation and achievement are causally related, then motivation is the cause of achievement."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-path-coefficients-in-a-weak-causal-model",
    "href": "notes/03-02-path-analysis.html#estimating-path-coefficients-in-a-weak-causal-model",
    "title": "🎶 Path Analysis",
    "section": "Estimating Path Coefficients in a Weak Causal Model",
    "text": "Estimating Path Coefficients in a Weak Causal Model\nAgain, we would want to estimate the path coefficients shown in the diagram for the weak causal model. In causal models, the path coefficients are not necessarily the correlations. To find the path coefficients we need to use the tracing rule. The tracing rule indicates that:\n\nThe correlation between two variables X and Y is equal to the sum of the possible products of all possible paths from each possible tracing from X to Y, with the following two exceptions:\n\nThe same variable is not entered more than once in the same tracing.\nA variable is not both entered and exited through an arrowhead."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#example-of-the-tracing-rule",
    "href": "notes/03-02-path-analysis.html#example-of-the-tracing-rule",
    "title": "🎶 Path Analysis",
    "section": "Example of the Tracing Rule",
    "text": "Example of the Tracing Rule\nAs an example, consider all the tracings (routes) that allow us to start at the academic ability variable and go to the achievement variable in the path diagram at right.\n\n\n\n\n\n\n\n\n\n\n\n\nThere are two possible tracings that conform to the tracing rule:\n\nStart at the academic ability variable and take p1 to the achievement variable.\nStart at the academic ability variable and take p2 to the motivation variable, then take p3 to the achievement variable."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#example-of-the-tracing-rule-cntd.",
    "href": "notes/03-02-path-analysis.html#example-of-the-tracing-rule-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Example of the Tracing Rule (cntd.)",
    "text": "Example of the Tracing Rule (cntd.)\nSimilarly, we could have started at the achievement variable and determined the tracings to get to the academic ability variable:\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart at the achievement variable and take p1 to the academic ability variable.\nStart at the achievement variable and take p3 to the motivation variable, then take p2 to the academic ability variable."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#important-note-regarding-the-tracing-rule",
    "href": "notes/03-02-path-analysis.html#important-note-regarding-the-tracing-rule",
    "title": "🎶 Path Analysis",
    "section": "Important Note Regarding the Tracing Rule",
    "text": "Important Note Regarding the Tracing Rule\nNote that when we are considering tracings, we do not have to worry about the direction of the arrow, only that there is a path we can trace. The only rule regarding arrowheads is that a variable can not be both entered and exited through an arrowhead."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#back-to-estimating-the-path-coeffients",
    "href": "notes/03-02-path-analysis.html#back-to-estimating-the-path-coeffients",
    "title": "🎶 Path Analysis",
    "section": "Back to Estimating the Path Coeffients",
    "text": "Back to Estimating the Path Coeffients\nEach tracing yields a product of the path coefficents used in the tracing. Thus the first tracing is \\(p1\\) (there is only one path, so the product is simply the path), and the second tracing yields \\(p2 \\times p3\\). Since the tracing rule says that the correlation between academic ability and achievement is equal to the sum of the products yielded by the tracings, we know that:\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ achievement}} &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + p2(p3)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#your-turn",
    "href": "notes/03-02-path-analysis.html#your-turn",
    "title": "🎶 Path Analysis",
    "section": "Your Turn",
    "text": "Your Turn\nUse the tracing rule to write two more equations. The first equation should represent the correlation between motivation and achievement, and the second equation should represent the correlation between ability and motivation."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#check-your-work",
    "href": "notes/03-02-path-analysis.html#check-your-work",
    "title": "🎶 Path Analysis",
    "section": "Check Your Work",
    "text": "Check Your Work\n\n\n\n\n\n\n\n\n\n\n\n\nTo represent the correlation between motivation and achievement we have two potential tracings:\n\nStart at the motivation and take p3 to the achievement variable.\nStart at the motivation variable and take p2 to the academic ability variable, then take p1 to the achievement variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{motivation,~ achievement}} &= p3 + p2(p1) \\\\[1em]\n.255&= p3 + p2(p1)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#check-your-work-1",
    "href": "notes/03-02-path-analysis.html#check-your-work-1",
    "title": "🎶 Path Analysis",
    "section": "Check Your Work",
    "text": "Check Your Work\n\n\n\n\n\n\n\n\n\n\n\n\nTo represent the correlation between academic ability and motivation we have one tracing:\n\nStart at the academic ability and take p2 to the motivation variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ motivation}} &= p2 \\\\[1em]\n.205&= p2\n\\end{split}\n\\]\nQUESTION: Why can’t we use the tracing that takes p1 from academic ability to achievement then takes p3 to motivation??"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#solving-for-p1-p2-and-p3",
    "href": "notes/03-02-path-analysis.html#solving-for-p1-p2-and-p3",
    "title": "🎶 Path Analysis",
    "section": "Solving for p1, p2, and p3",
    "text": "Solving for p1, p2, and p3\n\nEquationsp1p3Back to p1\n\n\nWe now have three equations with three unknowns. We can solve this system of equations to find p1, p2, and p3.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.255 &= p3 + p2(p1) \\\\[1em]\n.205 &= p2\n\\end{split}\n\\]\n\n\nSubstitute in .205 for p2 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(p3) \\\\[1em]\np1 &= .737 - .205(p3)\n\\end{split}\n\\]\n\n\nNow substitute .205 in for p2 and \\(.737 - .205(p3)\\) in for p1 in the second equation and solve for p3.\n\\[\n\\begin{split}\n.255 &= p3 + p2(p1) \\\\[1em]\n.255 &= p3 + .205(.737 - .205(p3)) \\\\[1em]\n.255 &= p3 + 0.151085 - 0.042025(p3) \\\\[1em]\n0.103915 &= 0.957975(p3) \\\\[1em]\np3 &= .108\n\\end{split}\n\\]\n\n\nNow substitute .205 in for p2 and .108 in for p3 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(.108) \\\\[1em]\n.737 &= p1 + 0.02214 \\\\[1em]\np1 &= .715\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#path-diagram-with-estimated-path-coefficients",
    "href": "notes/03-02-path-analysis.html#path-diagram-with-estimated-path-coefficients",
    "title": "🎶 Path Analysis",
    "section": "Path Diagram with Estimated Path Coefficients",
    "text": "Path Diagram with Estimated Path Coefficients"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#interpreting-path-coefficients",
    "href": "notes/03-02-path-analysis.html#interpreting-path-coefficients",
    "title": "🎶 Path Analysis",
    "section": "Interpreting Path Coefficients",
    "text": "Interpreting Path Coefficients\n\n\n\n\n\n\n\n\n\n\n\n\nPath coefficients are standardized coefficients which can be interpreted similar to standardized regression coefficients.\n\nGiven the adequacy of the path model, each 1-standard deviation increase in motivation increases achievement by .108 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases achievement by .715 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases motivation by .205 standard deviations, on average."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#interpreting-path-coefficients-cntd.",
    "href": "notes/03-02-path-analysis.html#interpreting-path-coefficients-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Interpreting Path Coefficients (cntd.)",
    "text": "Interpreting Path Coefficients (cntd.)\nThere are two things to note when we interpret path coefficients that are different from when we interpret regression coefficients.\n\nWe include “given the adequacy of the model” in our interpretation. This is important because the values of the path coefficients absolutely depend on the weak causal model specified (i.e., how you draw the path diagram).\nBecause we are positing a causal relationship, we can use the causal type language in our interpretation. E.g., an increase in X leads to an increase in Y.\n\nNote also that the interpretation is not controlling for anything. These are simple relationships that we are interpreting."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-path-coefficients-via-regression",
    "href": "notes/03-02-path-analysis.html#estimating-path-coefficients-via-regression",
    "title": "🎶 Path Analysis",
    "section": "Estimating Path Coefficients via Regression",
    "text": "Estimating Path Coefficients via Regression\n\nWe can also find the path coefficients using regression rather than algebra. To determine the path coefficients in the weak causal model, we fit a set of regression models using the “causes” as predictors of any particular effect.\n\n\n\n\n\n\n\n\n\n\nIn our path diagram there are two effects, so we would need to fit two separate regression models. The syntax for these models would be:\nachievement ~ 0 + ability + motivation\nmotivation ~ 0 + ability"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-path-coefficients-via-regression-cntd.",
    "href": "notes/03-02-path-analysis.html#estimating-path-coefficients-via-regression-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Estimating Path Coefficients via Regression (cntd.)",
    "text": "Estimating Path Coefficients via Regression (cntd.)\n\n# Path coefficients for paths to achievement\ntidy(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 2 × 5\n  term       estimate std.error statistic   p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ability       0.715    0.0216     33.1  8.26e-163\n2 motivation    0.108    0.0216      5.02 5.97e-  7\n\n\n\n\n# Path coefficient for paths to motivation\ntidy(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 5\n  term    estimate std.error statistic  p.value\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ability    0.205    0.0310      6.62 5.85e-11"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#residual-variation",
    "href": "notes/03-02-path-analysis.html#residual-variation",
    "title": "🎶 Path Analysis",
    "section": "Residual Variation",
    "text": "Residual Variation\nEven if the causal relationships were specified correctly, the model likely does not include ALL of the causes for motivation and achievement. There is also unaccounted for variation due to random variation and measurement error. To account for these three sources of variation in the weak causal model, we will add an error term to eavch of the effects in the path model."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-residual-variation",
    "href": "notes/03-02-path-analysis.html#estimating-residual-variation",
    "title": "🎶 Path Analysis",
    "section": "Estimating Residual Variation",
    "text": "Estimating Residual Variation\nWe can also estimate the path coefficients for the error terms. These path coefficients are computed as,\n\\[\n\\epsilon_k = \\sqrt{1 - R^2_k}\n\\] where, \\(R^2_k\\) is the \\(R^2\\)-value from the regression model fitted to compute the initial path coefficients."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#estimating-residual-variation-cntd.",
    "href": "notes/03-02-path-analysis.html#estimating-residual-variation-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Estimating Residual Variation (cntd.)",
    "text": "Estimating Residual Variation (cntd.)\n\n\n\n# Path coefficients for error term on achievement\nglance(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.554         0.554 0.668      621. 6.36e-176     2 -1014. 2034. 2049.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .554)\n\n[1] 0.6678323\n\n\n\n\n# Path coefficient for error term on motivation\nglance(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0420        0.0411 0.979      43.8 5.85e-11     1 -1397. 2798. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .0420)\n\n[1] 0.9787747"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#how-do-we-draw-path-diagram",
    "href": "notes/03-02-path-analysis.html#how-do-we-draw-path-diagram",
    "title": "🎶 Path Analysis",
    "section": "How Do We Draw Path Diagram?",
    "text": "How Do We Draw Path Diagram?\n\nTheory/prior research\nLogic/expert understanding/common sense\n\nWe also need to pay attention to time precedence. Cause does not operate backward in time. In our example, academic ability is well-documented as something that does not change after grade school. Thus, it would precede motivation and achievement in time."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#a-little-more-about-cause",
    "href": "notes/03-02-path-analysis.html#a-little-more-about-cause",
    "title": "🎶 Path Analysis",
    "section": "A Little More About Cause",
    "text": "A Little More About Cause\nWhat do we mean when we say “X causes Y”? Contrary to popular belief, we do not mean that changing X has a direct, and immediate change on Y. For example, it is now well known that smoking causes lung cancer.\n\nBut, not everyone who smokes ends up getting lung cancer."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#a-little-more-about-cause-cntd.",
    "href": "notes/03-02-path-analysis.html#a-little-more-about-cause-cntd.",
    "title": "🎶 Path Analysis",
    "section": "A Little More About Cause (cntd.)",
    "text": "A Little More About Cause (cntd.)\nInstead, cause is a probabilistic statement about the world. When we say smoking causes lung cancer what we mean, statistically, is that smoking increases the probability of developing lung cancer.\n\nMoreover, this increased probability is due to smoking, and not something else."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#requirements-for-cause",
    "href": "notes/03-02-path-analysis.html#requirements-for-cause",
    "title": "🎶 Path Analysis",
    "section": "Requirements for Cause",
    "text": "Requirements for Cause\nThere are three primary requirements for a causal relationship between X and Y.\n\nThere must be a relationship between X and Y. (Correlation is a necessary component of causation.)\nTime precedence (Causes must occur prior to effects.)\nRelationship must not be spurious."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#including-time-precedence-in-the-path-diagram",
    "href": "notes/03-02-path-analysis.html#including-time-precedence-in-the-path-diagram",
    "title": "🎶 Path Analysis",
    "section": "Including Time Precedence in the Path Diagram",
    "text": "Including Time Precedence in the Path Diagram\nTime precedence is often reflected in the orientation of the model. Variables that occur earlier in time are oriented further to the left side of the diagram than variables that occur later.\n\n\n\n\n\n\n\n\n\nIn the smoking example, to be considered the cause for lung cancer, smoking has to occur prior to the onset of lung cancer.1\nNote that the top–bottom orientation typically doesn’t mean anything—it is just used for aesthetics in the layout."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#including-time-precedence-in-the-path-diagram-cntd.",
    "href": "notes/03-02-path-analysis.html#including-time-precedence-in-the-path-diagram-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Including Time Precedence in the Path Diagram (cntd.)",
    "text": "Including Time Precedence in the Path Diagram (cntd.)\nIn our path diagram relating academic ability and motivation to acheivement, academic ability is furthest to the left (it occurs earliest), followed by motivation, and then academic achievement."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#common-cause",
    "href": "notes/03-02-path-analysis.html#common-cause",
    "title": "🎶 Path Analysis",
    "section": "Common Cause",
    "text": "Common Cause\nA common cause is a variable that is a cause of both X and Y which accounts for the relationship between X and Y. Consider the following example where we want to look at the causal impact of participation in Head Start programs on academic achievement.\n\nIn practice, the path coefficient tends to be negative. That is Head Start participants tend to have lower academic achievement than their non-Head Start peers."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#common-cause-cntd.",
    "href": "notes/03-02-path-analysis.html#common-cause-cntd.",
    "title": "🎶 Path Analysis",
    "section": "Common Cause (cntd.)",
    "text": "Common Cause (cntd.)\nA common cause of both Head Start participation and academic achievement is poverty. This is shown below.\n\nOnce we include poverty as a common cause, the path coefficient between Head Start participation and academic achievement switches direction. That is, after including poverty in the path model, Head Start participants tend to have higher academic achievement than their non-Head Start peers. This is akin to how effects in a regression model might change after we control for other predictors."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#common-cause-cntd.-1",
    "href": "notes/03-02-path-analysis.html#common-cause-cntd.-1",
    "title": "🎶 Path Analysis",
    "section": "Common Cause (cntd.)",
    "text": "Common Cause (cntd.)\nIn order to meet the third causal requirement, we need to include ALL common causes in the model. This requirement is the hardest to prove."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#non-recursive-models",
    "href": "notes/03-02-path-analysis.html#non-recursive-models",
    "title": "🎶 Path Analysis",
    "section": "Non-Recursive Models",
    "text": "Non-Recursive Models\nIn the path diagrams we have looked at so far, the causal paths have gone in only one direction; there is a distinct cause and effect. The error terms on the effect variables are also all uncorrelated. These are called recursive models. It is possible for variables to effect each other (e.g., predator–prey relationships), or for the error terms to be correlated. This is called a non-recursive model."
  },
  {
    "objectID": "notes/03-02-path-analysis.html#under-identified-models",
    "href": "notes/03-02-path-analysis.html#under-identified-models",
    "title": "🎶 Path Analysis",
    "section": "Under-Identified Models",
    "text": "Under-Identified Models\nThe problem with estimating the path coefficients in the non-recursive model is it is under-identified. In our predator–prey model, we need to estimate four path coefficients, but only have three correlations (equations) from which to do so. We can’t solve this without adding additional constraints!\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{split}\nr_{\\mathrm{H,W}} &= p1 + p2(p4) \\\\[1em]\nr_{\\mathrm{H,R}} &= p2 + p1(p3) \\\\[1em]\nr_{\\mathrm{H,W}} &= p3 + p4 + p1(p2)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis.html#over-identified-models",
    "href": "notes/03-02-path-analysis.html#over-identified-models",
    "title": "🎶 Path Analysis",
    "section": "Over-Identified Models",
    "text": "Over-Identified Models"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html",
    "href": "notes/03-02-path-analysis-students.html",
    "title": "Path Analysis",
    "section": "",
    "text": "Path diagrams are visual depictions of the hypothesized relationships between a set of variables. In this set of notes, we will introduce some of the ideas and concepts related to path diagrams, and the estimation of effects depicted in a path diagram (i.e., path analysis)."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#what-are-path-diagrams",
    "href": "notes/03-02-path-analysis-students.html#what-are-path-diagrams",
    "title": "Path Analysis",
    "section": "",
    "text": "Path diagrams are visual depictions of the hypothesized relationships between a set of variables. In this set of notes, we will introduce some of the ideas and concepts related to path diagrams, and the estimation of effects depicted in a path diagram (i.e., path analysis)."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#example",
    "href": "notes/03-02-path-analysis-students.html#example",
    "title": "Path Analysis",
    "section": "Example",
    "text": "Example\nTo help us in this endeavor, we will consider three potential student-level variables for our path model:\n\nAchievement\nAcademic Ability\nMotivation"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#simulating-data",
    "href": "notes/03-02-path-analysis-students.html#simulating-data",
    "title": "Path Analysis",
    "section": "Simulating Data",
    "text": "Simulating Data\n\n# Load Libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(corrr)\n\n# Create correlation matrix\ncorrs = matrix(\n  data = c(\n    1.000, 0.737, 0.255,\n    0.737, 1.000, 0.205,\n    0.255, 0.205, 1.000\n  ),\n  nrow = 3\n)\n\n# Create mean vector\nmeans = c(0, 0, 0) \n\n# Set sample size\nn = 1000           \n\n# Make simulation reproducible\nset.seed(1)\n\n\n# Simulate the data and convert to data frame\nsim_dat &lt;- data.frame(MASS::mvrnorm(n = 1000, mu = means, Sigma = corrs, empirical = TRUE)) |&gt;\n  rename(\n    achievement = X1,\n    ability = X2, \n    motivation = X3\n  )\n\n\n# View simulated data\nhead(sim_dat)\n\n  achievement    ability motivation\n1  0.33527960 -0.1297477  1.4114639\n2  0.99197530 -0.3813547  1.2637617\n3 -0.70788322  0.6913426 -0.5557938\n4 -0.87083130 -1.3095554 -1.1570792\n5 -0.05935273 -0.2583549 -0.1541515\n6  0.38943125  1.4254429 -0.6394095"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#reading-a-path-diagram",
    "href": "notes/03-02-path-analysis-students.html#reading-a-path-diagram",
    "title": "Path Analysis",
    "section": "Reading a Path Diagram",
    "text": "Reading a Path Diagram\nIn a path diagram,\n\nVariables we have data on (i.e., manifest or measured variables) are represented by squares or rectangles.\nLines between the variables indicate a relationship between the two connected variables. The lines between the different variables are called paths.\n\nIf the path has an arrowhead on both sides, it indicates the two connected variables are related (i.e., correlated).\nIf the path has an arrowhead on only one side, it indicates a hypothesized causal relationship. The cause is in the direction of the arrow; that is, the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Several symbols seen in path models."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#two-example-path-diagrams",
    "href": "notes/03-02-path-analysis-students.html#two-example-path-diagrams",
    "title": "Path Analysis",
    "section": "Two Example Path Diagrams",
    "text": "Two Example Path Diagrams\nIn both of the examples below, there are three manifest/measured variables (academic ability, motivation, and achievement) and three paths (p1, p2, and p3). The difference between the two is that in the first diagram, there are hypothesized causal relationships, while in the second diagram the relationships are not presumed to be causal.\n\n\n\n\n\n\n\n\nFigure 2: All paths represent correlations (no causal paths) between the manifest variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The causal paths between the manifest variables indicate a weak causal ordering."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#path-diagram-1",
    "href": "notes/03-02-path-analysis-students.html#path-diagram-1",
    "title": "Path Analysis",
    "section": "Path Diagram 1",
    "text": "Path Diagram 1\nThe model depicted in this path diagram posits that there are relationships between:\n\nAcademic ability and achievement (p1);\nAcademic ability and motivation (p2); and\nMotivation and achievement (p3).\n\n\n\n\n\n\n\n\n\nFigure 4: All paths represent correlations (no causal paths) between the manifest variables.\n\n\n\n\n\nThe double-headed arrows on the paths indicate that the variables are related (i.e., correlated), although there is no causal direction hypothesized.\n\nEstimating Path Coefficients in Path Diagram 1\nIn a path analysis, one goal is to estimate path coefficients. In this model, since the paths represent the relationship between two variables, the path coefficients are simply the bivariate correlations between each set of variables.\n\n# Compute correlations\nsim_dat |&gt;\n  correlate()\n\n# A tibble: 3 × 4\n  term        achievement ability motivation\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 achievement      NA       0.737      0.255\n2 ability           0.737  NA          0.205\n3 motivation        0.255   0.205     NA    \n\n\n\n\n\n\n\n\n\n\nFigure 5: All paths represent correlations (no causal paths) between the manifest variables. The correlations are also shown on the model."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#weak-causal-ordering",
    "href": "notes/03-02-path-analysis-students.html#weak-causal-ordering",
    "title": "Path Analysis",
    "section": "Weak Causal Ordering",
    "text": "Weak Causal Ordering\nIn path modeling, we often assert weak causal ordering on the variables in the model. With weak causal ordering, we are not saying X causes Y, but rather that if X and Y are casually related, one of these variables (typically X) is the cause of the other.\n\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nIn this model, the paths have arrows at only one end indicating the hypothesized causal relationship. The cause is in the direction of the arrow, that is the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line.\n\nAny weak causal model should be posited prior to collecting or looking at the data! Creating this model can also help you identify the data to collect.\n\nThe second path diagram from above represented the hypothesized causes and effects between the variables in the model.\n\n\n\n\n\n\n\n\nFigure 6: The causal paths between the manifest variables indicate a weak causal ordering.\n\n\n\n\n\nThe model depicted in this path diagram posits the following causal relationships:\n\nIf academic ability and achievement are causally related, then academic ability is the cause of achievement.\nIf academic ability and motivation are causally related, then academic ability is the cause of motivation.\nIf motivation and achievement are causally related, then motivation is the cause of achievement.\n\n\n\nEstimating Path Coefficients in a Weak Causal Model\nAgain, we would want to estimate the path coefficients shown in the diagram for the weak causal model. In causal models, the path coefficients are not necessarily the correlations. To find the path coefficients we need to use the tracing rule. The tracing rule indicates that:\n\nTRACING RULE\nThe correlation between two variables X and Y is equal to the sum of the possible products of all possible paths from each possible tracing from X to Y, with the following two exceptions:\n\nThe same variable is not entered more than once in the same tracing.\nA variable is not both entered and exited through an arrowhead.\n\n\n\n\n\nExample of the Tracing Rule\nAs an example, consider all the tracings (routes) that allow us to start at the academic ability variable and go to the achievement variable in the path diagram at right.\n\n\n\n\n\n\n\n\nFigure 7: The causal paths between the manifest variables indicate a weak causal ordering.\n\n\n\n\n\nThere are two possible tracings that conform to the tracing rule:\n\nStart at the academic ability variable and take p1 to the achievement variable.\nStart at the academic ability variable and take p2 to the motivation variable, then take p3 to the achievement variable.\n\nSimilarly, we could have started at the achievement variable and determined the tracings to get to the academic ability variable:\n\nStart at the achievement variable and take p1 to the academic ability variable.\nStart at the achievement variable and take p3 to the motivation variable, then take p2 to the academic ability variable.\n\n\nIMPORTANT\nNote that when we are considering tracings, we do not have to worry about the direction of the arrow, only that there is a path we can trace. The only rule regarding arrowheads is that a variable can not be both entered and exited through an arrowhead.\n\n\n\n\nBack to Estimating the Path Coeffients\nEach tracing yields a product of the path coefficents used in the tracing. Thus the first tracing is \\(p1\\) (there is only one path, so the product is simply the path), and the second tracing yields \\(p2 \\times p3\\). Since the tracing rule says that the correlation between academic ability and achievement is equal to the sum of the products yielded by the tracings, we know that:\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ achievement}} &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + p2(p3)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#your-turn",
    "href": "notes/03-02-path-analysis-students.html#your-turn",
    "title": "Path Analysis",
    "section": "Your Turn",
    "text": "Your Turn\nUse the tracing rule to write two more equations. The first equation should represent the correlation between motivation and achievement, and the second equation should represent the correlation between ability and motivation."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#check-your-work",
    "href": "notes/03-02-path-analysis-students.html#check-your-work",
    "title": "Path Analysis",
    "section": "Check Your Work",
    "text": "Check Your Work\nTo represent the correlation between motivation and achievement we have two potential tracings:\n\nStart at the motivation and take p3 to the achievement variable.\nStart at the motivation variable and take p2 to the academic ability variable, then take p1 to the achievement variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{motivation,~ achievement}} &= p3 + p2(p1) \\\\[1em]\n.255&= p3 + p2(p1)\n\\end{split}\n\\]\nTo represent the correlation between academic ability and motivation we have one tracing:\n\nStart at the academic ability and take p2 to the motivation variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ motivation}} &= p2 \\\\[1em]\n.205&= p2\n\\end{split}\n\\]\n\nQUESTION\nWhy can’t we use the tracing that takes p1 from academic ability to achievement then takes p3 to motivation??"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#check-your-work-1",
    "href": "notes/03-02-path-analysis-students.html#check-your-work-1",
    "title": "Path Analysis",
    "section": "Check Your Work",
    "text": "Check Your Work\n\n\n\n\n\n\n\n\n\n\n\n\nTo represent the correlation between academic ability and motivation we have one tracing:\n\nStart at the academic ability and take p2 to the motivation variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ motivation}} &= p2 \\\\[1em]\n.205&= p2\n\\end{split}\n\\]\nQUESTION: Why can’t we use the tracing that takes p1 from academic ability to achievement then takes p3 to motivation??"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#solving-for-p1-p2-and-p3",
    "href": "notes/03-02-path-analysis-students.html#solving-for-p1-p2-and-p3",
    "title": "Path Analysis",
    "section": "Solving for p1, p2, and p3",
    "text": "Solving for p1, p2, and p3\nWe now have three equations with three unknowns. We can solve this system of equations to find p1, p2, and p3.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.255 &= p3 + p2(p1) \\\\[1em]\n.205 &= p2\n\\end{split}\n\\]\nTo solve for p1, substitute in .205 for p2 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(p3) \\\\[1em]\np1 &= .737 - .205(p3)\n\\end{split}\n\\]\nTo solve for p3, substitute .205 in for p2 and \\(.737 - .205(p3)\\) in for p1 in the second equation and solve for p3.\n\\[\n\\begin{split}\n.255 &= p3 + p2(p1) \\\\[1em]\n.255 &= p3 + .205(.737 - .205(p3)) \\\\[1em]\n.255 &= p3 + 0.151085 - 0.042025(p3) \\\\[1em]\n0.103915 &= 0.957975(p3) \\\\[1em]\np3 &= .108\n\\end{split}\n\\]\nFinally, to solve for p1, substitute .205 in for p2 and .108 in for p3 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(.108) \\\\[1em]\n.737 &= p1 + 0.02214 \\\\[1em]\np1 &= .715\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nFigure 8: The path model between the manifest variables with the path coefficients."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#path-diagram-with-estimated-path-coefficients",
    "href": "notes/03-02-path-analysis-students.html#path-diagram-with-estimated-path-coefficients",
    "title": "Path Analysis",
    "section": "Path Diagram with Estimated Path Coefficients",
    "text": "Path Diagram with Estimated Path Coefficients"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#interpreting-path-coefficients",
    "href": "notes/03-02-path-analysis-students.html#interpreting-path-coefficients",
    "title": "Path Analysis",
    "section": "Interpreting Path Coefficients",
    "text": "Interpreting Path Coefficients\nPath coefficients are standardized coefficients which can be interpreted similar to standardized regression coefficients.\n\nGiven the adequacy of the path model, each 1-standard deviation increase in motivation increases achievement by .108 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases achievement by .715 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases motivation by .205 standard deviations, on average.\n\nThere are two things to note when we interpret path coefficients that are different from when we interpret regression coefficients.\n\nWe include “given the adequacy of the model” in our interpretation. This is important because the values of the path coefficients absolutely depend on the weak causal model specified (i.e., how you draw the path diagram).\nBecause we are positing a causal relationship, we can use the causal type language in our interpretation. E.g., an increase in X leads to an increase in Y.\n\nNote also that the interpretation is not controlling for anything. These are simple relationships that we are interpreting."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#interpreting-path-coefficients-cntd.",
    "href": "notes/03-02-path-analysis-students.html#interpreting-path-coefficients-cntd.",
    "title": "Path Analysis",
    "section": "Interpreting Path Coefficients (cntd.)",
    "text": "Interpreting Path Coefficients (cntd.)\nThere are two things to note when we interpret path coefficients that are different from when we interpret regression coefficients.\n\nWe include “given the adequacy of the model” in our interpretation. This is important because the values of the path coefficients absolutely depend on the weak causal model specified (i.e., how you draw the path diagram).\nBecause we are positing a causal relationship, we can use the causal type language in our interpretation. E.g., an increase in X leads to an increase in Y.\n\nNote also that the interpretation is not controlling for anything. These are simple relationships that we are interpreting."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#estimating-path-coefficients-via-regression",
    "href": "notes/03-02-path-analysis-students.html#estimating-path-coefficients-via-regression",
    "title": "Path Analysis",
    "section": "Estimating Path Coefficients via Regression",
    "text": "Estimating Path Coefficients via Regression\nWe can also find the path coefficients using regression rather than algebra. To determine the path coefficients in the weak causal model, we fit a set of regression models using the “causes” as predictors of any particular effect.\nIn our weak causal path diagram there are two effects, so we would need to fit two separate regression models. The syntax for these models would be:\nachievement ~ 0 + ability + motivation\nmotivation ~ 0 + ability\n\n# Path coefficients for paths to achievement\ntidy(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 2 × 5\n  term       estimate std.error statistic   p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ability       0.715    0.0216     33.1  8.26e-163\n2 motivation    0.108    0.0216      5.02 5.97e-  7\n\n# Path coefficient for paths to motivation\ntidy(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 5\n  term    estimate std.error statistic  p.value\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ability    0.205    0.0310      6.62 5.85e-11"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#estimating-path-coefficients-via-regression-cntd.",
    "href": "notes/03-02-path-analysis-students.html#estimating-path-coefficients-via-regression-cntd.",
    "title": "Path Analysis",
    "section": "Estimating Path Coefficients via Regression (cntd.)",
    "text": "Estimating Path Coefficients via Regression (cntd.)\n\n# Path coefficients for paths to achievement\ntidy(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 2 × 5\n  term       estimate std.error statistic   p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ability       0.715    0.0216     33.1  8.26e-163\n2 motivation    0.108    0.0216      5.02 5.97e-  7\n\n\n\n\n# Path coefficient for paths to motivation\ntidy(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 5\n  term    estimate std.error statistic  p.value\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ability    0.205    0.0310      6.62 5.85e-11"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#residual-variation",
    "href": "notes/03-02-path-analysis-students.html#residual-variation",
    "title": "Path Analysis",
    "section": "Residual Variation",
    "text": "Residual Variation\nEven if the causal relationships were specified correctly, the model likely does not include ALL of the causes for motivation and achievement. There is also unaccounted for variation due to random variation and measurement error. To account for these three sources of variation in the weak causal model, we will add an error term to each of the effects in the path model.\n\n\n\n\n\n\n\n\nFigure 9: The path model between the manifest variables with the path coefficients. Error terms representing unaccounted for causes are also included for each effect.\n\n\n\n\n\nWe can also estimate the path coefficients for the error terms. These path coefficients are computed as,\n\\[\n\\epsilon_k = \\sqrt{1 - R^2_k}\n\\] where, \\(R^2_k\\) is the \\(R^2\\)-value from the regression model fitted to compute the initial path coefficients.\n\n# Path coefficients for error term on achievement\nglance(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.554         0.554 0.668      621. 6.36e-176     2 -1014. 2034. 2049.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .554)\n\n[1] 0.6678323\n\n# Path coefficient for error term on motivation\nglance(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0420        0.0411 0.979      43.8 5.85e-11     1 -1397. 2798. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .0420)\n\n[1] 0.9787747\n\n\n\n\n\n\n\n\n\n\nFigure 10: The path model between the manifest variables with the path coefficients. Estimates for the error terms representing unaccounted for causes are also included for each effect."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#estimating-residual-variation",
    "href": "notes/03-02-path-analysis-students.html#estimating-residual-variation",
    "title": "Path Analysis",
    "section": "Estimating Residual Variation",
    "text": "Estimating Residual Variation\nWe can also estimate the path coefficients for the error terms. These path coefficients are computed as,\n\\[\n\\epsilon_k = \\sqrt{1 - R^2_k}\n\\] where, \\(R^2_k\\) is the \\(R^2\\)-value from the regression model fitted to compute the initial path coefficients."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#estimating-residual-variation-cntd.",
    "href": "notes/03-02-path-analysis-students.html#estimating-residual-variation-cntd.",
    "title": "Path Analysis",
    "section": "Estimating Residual Variation (cntd.)",
    "text": "Estimating Residual Variation (cntd.)\n\n\n\n# Path coefficients for error term on achievement\nglance(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.554         0.554 0.668      621. 6.36e-176     2 -1014. 2034. 2049.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .554)\n\n[1] 0.6678323\n\n\n\n\n# Path coefficient for error term on motivation\nglance(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0420        0.0411 0.979      43.8 5.85e-11     1 -1397. 2798. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .0420)\n\n[1] 0.9787747"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#how-do-we-draw-path-diagram",
    "href": "notes/03-02-path-analysis-students.html#how-do-we-draw-path-diagram",
    "title": "Path Analysis",
    "section": "How Do We Draw Path Diagram?",
    "text": "How Do We Draw Path Diagram?\n\nTheory/prior research\nLogic/expert understanding/common sense\n\nWe also need to pay attention to time precedence. Cause does not operate backward in time. In our example, academic ability is well-documented as something that does not change after grade school. Thus, it would precede motivation and achievement in time."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#a-little-more-about-cause",
    "href": "notes/03-02-path-analysis-students.html#a-little-more-about-cause",
    "title": "Path Analysis",
    "section": "A Little More About Cause",
    "text": "A Little More About Cause\nWhat do we mean when we say “X causes Y”? Contrary to popular belief, we do not mean that changing X has a direct, and immediate change on Y. For example, it is now well known that smoking causes lung cancer.\n\n\n\n\n\n\n\n\nFigure 11: The path model showing that smoking causes cancer.\n\n\n\n\n\nBut, not everyone who smokes ends up getting lung cancer. Instead, cause is a probabilistic statement about the world. When we say smoking causes lung cancer what we mean, statistically, is that smoking increases the probability of developing lung cancer.\nMoreover, this increased probability is due to smoking, and not something else.\nThere are three primary requirements for a causal relationship between X and Y.\n\nThere must be a relationship between X and Y. (Correlation is a necessary component of causation.)\nTime precedence (Causes must occur prior to effects.)\nRelationship must not be spurious."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#a-little-more-about-cause-cntd.",
    "href": "notes/03-02-path-analysis-students.html#a-little-more-about-cause-cntd.",
    "title": "Path Analysis",
    "section": "A Little More About Cause (cntd.)",
    "text": "A Little More About Cause (cntd.)\nInstead, cause is a probabilistic statement about the world. When we say smoking causes lung cancer what we mean, statistically, is that smoking increases the probability of developing lung cancer.\n\nMoreover, this increased probability is due to smoking, and not something else."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#requirements-for-cause",
    "href": "notes/03-02-path-analysis-students.html#requirements-for-cause",
    "title": "Path Analysis",
    "section": "Requirements for Cause",
    "text": "Requirements for Cause\nThere are three primary requirements for a causal relationship between X and Y.\n\nThere must be a relationship between X and Y. (Correlation is a necessary component of causation.)\nTime precedence (Causes must occur prior to effects.)\nRelationship must not be spurious."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#including-time-precedence-in-the-path-diagram",
    "href": "notes/03-02-path-analysis-students.html#including-time-precedence-in-the-path-diagram",
    "title": "Path Analysis",
    "section": "Including Time Precedence in the Path Diagram",
    "text": "Including Time Precedence in the Path Diagram\nTime precedence is often reflected in the orientation of the model. Variables that occur earlier in time are oriented further to the left side of the diagram than variables that occur later.\n\n\n\n\n\n\n\n\nFigure 12: The path model showing that smoking causes cancer. The cause (smoking) is to the left of the effect (cancer).\n\n\n\n\n\nIn the smoking example, to be considered the cause for lung cancer, smoking has to occur prior to the onset of lung cancer.1\nIn our path diagram relating academic ability and motivation to acheivement, academic ability is furthest to the left (it occurs earliest), followed by motivation, and then academic achievement.\n\n\n\n\n\n\n\n\nFigure 13: The path model showing the effect of academic ability and motivation on acheivement."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#including-time-precedence-in-the-path-diagram-cntd.",
    "href": "notes/03-02-path-analysis-students.html#including-time-precedence-in-the-path-diagram-cntd.",
    "title": "Path Analysis",
    "section": "Including Time Precedence in the Path Diagram (cntd.)",
    "text": "Including Time Precedence in the Path Diagram (cntd.)\nIn our path diagram relating academic ability and motivation to acheivement, academic ability is furthest to the left (it occurs earliest), followed by motivation, and then academic achievement."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#common-cause",
    "href": "notes/03-02-path-analysis-students.html#common-cause",
    "title": "Path Analysis",
    "section": "Common Cause",
    "text": "Common Cause\nA common cause is a variable that is a cause of both X and Y which accounts for the relationship between X and Y. Consider the following example where we want to look at the causal impact of participation in Head Start programs on academic achievement.\n\n\n\n\n\n\n\n\nFigure 14: The path model showing the effect of Head Start enrollment on acheivement.\n\n\n\n\n\nIn practice, the path coefficient tends to be negative. That is Head Start participants tend to have lower academic achievement than their non-Head Start peers.\nA common cause of both Head Start participation and academic achievement is poverty. This is shown below.\n\n\n\n\n\n\n\n\nFigure 15: The path model showing the effect of Head Start enrollment on acheivement. Poverty is a common cause of both Head Start enrollment and acheivement.\n\n\n\n\n\nOnce we include poverty as a common cause, the path coefficient between Head Start participation and academic achievement switches direction. That is, after including poverty in the path model, Head Start participants tend to have higher academic achievement than their non-Head Start peers. This is akin to how effects in a regression model might change after we control for other predictors.\n\nIMPORTANT\nIn order to meet the third causal requirement, we need to include ALL common causes in the model. This requirement is the hardest to meet!"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#common-cause-cntd.",
    "href": "notes/03-02-path-analysis-students.html#common-cause-cntd.",
    "title": "Path Analysis",
    "section": "Common Cause (cntd.)",
    "text": "Common Cause (cntd.)\nA common cause of both Head Start participation and academic achievement is poverty. This is shown below.\n\n\n\n\n\n\n\n\n\nOnce we include poverty as a common cause, the path coefficient between Head Start participation and academic achievement switches direction. That is, after including poverty in the path model, Head Start participants tend to have higher academic achievement than their non-Head Start peers. This is akin to how effects in a regression model might change after we control for other predictors."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#common-cause-cntd.-1",
    "href": "notes/03-02-path-analysis-students.html#common-cause-cntd.-1",
    "title": "Path Analysis",
    "section": "Common Cause (cntd.)",
    "text": "Common Cause (cntd.)\nIn order to meet the third causal requirement, we need to include ALL common causes in the model. This requirement is the hardest to prove."
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#non-recursive-models",
    "href": "notes/03-02-path-analysis-students.html#non-recursive-models",
    "title": "Path Analysis",
    "section": "Non-Recursive Models",
    "text": "Non-Recursive Models\nIn the path diagrams we have looked at so far, the causal paths have gone in only one direction; there is a distinct cause and effect. The error terms on the effect variables are also all uncorrelated. These are called recursive models. It is possible for variables to effect each other (e.g., predator–prey relationships), or for the error terms to be correlated. This is called a non-recursive model.\n\n\n\n\n\n\n\n\nFigure 16: The path model showing the effect of habitat on wolf and rabbit populations. The model is non-recursive since the wolf and rabbit populations effect each other.\n\n\n\n\n\n\n\nUnder-Identified Models\nThe problem with estimating the path coefficients in the non-recursive model is it is under-identified. In our predator–prey model, we need to estimate four path coefficients, but only have three correlations (equations) from which to do so. We can’t solve this without adding additional constraints!\n\\[\n\\begin{split}\nr_{\\mathrm{H,W}} &= p1 + p2(p4) \\\\[1em]\nr_{\\mathrm{H,R}} &= p2 + p1(p3) \\\\[1em]\nr_{\\mathrm{H,W}} &= p3 + p4 + p1(p2)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#under-identified-models",
    "href": "notes/03-02-path-analysis-students.html#under-identified-models",
    "title": "Path Analysis",
    "section": "Under-Identified Models",
    "text": "Under-Identified Models\nThe problem with estimating the path coefficients in the non-recursive model is it is under-identified. In our predator–prey model, we need to estimate four path coefficients, but only have three correlations (equations) from which to do so. We can’t solve this without adding additional constraints!\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{split}\nr_{\\mathrm{H,W}} &= p1 + p2(p4) \\\\[1em]\nr_{\\mathrm{H,R}} &= p2 + p1(p3) \\\\[1em]\nr_{\\mathrm{H,W}} &= p3 + p4 + p1(p2)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#over-identified-models",
    "href": "notes/03-02-path-analysis-students.html#over-identified-models",
    "title": "Path Analysis",
    "section": "Over-Identified Models",
    "text": "Over-Identified Models"
  },
  {
    "objectID": "notes/03-02-path-analysis-students.html#footnotes",
    "href": "notes/03-02-path-analysis-students.html#footnotes",
    "title": "Path Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the top–bottom orientation typically doesn’t mean anything—it is just used for aesthetics in the layout.↩︎"
  },
  {
    "objectID": "notes/03-03-more-path-analysis.html",
    "href": "notes/03-03-more-path-analysis.html",
    "title": "More Path Analysis",
    "section": "",
    "text": "In this set of notes, we will use the data from path-model-achievement.csv to fit a path model to explain effects on high school achievement. The data were simulated to include attributes for 1000 students from information provided by Keith (2015).\n\n[CSV]\n[R Script File]\n\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(lavaan)\n\n# Import data\nkeith = read_csv(\"https://raw.githubusercontent.com/zief0002/redesigned-adventure/main/data/path-model-achievement.csv\")\nkeith\n\n# A tibble: 1,000 × 5\n   achieve ability motivation coursework fam_back\n     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.511   0.656      -1.10       0.130   0.300 \n 2 -0.569   0.273      -1.48      -1.68   -0.419 \n 3  0.997  -0.402      -0.284      1.29   -0.334 \n 4  0.681   1.31        1.77       0.801   1.14  \n 5 -0.838   0.281       1.51       0.415   0.872 \n 6  0.206  -0.884      -0.128      0.591  -1.29  \n 7  1.07    1.92       -0.405      1.41    0.529 \n 8 -0.418  -0.857       0.207     -1.59   -2.23  \n 9  0.0460 -0.0173     -1.40      -1.58   -2.72  \n10  0.742   0.312      -1.82      -0.400  -0.0957\n# ℹ 990 more rows\n\n\n\n\nHypothesized Path Model\nThe hypothesized path model is shown below. This model was based on previous literature related to student achievement and on Keith’s experiences as an educator and educational researcher.\n\n\n\n\n\n\n\n\nFigure 1: Hypothesized path model for several predictors of high school achievement.\n\n\n\n\n\n\n\n\nEstimating the Path Coefficients\nWe need to estimate 10 path coefficients between the measured variables and four additional paths between unmeasured variables and each of the measured effects. To do this we can use a regression model that includes the hypothesized causes to predict variation in each of the effects. The regression coefficients obtained are the path coefficients between each of the hypothesized causes and effect.\nFor example, the variable high school motivation is an effect in our model (i.e., it has arrows going into it). The hypothesized causes of high school motivation are family background, and academic ability. To obtain those two path coefficients:\n\n# Fit model\nlm.motivation = lm(motivation ~ 1 + fam_back + ability, data = keith)\n\n# Obtain path coefficients\ntidy(lm.motivation)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic    p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept) -1.09e-16    0.0308 -3.54e-15 1.00      \n2 fam_back     1.27e- 1    0.0339  3.74e+ 0 0.000198  \n3 ability      1.52e- 1    0.0339  4.50e+ 0 0.00000777\n\n\nThe path coefficient for the path from family background to high school motivation (p4) is 0.127, and that for the path from academic ability to high school motivation (p7) is 0.152. These represent the direct effects of family background and academic ability on high school motivation, respectively. Here we interpret the direct effect of academic ability on high school motivation:\n\nGiven the adequacy of our model, academic ability has a small positive effect on high school motivation. Each one-standard deviation increase in academic ability will result in a 0.127-standard deviation unit change on high school motivation, on average.\n\nWe can also estimate the path between the unmeasured variables (\\(d_2\\)) and high school motivation. This is computed as:\n\\[\n\\sqrt{1-R^2}\n\\]\nwhere \\(R^2\\) is the coefficient of determination from the fitted regression. In our example\n\n# Obtain R2\nglance(lm.motivation)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0552        0.0534 0.973      29.2 4.97e-13     2 -1390. 2788. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Compute path\nsqrt(1 - 0.0552)\n\n[1] 0.9720082\n\n\nThe path between \\(d_2\\) and high school motivation is 0.972. This represents the effects of all other causes of high school motivation that we did not include in our hypothesized path model. This suggests that many of the influences of high school motivation are unaccounted for by the path model. Presumably, these are not important in positing the causal model. (Note if they are important, they should be included as measured variables in the path model.) The regression models to estimate the remaining paths are provided below (output not given), as well as the path estimates for the hypothesized path model.\n\n# Effect: Academic Ability\nlm.abilty = lm(ability ~ 1 + fam_back, data = sim_dat)\ntidy(lm.abilty)\nglance(lm.abilty)\nsqrt(1 - 0.174)\n\n# Effect: High School Coursework\nlm.coursework = lm(coursework ~ 1 + fam_back + ability + motivation, data = sim_dat)\ntidy(lm.coursework)\nglance(lm.coursework)\nsqrt(1 - 0.348)\n\n# Effect: High School Achievement\nlm.achieve = lm(achieve ~ 1 + fam_back + ability + motivation + coursework, data = sim_dat)\ntidy(achieve)\nglance(lm.achieve)\nsqrt(1 - 0.629)\n\n\n\n\n\n\n\n\n\nFigure 2: Hypothesized path model for several predictors of high school achievement. The path estimates are also included.\n\n\n\n\n\n\n\n\nEstimating Effects\nWe can now use the path coefficients to estimate the effects on high school achievement. Based on the hypothesized path model, each of the measured variables has a direct effect on high school achievement.\n\nThe direct effect of family background on high school achievement is 0.069.\nThe direct effect of academic ability on high school achievement is 0.551.\nThe direct effect of high school motivation on high school achievement is 0.013.\nThe direct effect of high school coursework on high school achievement is 0.310.\n\nThere are also indirect effects of these four causes of high school achievement. For example high school motivation not only directly influences high school achievement, but also influences it by influencing high school coursework (i.e., more motivated students take higher level courses which results in higher achievement).\nEach indirect effect is computed as the product of the path coefficients connecting the potential cause and effect. For example, the indirect effect of high school motivation on high school achievement via high school coursework is:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effect} &= 0.267 \\times 0.310 \\\\[1em]\n&= 0.083\n\\end{split}\n\\]\nFor some of these measured variables there are multiple indirect effects on achievement. The overall indirect effects are computed as the sum of each of the indirect effects. For example the indirect effects of academic ability on high school achievement are:\n\nAcademic ability influences high school coursework which, in turn, influences high school achievement. This indirect effect is \\(0.374 \\times 0.310 = 0.083\\)\nAcademic ability influences high school motivation which, in turn, influences high school achievement. This indirect effect is \\(0.152 \\times 0.013 = 0.002\\)\nAcademic ability influences high school motivation which, in turn, influences high school coursework, which then influences high school achievement. This indirect effect is \\(0.152 \\times 0.267 \\times 0.310 = 0.013\\)\n\nThus the overall indirect effects of academic ability on high school achievement is:\n\\[\n\\begin{split}\n\\mathrm{All~Indirect~Effects} &=  (0.374 \\times 0.310) + (0.152 \\times 0.013) + (0.152 \\times 0.267 \\times 0.310) \\\\[1em]\n&= 0.130\n\\end{split}\n\\]\nThe sum total of the direct and indirect effects give us the total effect of each hypothesized cause. For example, the total effect of academic ability on high school achievement is:\n\\[\n\\begin{split}\n\\mathrm{Total~Effects} &=  0.551 + 0.130 \\\\[1em]\n&= 0.681\n\\end{split}\n\\]\nPutting all of this together,\n\nGiven the adequacy of our model, academic ability has a fairly large, positive effect on high school achievement. Each one-standard deviation increase in academic ability will result in a 0.681-standard deviation unit change in high school achievement, on average. Much of that influence is from the direct effect of academic ability on high school achievement (\\(\\hat\\beta=0.551\\)). A small part of academic ability’s effect on high school achievement is due to its influence on other factors (e.g., motivation) which, in turn, effect high school achievement.\n\nThe table below gives the direct, indirect, and total effects for each of the hypothesized causes of high school achievement.\n\n\n\nStandardized direct, indirect, and total effects of substantive predictors influencing high school achievment.\n\n\n\n\n\n\n\n\n\n\nEffect\n\n\n\nMeasure\nDirect\nIndirect\nTotal\n\n\n\n\nFamily Background\n0.069\n0.348\n0.417\n\n\nAcademic Ability\n0.551\n0.131\n0.682\n\n\nHigh School Motivation\n0.013\n0.083\n0.096\n\n\nHigh School Coursework\n0.310\n---\n0.310\n\n\n\n\n\nThe largest direct influences of high school achievement are from high school coursework and academic ability. Family background also has a large influence on high school achievement, but primarily indirectly. Lastly, high school motivation has a small effect both directly and indirectly on high school achievement.\n\n\n\nFitting the Path Model with lavaan\nThe {lavaan} package includes the sem() function, which can estimate coefficients in a path model. To use this function, we define the path model by identifying each of the effects on a separate line in a character string. This string takes the model formula from each of the lm() functions we fitted earlier. We can also include the path names as multipliers of the influences in this model formula. Here we use the paths from our hypothesized path model to define the effects in the path model:\n\n# Define path model\npath.model = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n\"\n\nThen we can give this model as input to the sem() function along with the data frame that includes the data to estimate the model. We assign this to an object and use summary() to obtain the results. The rsquare=TRUE argument in summary() print the \\(R^2\\) values, the latter of which we can use to estimate the paths from the unmeasured variables to the respective effects. You can also include other options to output additional estimates (e.g., ci = TRUE for confidence intervals). See here for additional information.\n\n# Fit model\npm.1 = sem(path.model, data = keith)\n\n# Results\nsummary(pm.1, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\n\nWe can also estimate and obtain tests for the indirect effects as well. To do this we need to add model syntax defining each indirect effect we wish to include. This model syntax uses the path names to define how to compute that effect. For example to include the indirect effect of academic ability on high school achievement via high school motivation we would add the following line into our character string:\nindirect_ability_via_motivation := p7*p8\nThe path model would then be defined and fitted:\n\n# Define path model\npath.model.2 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_ability_via_motivation := p7*p8\n\"\n\n# Fit model\npm.2 = sem(path.model.2, data = keith)\n\n# Results\nsummary(pm.2, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indrct_blty_v_    0.002    0.003    0.599    0.549\n\n\nHere the indirect effect of academic ability on high school achievement via high school motivation is not statistically significant. This is likely because the direct effect of high school motivation on high school achievement is also not statistically significant (i.e., if that path is 0 then the product we get when computing the indirect effect will also be 0).\nHere we define and compute the overall indirect effects for each of the hypothesized causes:\n\n# Define path model\npath.model.3 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p8 + p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n\"\n\n# Fit model\npm.3 = sem(path.model.3, data = keith)\n\n# Results\nsummary(pm.2, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indrct_blty_v_    0.002    0.003    0.599    0.549\n\n\nFinally, we can fit a path model and estimate the direct, indirect, and total effects.\n\n# Define path model\npath.model.4 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p8 + p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n  total_fam_back := p3 + p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  total_ability := p6 + p7*p8 + p7*p9*p10 + p5*p10\n  total_motivation := p8 + p9*p10\n  total_coursework := p10\n\"\n\n# Fit model\npm.4 = sem(path.model.4, data = keith)\n\n# Results\nsummary(pm.4, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indirct_fm_bck    0.348    0.024   14.751    0.000\n    indirect_ablty    0.131    0.013    9.868    0.000\n    indirect_mtvtn    0.083    0.010    8.003    0.000\n    total_fam_back    0.417    0.029   14.508    0.000\n    total_ability     0.682    0.023   29.460    0.000\n    total_motivatn    0.095    0.021    4.448    0.000\n    total_courswrk    0.310    0.024   12.996    0.000\n\n\n\n\n\nEmpirically Reducing the Model\nThe path between high school motivation and and high school achievement was not statistically significant (p = 0.546). This suggests that the effect we saw in the data may be attributable to sampling variation. One possibility is to reduce the model by removing this path.\n\n\n\n\n\nReduced path model for several predictors of high school achievement. In this model the direct effect between high school motivation and high school achievement was removed (path p8).\n\n\n\n\nNote that removing a path can change both direct and indirect effects. In our path model, we would need to remove any path that traverses through path p8. There are two approaches to doing this:\nOne approach is to remove the path and use the original estimates to re-compute the direct, indirect, and total effects. For example, to compute the indirect effects of academic ability on high school achievement we would use:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effects} &=  (0.374 \\times 0.310)  + (0.152 \\times 0.267 \\times 0.310) \\\\[1em]\n&= 0.128521\n\\end{split}\n\\]\nA second approach is to re-fit the model to the empirical data. The fitted path model is shown below.\n\n# Define path model\npath.model.5 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6  + p1*p7*p9*p10 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n  total_fam_back := p3 + p1*p5*p10 + p1*p6  + p1*p7*p9*p10 + p4*p9*p10 + p2*p10\n  total_ability := p6 + p7*p9*p10 + p5*p10\n  total_coursework := p10\n\"\n\n# Fit model\npm.5 = sem(path.model.5, data = keith)\n\n# Results\nsummary(pm.5, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.365\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.546\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.070    0.022    3.240    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    courswrk (p10)    0.314    0.023   13.841    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indirct_fm_bck    0.347    0.024   14.740    0.000\n    indirect_ablty    0.130    0.013    9.866    0.000\n    indirect_mtvtn    0.084    0.010    8.189    0.000\n    total_fam_back    0.417    0.029   14.508    0.000\n    total_ability     0.682    0.023   29.460    0.000\n    total_courswrk    0.314    0.023   13.841    0.000\n\n\nComputing the indirect effects of academic ability on high school achievement for this re-fitted model we would use:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effects} &=  (0.374 \\times 0.314)  + (0.152 \\times 0.267 \\times 0.314) \\\\[1em]\n&= 0.1301794\n\\end{split}\n\\]\nOr, we could extract the estimate for indirect_ablty from the output. To get these results to more decimal places, we can use the parameterEstimates() function, which outputs a data frame of the results. We want to extract the est column, which includes the estimates. Note that the indirect effects of academic ability on high school achievement is the 16th element of the est vector.\n\n# Data frame of parameter estimates\nparameterEstimates(pm.5)\n\n                   lhs op                                              rhs\n1              ability  ~                                         fam_back\n2           motivation  ~                                         fam_back\n3           motivation  ~                                          ability\n4           coursework  ~                                         fam_back\n5           coursework  ~                                          ability\n6           coursework  ~                                       motivation\n7              achieve  ~                                         fam_back\n8              achieve  ~                                          ability\n9              achieve  ~                                       coursework\n10             ability ~~                                          ability\n11          motivation ~~                                       motivation\n12          coursework ~~                                       coursework\n13             achieve ~~                                          achieve\n14            fam_back ~~                                         fam_back\n15   indirect_fam_back :=    p1*p5*p10+p1*p6+p1*p7*p9*p10+p4*p9*p10+p2*p10\n16    indirect_ability :=                                 p7*p9*p10+p5*p10\n17 indirect_motivation :=                                           p9*p10\n18      total_fam_back := p3+p1*p5*p10+p1*p6+p1*p7*p9*p10+p4*p9*p10+p2*p10\n19       total_ability :=                              p6+p7*p9*p10+p5*p10\n20    total_coursework :=                                              p10\n                 label   est    se      z pvalue ci.lower ci.upper\n1                   p1 0.417 0.029 14.508  0.000    0.361    0.473\n2                   p4 0.127 0.034  3.741  0.000    0.060    0.193\n3                   p7 0.152 0.034  4.502  0.000    0.086    0.219\n4                   p2 0.165 0.028  5.838  0.000    0.110    0.221\n5                   p5 0.374 0.028 13.194  0.000    0.319    0.430\n6                   p9 0.267 0.026 10.158  0.000    0.215    0.318\n7                   p3 0.070 0.022  3.240  0.001    0.028    0.113\n8                   p6 0.551 0.023 23.758  0.000    0.506    0.597\n9                  p10 0.314 0.023 13.841  0.000    0.270    0.359\n10                     0.825 0.037 22.361  0.000    0.753    0.898\n11                     0.944 0.042 22.361  0.000    0.861    1.027\n12                     0.651 0.029 22.361  0.000    0.594    0.708\n13                     0.371 0.017 22.361  0.000    0.338    0.403\n14                     0.999 0.000     NA     NA    0.999    0.999\n15   indirect_fam_back 0.347 0.024 14.740  0.000    0.301    0.393\n16    indirect_ability 0.130 0.013  9.866  0.000    0.105    0.156\n17 indirect_motivation 0.084 0.010  8.189  0.000    0.064    0.104\n18      total_fam_back 0.417 0.029 14.508  0.000    0.361    0.473\n19       total_ability 0.682 0.023 29.460  0.000    0.636    0.727\n20    total_coursework 0.314 0.023 13.841  0.000    0.270    0.359\n\n# Estimates column\nparameterEstimates(pm.5)$est\n\n [1] 0.41700000 0.12651448 0.15224346 0.16516282 0.37442021 0.26686292\n [7] 0.07021160 0.55114506 0.31441104 0.82528489 0.94380759 0.65137255\n[13] 0.37079389 0.99900000 0.34678840 0.13049578 0.08390465 0.41700000\n[19] 0.68164084 0.31441104\n\n# Indirect effects of academic ability on high school achievement\nparameterEstimates(pm.5)$est[16]\n\n[1] 0.1304958\n\n\nWhether you reduce the model and re-fit it to the data, or use the estimates from the original fitted model and set non-significant effects to zero, the differences are minimal; in this case only one of the path coefficients changed when rounded to three decimal places.\n\n\n\n\n\nReferences\n\nKeith, T. V. (2015). Multiple regression and beyond: An introduction to multiple regression and structural equation modeling (2nd ed.). New York: Routledge."
  },
  {
    "objectID": "notes/01-06-regression-from-summary-measures.html",
    "href": "notes/01-06-regression-from-summary-measures.html",
    "title": "Regression from Summary Measures",
    "section": "",
    "text": "When we have raw data, we can fit a regression model using the lm() function and obtain model- and coefficient-level summaries using the glance() and tidy() functions respectively. However, there are times we might want to compute regression coefficients and standard errors, but are not given the raw data. This is common for example in articles, which often report summaries rather than providing raw data. In this set of notes, we will examine how we can fit a regression model to summaries of the data, rather than to the raw data itself.\nSo long as we have certain statistical information we can compute the regression coefficients and standard errors without having the raw data. To do this, we need the sample size, means, standard deviations, and correlations between variables. For example, consider the following summary table:\nTable 1: Correlation matrix for five attributes measured on n = 1,000 students. Means (standard deviations) for each attribute are provided on the main diagonal.\n\n\n\n\n\n\n\nCorrelation matrix for five attributes measured on *n* = 1,000 students. Means (standard deviations) for each attribute are provided on the main diagonal.\n\n\nMeasure\n1.\n2.\n3.\n4.\n5.\n\n\n\n\n1. Achievement\n50 (10)\n---\n---\n---\n---\n\n\n2. Ability\n0.737\n100 (15)\n---\n---\n---\n\n\n3. Motivation\n0.255\n0.205\n50 (10)\n---\n---\n\n\n4. Previous Coursework\n0.615\n0.498\n0.375\n4 (2)\n---\n\n\n5. Family Background\n0.417\n0.417\n0.190\n0.372\n0 (1)\nSay we wanted to use this information to fit a regression model to predict variation in achievement using the other four predictors. Mathematically,\n\\[\n\\begin{split}\n\\mathrm{Achievement}_i = &\\beta_0 + \\beta_1(\\mathrm{Ability}_i) + \\beta_2(\\mathrm{Motivation}_i) + \\\\\n&\\beta_3(\\mathrm{Coursework~}_i) + \\beta_4(\\mathrm{Family~Background}_i) + \\epsilon_i\n\\end{split}\n\\]\nTo do this we are going to create the correlation matrix, the vector of attribute means, and the vector of attribute standard deviations.\n# Create correlation matrix\ncorrs = matrix(\n  data = c(\n    1.000, 0.737, 0.255, 0.615, 0.417,\n    0.737, 1.000, 0.205, 0.498, 0.417,\n    0.255, 0.205, 1.000, 0.375, 0.190,\n    0.615, 0.498, 0.375, 1.000, 0.372,\n    0.417, 0.417, 0.190, 0.372, 1.000\n  ),\n  nrow = 5\n)\n\n# View correlation matrix\ncorrs\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 1.000 0.737 0.255 0.615 0.417\n[2,] 0.737 1.000 0.205 0.498 0.417\n[3,] 0.255 0.205 1.000 0.375 0.190\n[4,] 0.615 0.498 0.375 1.000 0.372\n[5,] 0.417 0.417 0.190 0.372 1.000\n\n# Create mean vector\nmeans = c(50, 100, 50, 4, 0)\n\n# Create sd vector\nsds = c(10, 15, 10, 2, 1)\n\n# Set sample size\nn = 1000"
  },
  {
    "objectID": "notes/01-06-regression-from-summary-measures.html#going-from-the-correlations-to-covariances",
    "href": "notes/01-06-regression-from-summary-measures.html#going-from-the-correlations-to-covariances",
    "title": "Regression from Summary Measures",
    "section": "Going From the Correlations to Covariances",
    "text": "Going From the Correlations to Covariances\nThe summary measures we have give correlations, not covariances. However, it is quite easy to convert a correlation matrix to a covariance matrix. From the chapter Statistical Application: SSCP, Variance–Covariance, and Correlation Matrices in Matrix Algebra for Educational Scientists, know that:\n\\[\n\\mathbf{R} = \\mathbf{S} \\boldsymbol\\Sigma \\mathbf{S}\n\\]\nwhere R is the correlation matrix, \\(\\boldsymbol\\Sigma\\) is the covariance matrix, and S is a diagonal scaling matrix with diagonal elements equal to the reciprocal of the standard deviations of each of the variables in the covariance matrix. Re-arranging this:\n\\[\n\\boldsymbol\\Sigma = \\mathbf{S}^{-1} \\mathbf{R} \\mathbf{S}^{-1}\n\\]\nRecall that the inverse of a diagonal matrix is another diagonal matrix where the diagonal elements are the resciprocals of the original. Thus the diagonal elements of the inverse of our scaling matrix will be the standard deviations of the variables. Using this formula, we can now convert the given correlation matrix to a covariance matrix.\n\n# Compute the scaling matrix S^(-1)\nS_inv = diag(sds)\nS_inv\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10    0    0    0    0\n[2,]    0   15    0    0    0\n[3,]    0    0   10    0    0\n[4,]    0    0    0    2    0\n[5,]    0    0    0    0    1\n\n# Compute covariance matrix\ncovs = S_inv %*% corrs %*% S_inv\ncovs\n\n       [,1]    [,2]   [,3]   [,4]  [,5]\n[1,] 100.00 110.550  25.50 12.300 4.170\n[2,] 110.55 225.000  30.75 14.940 6.255\n[3,]  25.50  30.750 100.00  7.500 1.900\n[4,]  12.30  14.940   7.50  4.000 0.744\n[5,]   4.17   6.255   1.90  0.744 1.000\n\n\nAdding in the row and column names:\n\n\n\n\nTable 2: Variance-covariance matrix for five attributes measured on n = 1,000 students. The blue elements compose Cov(X, y) and the red elements constitute Cov(X, X).\n\n\n\n\n\n\n\nVariance-covariance matrix for five attributes measured on *n* = 1,000 students. The blue elements compose Cov(X, y) and the red elements constitute Cov(X, X).\n\n\nAchievement\nAbility\nMotivation\nPrevious Coursework\nFamily Background\n\n\n\n\n100.00\n110.550\n25.50\n12.300\n4.170\n\n\n110.55\n225.000\n30.75\n14.940\n6.255\n\n\n25.50\n30.750\n100.00\n7.500\n1.900\n\n\n12.30\n14.940\n7.50\n4.000\n0.744\n\n\n4.17\n6.255\n1.90\n0.744\n1.000\n\n\n\n\n\n\n\n\n\n\n\nRemember, the diagonal elements in the covariance matrix are variances of each variable and the off-diagonal elements are the covariances between two variables. For example, the first diagonal element is 100, which is the variance of the achievement variable. The remaining elements in the first column indicate the covariances between achievement and ability, achievement and motivation, achievement and previous coursework, and achievement and family background."
  },
  {
    "objectID": "notes/01-06-regression-from-summary-measures.html#finding-the-predictor-coefficients",
    "href": "notes/01-06-regression-from-summary-measures.html#finding-the-predictor-coefficients",
    "title": "Regression from Summary Measures",
    "section": "Finding the Predictor Coefficients",
    "text": "Finding the Predictor Coefficients\nTo compute the coefficients for the predictors, we need to obtain two things:\n\nCov(X, X): The variance-covariance matrix of the predictors, and\nCov(X, y): The vector that contains the covariances between the outcome and each predictor. (Note: This vector does not include the variance of the outcome, only the covariances with the predictors.)\n\nIn our example, the elements in the first column (or row) of the variance-covariance matrix other than the variance of the outcome are the values in the vector Cov(X, y). The elements in the other rows/columns make up the elements of the Cov(X, X) matrix. In Table 2 the blue elements compose Cov(X, y) and the red elements constitute Cov(X, X). We can create this vector and matrix using indexing.\n\n# Create Cov(X, y)\ncov_xy = covs[-1 , 1]\ncov_xy\n\n[1] 110.55  25.50  12.30   4.17\n\n# Create Cov(X, X)\ncov_xx = covs[-1 , -1]\ncov_xx\n\n        [,1]   [,2]   [,3]  [,4]\n[1,] 225.000  30.75 14.940 6.255\n[2,]  30.750 100.00  7.500 1.900\n[3,]  14.940   7.50  4.000 0.744\n[4,]   6.255   1.90  0.744 1.000\n\n\nThen we can use these to compute the predictor coefficients.\n\n# Compute predictor coefficients\nb = solve(cov_xx) %*% cov_xy\nb\n\n           [,1]\n[1,] 0.36737330\n[2,] 0.01257721\n[3,] 1.55001342\n[4,] 0.69497334\n\n\nThus the coefficient estimates are:\n\n\\(\\hat\\beta_{\\mathrm{Ability}} = 0.367\\)\n\\(\\hat\\beta_{\\mathrm{Motivation}} = 0.013\\)\n\\(\\hat\\beta_{\\mathrm{Previous~Coursework}} = 1.550\\)\n\\(\\hat\\beta_{\\mathrm{Family~Background}} = 0.695\\)"
  },
  {
    "objectID": "notes/01-06-regression-from-summary-measures.html#computing-the-intercept",
    "href": "notes/01-06-regression-from-summary-measures.html#computing-the-intercept",
    "title": "Regression from Summary Measures",
    "section": "Computing the Intercept",
    "text": "Computing the Intercept\nTo compute the intercept, we can use the following:\n\\[\n\\hat\\beta_0 = \\bar{y} - \\bar{\\mathbf{x}}^\\intercal\\mathbf{b}\n\\]\nwhere \\(\\bar{y}\\) is the mean of the outcome, \\(\\bar{\\mathbf{x}}\\) is the vector of predictor means, and b is the associated vector of predictor coefficients.\nIn our example,\n\n# Compute intercept\nb_0 = means[1] - t(means[2:5]) %*% b\nb_0\n\n         [,1]\n[1,] 6.433756\n\n\nThus the fitted equation for the unstandardized model is:\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Achievement}_i} = &6.434 + 0.367(\\mathrm{Ability}_i) + 0.013(\\mathrm{Motivation}_i) + \\\\\n&1.550(\\mathrm{Coursework~}_i) + 0.695(\\mathrm{Family~Background}_i)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/01-06-regression-from-summary-measures.html#compute-mathbfxintercalmathbfx-matrix",
    "href": "notes/01-06-regression-from-summary-measures.html#compute-mathbfxintercalmathbfx-matrix",
    "title": "Regression from Summary Measures",
    "section": "Compute \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix",
    "text": "Compute \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) Matrix\nThe elements in the first row and column of the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix are functions of the sample size and means of the predictor variables. Namely the first element in the first row and column is n and the remaining elements in the that row and column are created as \\(n\\mathbf{M_x}\\), where n is the sample size, and \\(\\mathbf{M_x}\\) is the vector of predictor means. The remaining elements constitute a submatrix defined as:\n\\[\n(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}} = (n-1)\\mathrm{Cov}(X,X) + n(\\mathbf{M_x})(\\mathbf{M_x}^\\intercal)\n\\]\nwhere n is the sample size, Cov(X,X) is the variance covariance matrix ofthe predictors, and \\(\\mathbf{M_x}\\) is again the vector of predictor means. We cn then create the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix as:\n\\[\n\\require{color}\n\\begin{split}\n\\mathbf{X}^\\intercal\\mathbf{X} &= \\begin{bmatrix}{\\color[rgb]{0.044147,0.363972,0.636955}n} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\mathbf{M_x}}\\\\{\\color[rgb]{0.044147,0.363972,0.636955}n\\mathbf{M_x}} & {\\color[rgb]{0.9058824, 0.1137255, 0.2117647}(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}}\\end{bmatrix}\n\\\\[2ex]\n&= \\begin{bmatrix}{\\color[rgb]{0.044147,0.363972,0.636955}n} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X1}} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X2}} &  {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X3}} & {\\color[rgb]{0.044147,0.363972,0.636955}\\ldots} & {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{Xk}}\\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X1}} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X2}} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{X3}} & & & {\\color[rgb]{0.9058824, 0.1137255, 0.2117647}(\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}} & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}\\vdots} & & & & &  \\\\ {\\color[rgb]{0.044147,0.363972,0.636955}n\\overline{Xk}} & & & & &   \\end{bmatrix}\n\\end{split}\n\\]\nTo create this matrix using R, we will:\n\nCompute the submatrix \\((\\mathbf{X}^\\intercal\\mathbf{X})_{\\mathrm{Sub}}\\).\nBind the \\(n\\mathbf{M_x}\\) vector to the top of the submatrix.\nBind the vector that contains n and \\(n\\mathbf{M_x}\\) to the left of the resulting matrix from Step 2.\n\nThe resulting matrix is the \\(\\mathbf{X}^\\intercal\\mathbf{X}\\) matrix. For our example,\n\n# Step 1: Create submatrix\nsub_mat = 999 * cov_xx + 1000 * means[2:5] %*% t(means[2:5])\nsub_mat\n\n             [,1]      [,2]       [,3]     [,4]\n[1,] 10224775.000 5030719.2 414925.060 6248.745\n[2,]  5030719.250 2599900.0 207492.500 1898.100\n[3,]   414925.060  207492.5  19996.000  743.256\n[4,]     6248.745    1898.1    743.256  999.000\n\n# Step 2: Bind n(M_x) to top of submatrix\nmat_2 = rbind(1000 * means[2:5], sub_mat)\nmat_2\n\n             [,1]      [,2]       [,3]     [,4]\n[1,]   100000.000   50000.0   4000.000    0.000\n[2,] 10224775.000 5030719.2 414925.060 6248.745\n[3,]  5030719.250 2599900.0 207492.500 1898.100\n[4,]   414925.060  207492.5  19996.000  743.256\n[5,]     6248.745    1898.1    743.256  999.000\n\n# Step 3: Bind vector to left of Step 2 matrix\nXtX = cbind(c(1000, 1000 * means[2:5]), mat_2)\nXtX\n\n       [,1]         [,2]      [,3]       [,4]     [,5]\n[1,]   1000   100000.000   50000.0   4000.000    0.000\n[2,] 100000 10224775.000 5030719.2 414925.060 6248.745\n[3,]  50000  5030719.250 2599900.0 207492.500 1898.100\n[4,]   4000   414925.060  207492.5  19996.000  743.256\n[5,]      0     6248.745    1898.1    743.256  999.000\n\n\nWe can then scale this matrix using our previous estimate of the residual variance to obtain the variance-covariance matrix for the coefficients and compute the coefficient standard errors.\n\n# Compute var-cov matrix of b\ncov_b = s2_e * solve(XtX)\ncov_b\n\n            [,1]            [,2]            [,3]         [,4]          [,5]\n[1,]  2.86183238 -0.021078176856 -0.018522600965  0.052341868  0.1280945885\n[2,] -0.02107818  0.000240314903 -0.000001964506 -0.000713772 -0.0009683908\n[3,] -0.01852260 -0.000001964506  0.000435428791 -0.000763097 -0.0002472826\n[4,]  0.05234187 -0.000713772031 -0.000763096999  0.014297546 -0.0047228461\n[5,]  0.12809459 -0.000968390764 -0.000247282552 -0.004722846  0.0473303248\n\n# Compute SEs\nse = sqrt(diag(cov_b))\nse\n\n[1] 1.69169512 0.01550209 0.02086693 0.11957235 0.21755534\n\n\nThese are the standard errors for the intercept and each predictor. Here I add the coefficients and standard errors to a data frame and use them to compute t-values, associated p-values, and confidence intervals.\n\n# Load library\nlibrary(tidyverse)\n\n# Create regression table\ndata.frame(\n  Predictor = c(\"Intercept\", \"Ability\", \"Motivation\", \"Previous Coursework\", \"Family Background\"),\n  B = c(b_0, b),\n  SE = se\n) |&gt;\n  mutate(\n    t = round(B /SE, 3),\n    p = round(2 * pt(-abs(t), df = 995), 5),\n    CI = paste0(\"(\", round(B + qt(.025, df = 995)*SE, 3), \", \", round(B + qt(.975, df = 995)*SE, 3), \")\")\n  )\n\n            Predictor          B         SE      t       p              CI\n1           Intercept 6.43375597 1.69169512  3.803 0.00015  (3.114, 9.753)\n2             Ability 0.36737330 0.01550209 23.698 0.00000  (0.337, 0.398)\n3          Motivation 0.01257721 0.02086693  0.603 0.54665 (-0.028, 0.054)\n4 Previous Coursework 1.55001342 0.11957235 12.963 0.00000  (1.315, 1.785)\n5   Family Background 0.69497334 0.21755534  3.194 0.00145  (0.268, 1.122)"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html",
    "href": "notes/02-01-intro-to-path-analysis.html",
    "title": "Introduction to Path Analysis",
    "section": "",
    "text": "Path diagrams are visual depictions of the hypothesized relationships between a set of variables. In this set of notes, we will introduce some of the ideas and concepts related to path diagrams, and the estimation of effects depicted in a path diagram (i.e., path analysis)."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#what-are-path-diagrams",
    "href": "notes/02-01-intro-to-path-analysis.html#what-are-path-diagrams",
    "title": "Introduction to Path Analysis",
    "section": "",
    "text": "Path diagrams are visual depictions of the hypothesized relationships between a set of variables. In this set of notes, we will introduce some of the ideas and concepts related to path diagrams, and the estimation of effects depicted in a path diagram (i.e., path analysis)."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#example",
    "href": "notes/02-01-intro-to-path-analysis.html#example",
    "title": "Introduction to Path Analysis",
    "section": "Example",
    "text": "Example\nTo help us in this endeavor, we will consider three potential student-level variables for our path model:\n\nAchievement\nAcademic Ability\nMotivation"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#simulating-data",
    "href": "notes/02-01-intro-to-path-analysis.html#simulating-data",
    "title": "Introduction to Path Analysis",
    "section": "Simulating Data",
    "text": "Simulating Data\n\n# Load Libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(corrr)\n\n# Create correlation matrix\ncorrs = matrix(\n  data = c(\n    1.000, 0.737, 0.255,\n    0.737, 1.000, 0.205,\n    0.255, 0.205, 1.000\n  ),\n  nrow = 3\n)\n\n# Create mean vector\nmeans = c(0, 0, 0) \n\n# Set sample size\nn = 1000           \n\n# Make simulation reproducible\nset.seed(1)\n\n\n# Simulate the data and convert to data frame\nsim_dat &lt;- data.frame(MASS::mvrnorm(n = 1000, mu = means, Sigma = corrs, empirical = TRUE)) |&gt;\n  rename(\n    achievement = X1,\n    ability = X2, \n    motivation = X3\n  )\n\n\n# View simulated data\nhead(sim_dat)\n\n  achievement    ability motivation\n1  0.33527960 -0.1297477  1.4114639\n2  0.99197530 -0.3813547  1.2637617\n3 -0.70788322  0.6913426 -0.5557938\n4 -0.87083130 -1.3095554 -1.1570792\n5 -0.05935273 -0.2583549 -0.1541515\n6  0.38943125  1.4254429 -0.6394095"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#reading-a-path-diagram",
    "href": "notes/02-01-intro-to-path-analysis.html#reading-a-path-diagram",
    "title": "Introduction to Path Analysis",
    "section": "Reading a Path Diagram",
    "text": "Reading a Path Diagram\nIn a path diagram,\n\nVariables we have data on (i.e., manifest or measured variables) are represented by squares or rectangles.\nLines between the variables indicate a relationship between the two connected variables. The lines between the different variables are called paths.\n\nIf the path has an arrowhead on both sides, it indicates the two connected variables are related (i.e., correlated).\nIf the path has an arrowhead on only one side, it indicates a hypothesized causal relationship. The cause is in the direction of the arrow; that is, the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line.\n\n\n\n\n\n\n\n\n\n\nFigure 1: Several symbols seen in path models."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#two-example-path-diagrams",
    "href": "notes/02-01-intro-to-path-analysis.html#two-example-path-diagrams",
    "title": "Introduction to Path Analysis",
    "section": "Two Example Path Diagrams",
    "text": "Two Example Path Diagrams\nIn both of the examples below, there are three manifest/measured variables (academic ability, motivation, and achievement) and three paths (p1, p2, and p3). The difference between the two is that in the first diagram, there are hypothesized causal relationships, while in the second diagram the relationships are not presumed to be causal.\n\n\n\n\n\n\n\n\nFigure 2: All paths represent correlations (no causal paths) between the manifest variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The causal paths between the manifest variables indicate a weak causal ordering."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#path-diagram-1",
    "href": "notes/02-01-intro-to-path-analysis.html#path-diagram-1",
    "title": "Introduction to Path Analysis",
    "section": "Path Diagram 1",
    "text": "Path Diagram 1\nThe model depicted in this path diagram posits that there are relationships between:\n\nAcademic ability and achievement (p1);\nAcademic ability and motivation (p2); and\nMotivation and achievement (p3).\n\n\n\n\n\n\n\n\n\nFigure 4: All paths represent correlations (no causal paths) between the manifest variables.\n\n\n\n\n\nThe double-headed arrows on the paths indicate that the variables are related (i.e., correlated), although there is no causal direction hypothesized.\n\nEstimating Path Coefficients in Path Diagram 1\nIn a path analysis, one goal is to estimate path coefficients. In this model, since the paths represent the relationship between two variables, the path coefficients are simply the bivariate correlations between each set of variables.\n\n# Compute correlations\nsim_dat |&gt;\n  correlate()\n\n# A tibble: 3 × 4\n  term        achievement ability motivation\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1 achievement      NA       0.737      0.255\n2 ability           0.737  NA          0.205\n3 motivation        0.255   0.205     NA    \n\n\n\n\n\n\n\n\n\n\nFigure 5: All paths represent correlations (no causal paths) between the manifest variables. The correlations are also shown on the model."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#weak-causal-ordering",
    "href": "notes/02-01-intro-to-path-analysis.html#weak-causal-ordering",
    "title": "Introduction to Path Analysis",
    "section": "Weak Causal Ordering",
    "text": "Weak Causal Ordering\nIn path modeling, we often assert weak causal ordering on the variables in the model. With weak causal ordering, we are not saying X causes Y, but rather that if X and Y are casually related, one of these variables (typically X) is the cause of the other.\n\n\n\n\n\nflowchart LR\n  A[X] --&gt; B[Y]\n\n\n\n\n\n\nIn this model, the paths have arrows at only one end indicating the hypothesized causal relationship. The cause is in the direction of the arrow, that is the hypothesized cause is at the butt end of the line, while the hypothesized effect is at the arrowhead side of the line.\n\nAny weak causal model should be posited prior to collecting or looking at the data! Creating this model can also help you identify the data to collect.\n\nThe second path diagram from above represented the hypothesized causes and effects between the variables in the model.\n\n\n\n\n\n\n\n\nFigure 6: The causal paths between the manifest variables indicate a weak causal ordering.\n\n\n\n\n\nThe model depicted in this path diagram posits the following causal relationships:\n\nIf academic ability and achievement are causally related, then academic ability is the cause of achievement.\nIf academic ability and motivation are causally related, then academic ability is the cause of motivation.\nIf motivation and achievement are causally related, then motivation is the cause of achievement.\n\n\n\nEstimating Path Coefficients in a Weak Causal Model\nAgain, we would want to estimate the path coefficients shown in the diagram for the weak causal model. In causal models, the path coefficients are not necessarily the correlations. To find the path coefficients we need to use the tracing rule. The tracing rule indicates that:\n\nTRACING RULE\nThe correlation between two variables X and Y is equal to the sum of the possible products of all possible paths from each possible tracing from X to Y, with the following two exceptions:\n\nThe same variable is not entered more than once in the same tracing.\nA variable is not both entered and exited through an arrowhead.\n\n\n\n\n\nExample of the Tracing Rule\nAs an example, consider all the tracings (routes) that allow us to start at the academic ability variable and go to the achievement variable in the path diagram at right.\n\n\n\n\n\n\n\n\nFigure 7: The causal paths between the manifest variables indicate a weak causal ordering.\n\n\n\n\n\nThere are two possible tracings that conform to the tracing rule:\n\nStart at the academic ability variable and take p1 to the achievement variable.\nStart at the academic ability variable and take p2 to the motivation variable, then take p3 to the achievement variable.\n\nSimilarly, we could have started at the achievement variable and determined the tracings to get to the academic ability variable:\n\nStart at the achievement variable and take p1 to the academic ability variable.\nStart at the achievement variable and take p3 to the motivation variable, then take p2 to the academic ability variable.\n\n\nIMPORTANT\nNote that when we are considering tracings, we do not have to worry about the direction of the arrow, only that there is a path we can trace. The only rule regarding arrowheads is that a variable can not be both entered and exited through an arrowhead.\n\n\n\n\nBack to Estimating the Path Coeffients\nEach tracing yields a product of the path coefficents used in the tracing. Thus the first tracing is \\(p1\\) (there is only one path, so the product is simply the path), and the second tracing yields \\(p2 \\times p3\\). Since the tracing rule says that the correlation between academic ability and achievement is equal to the sum of the products yielded by the tracings, we know that:\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ achievement}} &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + p2(p3)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#your-turn",
    "href": "notes/02-01-intro-to-path-analysis.html#your-turn",
    "title": "Introduction to Path Analysis",
    "section": "Your Turn",
    "text": "Your Turn\nUse the tracing rule to write two more equations. The first equation should represent the correlation between motivation and achievement, and the second equation should represent the correlation between ability and motivation.\n\nShow/Hide Solution\n\n\nTo represent the correlation between motivation and achievement we have two potential tracings:\n\nStart at the motivation and take p3 to the achievement variable.\nStart at the motivation variable and take p2 to the academic ability variable, then take p1 to the achievement variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{motivation,~ achievement}} &= p3 + p2(p1) \\\\[1em]\n.255&= p3 + p2(p1)\n\\end{split}\n\\]\nTo represent the correlation between academic ability and motivation we have one tracing:\n\nStart at the academic ability and take p2 to the motivation variable.\n\n\\[\n\\begin{split}\nr_{\\mathrm{academic~ability,~ motivation}} &= p2 \\\\[1em]\n.205&= p2\n\\end{split}\n\\]\n\n\nQUESTION\nWhy can’t we use the tracing that takes p1 from academic ability to achievement then takes p3 to motivation??"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#solving-for-p1-p2-and-p3",
    "href": "notes/02-01-intro-to-path-analysis.html#solving-for-p1-p2-and-p3",
    "title": "Introduction to Path Analysis",
    "section": "Solving for p1, p2, and p3",
    "text": "Solving for p1, p2, and p3\nWe now have three equations with three unknowns. We can solve this system of equations to find p1, p2, and p3.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.255 &= p3 + p2(p1) \\\\[1em]\n.205 &= p2\n\\end{split}\n\\]\nTo solve for p1, substitute in .205 for p2 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(p3) \\\\[1em]\np1 &= .737 - .205(p3)\n\\end{split}\n\\]\nTo solve for p3, substitute .205 in for p2 and \\(.737 - .205(p3)\\) in for p1 in the second equation and solve for p3.\n\\[\n\\begin{split}\n.255 &= p3 + p2(p1) \\\\[1em]\n.255 &= p3 + .205(.737 - .205(p3)) \\\\[1em]\n.255 &= p3 + 0.151085 - 0.042025(p3) \\\\[1em]\n0.103915 &= 0.957975(p3) \\\\[1em]\np3 &= .108\n\\end{split}\n\\]\nFinally, to solve for p1, substitute .205 in for p2 and .108 in for p3 in the first equation and solve for p1.\n\\[\n\\begin{split}\n.737 &= p1 + p2(p3) \\\\[1em]\n.737 &= p1 + .205(.108) \\\\[1em]\n.737 &= p1 + 0.02214 \\\\[1em]\np1 &= .715\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nFigure 8: The path model between the manifest variables with the path coefficients."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#interpreting-path-coefficients",
    "href": "notes/02-01-intro-to-path-analysis.html#interpreting-path-coefficients",
    "title": "Introduction to Path Analysis",
    "section": "Interpreting Path Coefficients",
    "text": "Interpreting Path Coefficients\nPath coefficients are standardized coefficients which can be interpreted similar to standardized regression coefficients.\n\nGiven the adequacy of the path model, each 1-standard deviation increase in motivation increases achievement by .108 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases achievement by .715 standard deviations, on average.\nGiven the adequacy of the path model, each 1-standard deviation increase in academic ability increases motivation by .205 standard deviations, on average.\n\nThere are two things to note when we interpret path coefficients that are different from when we interpret regression coefficients.\n\nWe include “given the adequacy of the model” in our interpretation. This is important because the values of the path coefficients absolutely depend on the weak causal model specified (i.e., how you draw the path diagram).\nBecause we are positing a causal relationship, we can use the causal type language in our interpretation. E.g., an increase in X leads to an increase in Y.\n\nNote also that the interpretation is not controlling for anything. These are simple relationships that we are interpreting."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#estimating-path-coefficients-via-regression",
    "href": "notes/02-01-intro-to-path-analysis.html#estimating-path-coefficients-via-regression",
    "title": "Introduction to Path Analysis",
    "section": "Estimating Path Coefficients via Regression",
    "text": "Estimating Path Coefficients via Regression\nWe can also find the path coefficients using regression rather than algebra. To determine the path coefficients in the weak causal model, we fit a set of regression models using the “causes” as predictors of any particular effect.\nIn our weak causal path diagram there are two effects, so we would need to fit two separate regression models. The syntax for these models would be:\nachievement ~ 0 + ability + motivation\nmotivation ~ 0 + ability\n\n# Path coefficients for paths to achievement\ntidy(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 2 × 5\n  term       estimate std.error statistic   p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ability       0.715    0.0216     33.1  8.26e-163\n2 motivation    0.108    0.0216      5.02 5.97e-  7\n\n# Path coefficient for paths to motivation\ntidy(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 5\n  term    estimate std.error statistic  p.value\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ability    0.205    0.0310      6.62 5.85e-11"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#residual-variation",
    "href": "notes/02-01-intro-to-path-analysis.html#residual-variation",
    "title": "Introduction to Path Analysis",
    "section": "Residual Variation",
    "text": "Residual Variation\nEven if the causal relationships were specified correctly, the model likely does not include ALL of the causes for motivation and achievement. There is also unaccounted for variation due to random variation and measurement error. To account for these three sources of variation in the weak causal model, we will add an error term to each of the effects in the path model.\n\n\n\n\n\n\n\n\nFigure 9: The path model between the manifest variables with the path coefficients. Error terms representing unaccounted for causes are also included for each effect.\n\n\n\n\n\nWe can also estimate the path coefficients for the error terms. These path coefficients are computed as,\n\\[\n\\epsilon_k = \\sqrt{1 - R^2_k}\n\\] where, \\(R^2_k\\) is the \\(R^2\\)-value from the regression model fitted to compute the initial path coefficients.\n\n# Path coefficients for error term on achievement\nglance(lm(achievement ~ 0 + ability + motivation, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.554         0.554 0.668      621. 6.36e-176     2 -1014. 2034. 2049.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .554)\n\n[1] 0.6678323\n\n# Path coefficient for error term on motivation\nglance(lm(motivation ~ 0 + ability, data = sim_dat))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0420        0.0411 0.979      43.8 5.85e-11     1 -1397. 2798. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nsqrt(1 - .0420)\n\n[1] 0.9787747\n\n\n\n\n\n\n\n\n\n\nFigure 10: The path model between the manifest variables with the path coefficients. Estimates for the error terms representing unaccounted for causes are also included for each effect."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#how-do-we-draw-path-diagram",
    "href": "notes/02-01-intro-to-path-analysis.html#how-do-we-draw-path-diagram",
    "title": "Introduction to Path Analysis",
    "section": "How Do We Draw Path Diagram?",
    "text": "How Do We Draw Path Diagram?\n\nTheory/prior research\nLogic/expert understanding/common sense\n\nWe also need to pay attention to time precedence. Cause does not operate backward in time. In our example, academic ability is well-documented as something that does not change after grade school. Thus, it would precede motivation and achievement in time."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#a-little-more-about-cause",
    "href": "notes/02-01-intro-to-path-analysis.html#a-little-more-about-cause",
    "title": "Introduction to Path Analysis",
    "section": "A Little More About Cause",
    "text": "A Little More About Cause\nWhat do we mean when we say “X causes Y”? Contrary to popular belief, we do not mean that changing X has a direct, and immediate change on Y. For example, it is now well known that smoking causes lung cancer.\n\n\n\n\n\n\n\n\nFigure 11: The path model showing that smoking causes cancer.\n\n\n\n\n\nBut, not everyone who smokes ends up getting lung cancer. Instead, cause is a probabilistic statement about the world. When we say smoking causes lung cancer what we mean, statistically, is that smoking increases the probability of developing lung cancer.\nMoreover, this increased probability is due to smoking, and not something else.\nThere are three primary requirements for a causal relationship between X and Y.\n\nThere must be a relationship between X and Y. (Correlation is a necessary component of causation.)\nTime precedence (Causes must occur prior to effects.)\nRelationship must not be spurious."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#including-time-precedence-in-the-path-diagram",
    "href": "notes/02-01-intro-to-path-analysis.html#including-time-precedence-in-the-path-diagram",
    "title": "Introduction to Path Analysis",
    "section": "Including Time Precedence in the Path Diagram",
    "text": "Including Time Precedence in the Path Diagram\nTime precedence is often reflected in the orientation of the model. Variables that occur earlier in time are oriented further to the left side of the diagram than variables that occur later.\n\n\n\n\n\n\n\n\nFigure 12: The path model showing that smoking causes cancer. The cause (smoking) is to the left of the effect (cancer).\n\n\n\n\n\nIn the smoking example, to be considered the cause for lung cancer, smoking has to occur prior to the onset of lung cancer.1\nIn our path diagram relating academic ability and motivation to acheivement, academic ability is furthest to the left (it occurs earliest), followed by motivation, and then academic achievement.\n\n\n\n\n\n\n\n\nFigure 13: The path model showing the effect of academic ability and motivation on acheivement."
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#common-cause",
    "href": "notes/02-01-intro-to-path-analysis.html#common-cause",
    "title": "Introduction to Path Analysis",
    "section": "Common Cause",
    "text": "Common Cause\nA common cause is a variable that is a cause of both X and Y which accounts for the relationship between X and Y. Consider the following example where we want to look at the causal impact of participation in Head Start programs on academic achievement.\n\n\n\n\n\n\n\n\nFigure 14: The path model showing the effect of Head Start enrollment on acheivement.\n\n\n\n\n\nIn practice, the path coefficient tends to be negative. That is Head Start participants tend to have lower academic achievement than their non-Head Start peers.\nA common cause of both Head Start participation and academic achievement is poverty. This is shown below.\n\n\n\n\n\n\n\n\nFigure 15: The path model showing the effect of Head Start enrollment on acheivement. Poverty is a common cause of both Head Start enrollment and acheivement.\n\n\n\n\n\nOnce we include poverty as a common cause, the path coefficient between Head Start participation and academic achievement switches direction. That is, after including poverty in the path model, Head Start participants tend to have higher academic achievement than their non-Head Start peers. This is akin to how effects in a regression model might change after we control for other predictors.\n\nIMPORTANT\nIn order to meet the third causal requirement, we need to include ALL common causes in the model. This requirement is the hardest to meet!"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#non-recursive-models",
    "href": "notes/02-01-intro-to-path-analysis.html#non-recursive-models",
    "title": "Introduction to Path Analysis",
    "section": "Non-Recursive Models",
    "text": "Non-Recursive Models\nIn the path diagrams we have looked at so far, the causal paths have gone in only one direction; there is a distinct cause and effect. The error terms on the effect variables are also all uncorrelated. These are called recursive models. It is possible for variables to effect each other (e.g., predator–prey relationships), or for the error terms to be correlated. This is called a non-recursive model.\n\n\n\n\n\n\n\n\nFigure 16: The path model showing the effect of habitat on wolf and rabbit populations. The model is non-recursive since the wolf and rabbit populations effect each other.\n\n\n\n\n\n\n\nUnder-Identified Models\nThe problem with estimating the path coefficients in the non-recursive model is it is under-identified. In our predator–prey model, we need to estimate four path coefficients, but only have three correlations (equations) from which to do so. We can’t solve this without adding additional constraints!\n\\[\n\\begin{split}\nr_{\\mathrm{H,W}} &= p1 + p2(p4) \\\\[1em]\nr_{\\mathrm{H,R}} &= p2 + p1(p3) \\\\[1em]\nr_{\\mathrm{H,W}} &= p3 + p4 + p1(p2)\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/02-01-intro-to-path-analysis.html#footnotes",
    "href": "notes/02-01-intro-to-path-analysis.html#footnotes",
    "title": "Introduction to Path Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the top–bottom orientation typically doesn’t mean anything—it is just used for aesthetics in the layout.↩︎"
  },
  {
    "objectID": "notes/02-02-path-analysis-using-r.html",
    "href": "notes/02-02-path-analysis-using-r.html",
    "title": "Path Analysis Using R",
    "section": "",
    "text": "In this set of notes, we will use the data from path-model-achievement.csv to fit a path model to explain effects on high school achievement. The data were simulated to include attributes for 1000 students from information provided by Keith (2015).\n\n# Load libraries\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(lavaan)\n\n# Import data\nkeith = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/path-model-achievement.csv\")\nkeith\n\n# A tibble: 1,000 × 5\n   achieve ability motivation coursework fam_back\n     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.511   0.656      -1.10       0.130   0.300 \n 2 -0.569   0.273      -1.48      -1.68   -0.419 \n 3  0.997  -0.402      -0.284      1.29   -0.334 \n 4  0.681   1.31        1.77       0.801   1.14  \n 5 -0.838   0.281       1.51       0.415   0.872 \n 6  0.206  -0.884      -0.128      0.591  -1.29  \n 7  1.07    1.92       -0.405      1.41    0.529 \n 8 -0.418  -0.857       0.207     -1.59   -2.23  \n 9  0.0460 -0.0173     -1.40      -1.58   -2.72  \n10  0.742   0.312      -1.82      -0.400  -0.0957\n# ℹ 990 more rows\n\n\n\n\nHypothesized Path Model\nThe hypothesized path model is shown below. This model was based on previous literature related to student achievement and on Keith’s experiences as an educator and educational researcher.\n\n\n\n\n\n\n\n\nFigure 1: Hypothesized path model for several predictors of high school achievement.\n\n\n\n\n\n\n\n\nEstimating the Path Coefficients\nWe need to estimate 10 path coefficients between the measured variables and four additional paths between unmeasured variables and each of the measured effects. To do this we can use a regression model that includes the hypothesized causes to predict variation in each of the effects. The regression coefficients obtained are the path coefficients between each of the hypothesized causes and effect.\nFor example, the variable high school motivation is an effect in our model (i.e., it has arrows going into it). The hypothesized causes of high school motivation are family background, and academic ability. To obtain those two path coefficients:\n\n# Fit model\nlm.motivation = lm(motivation ~ 1 + fam_back + ability, data = keith)\n\n# Obtain path coefficients\ntidy(lm.motivation)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic    p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept) -1.09e-16    0.0308 -3.54e-15 1.00      \n2 fam_back     1.27e- 1    0.0339  3.74e+ 0 0.000198  \n3 ability      1.52e- 1    0.0339  4.50e+ 0 0.00000777\n\n\nThe path coefficient for the path from family background to high school motivation (p4) is 0.127, and that for the path from academic ability to high school motivation (p7) is 0.152. These represent the direct effects of family background and academic ability on high school motivation, respectively. Here we interpret the direct effect of academic ability on high school motivation:\n\nGiven the adequacy of our model, academic ability has a small positive effect on high school motivation. Each one-standard deviation increase in academic ability will result in a 0.127-standard deviation unit change on high school motivation, on average.\n\nWe can also estimate the path between the unmeasured variables (\\(d_2\\)) and high school motivation. This is computed as:\n\\[\n\\sqrt{1-R^2}\n\\]\nwhere \\(R^2\\) is the coefficient of determination from the fitted regression. In our example\n\n# Obtain R2\nglance(lm.motivation)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0552        0.0534 0.973      29.2 4.97e-13     2 -1390. 2788. 2808.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Compute path\nsqrt(1 - 0.0552)\n\n[1] 0.9720082\n\n\nThe path between \\(d_2\\) and high school motivation is 0.972. This represents the effects of all other causes of high school motivation that we did not include in our hypothesized path model. This suggests that many of the influences of high school motivation are unaccounted for by the path model. Presumably, these are not important in positing the causal model. (Note if they are important, they should be included as measured variables in the path model.) The regression models to estimate the remaining paths are provided below (output not given), as well as the path estimates for the hypothesized path model.\n\n# Effect: Academic Ability\nlm.abilty = lm(ability ~ 1 + fam_back, data = sim_dat)\ntidy(lm.abilty)\nglance(lm.abilty)\nsqrt(1 - 0.174)\n\n# Effect: High School Coursework\nlm.coursework = lm(coursework ~ 1 + fam_back + ability + motivation, data = sim_dat)\ntidy(lm.coursework)\nglance(lm.coursework)\nsqrt(1 - 0.348)\n\n# Effect: High School Achievement\nlm.achieve = lm(achieve ~ 1 + fam_back + ability + motivation + coursework, data = sim_dat)\ntidy(achieve)\nglance(lm.achieve)\nsqrt(1 - 0.629)\n\n\n\n\n\n\n\n\n\nFigure 2: Hypothesized path model for several predictors of high school achievement. The path estimates are also included.\n\n\n\n\n\n\n\n\nEstimating Effects\nWe can now use the path coefficients to estimate the effects on high school achievement. Based on the hypothesized path model, each of the measured variables has a direct effect on high school achievement.\n\nThe direct effect of family background on high school achievement is 0.069.\nThe direct effect of academic ability on high school achievement is 0.551.\nThe direct effect of high school motivation on high school achievement is 0.013.\nThe direct effect of high school coursework on high school achievement is 0.310.\n\nThere are also indirect effects of these four causes of high school achievement. For example high school motivation not only directly influences high school achievement, but also influences it by influencing high school coursework (i.e., more motivated students take higher level courses which results in higher achievement).\nEach indirect effect is computed as the product of the path coefficients connecting the potential cause and effect. For example, the indirect effect of high school motivation on high school achievement via high school coursework is:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effect} &= 0.267 \\times 0.310 \\\\[1em]\n&= 0.083\n\\end{split}\n\\]\nFor some of these measured variables there are multiple indirect effects on achievement. The overall indirect effects are computed as the sum of each of the indirect effects. For example the indirect effects of academic ability on high school achievement are:\n\nAcademic ability influences high school coursework which, in turn, influences high school achievement. This indirect effect is \\(0.374 \\times 0.310 = 0.083\\)\nAcademic ability influences high school motivation which, in turn, influences high school achievement. This indirect effect is \\(0.152 \\times 0.013 = 0.002\\)\nAcademic ability influences high school motivation which, in turn, influences high school coursework, which then influences high school achievement. This indirect effect is \\(0.152 \\times 0.267 \\times 0.310 = 0.013\\)\n\nThus the overall indirect effects of academic ability on high school achievement is:\n\\[\n\\begin{split}\n\\mathrm{All~Indirect~Effects} &=  (0.374 \\times 0.310) + (0.152 \\times 0.013) + (0.152 \\times 0.267 \\times 0.310) \\\\[1em]\n&= 0.130\n\\end{split}\n\\]\nThe sum total of the direct and indirect effects give us the total effect of each hypothesized cause. For example, the total effect of academic ability on high school achievement is:\n\\[\n\\begin{split}\n\\mathrm{Total~Effects} &=  0.551 + 0.130 \\\\[1em]\n&= 0.681\n\\end{split}\n\\]\nPutting all of this together,\n\nGiven the adequacy of our model, academic ability has a fairly large, positive effect on high school achievement. Each one-standard deviation increase in academic ability will result in a 0.681-standard deviation unit change in high school achievement, on average. Much of that influence is from the direct effect of academic ability on high school achievement (\\(\\hat\\beta=0.551\\)). A small part of academic ability’s effect on high school achievement is due to its influence on other factors (e.g., motivation) which, in turn, effect high school achievement.\n\nThe table below gives the direct, indirect, and total effects for each of the hypothesized causes of high school achievement.\n\n#| label: tbl-effects\n#| tbl-cap: \"Standardized direct, indirect, and total effects of substantive predictors influencing high school achievment.\"\n#| echo: false\n\ndata.frame(\n  Measure =  c(\"Family Background\", \"Academic Ability\", \"High School Motivation\", \"High School Coursework\"),\n  Direct =   c(\"0.069\", \"0.551\", \"0.013\", \"0.310\"),\n  Indirect = c(\"0.348\", \"0.131\", \"0.083\", \"---\"),\n  Total =    c(\"0.417\", \"0.682\", \"0.096\", \"0.310\")\n)  |&gt;\n  kable(\n    format = 'html',\n    col.names = c(\"Measure\", \"Direct\", \"Indirect\", \"Total\"),\n    align = c(\"l\", \"c\", \"c\", \"c\"),\n    caption = \"Hello\"\n  ) |&gt;\n  add_header_above(c(\" \" = 1, \"Effect\" = 3))\n\n\nHello\n\n\n\n\n\n\n\n\n\n\nEffect\n\n\n\nMeasure\nDirect\nIndirect\nTotal\n\n\n\n\nFamily Background\n0.069\n0.348\n0.417\n\n\nAcademic Ability\n0.551\n0.131\n0.682\n\n\nHigh School Motivation\n0.013\n0.083\n0.096\n\n\nHigh School Coursework\n0.310\n---\n0.310\n\n\n\n\n\nThe largest direct influences of high school achievement are from high school coursework and academic ability. Family background also has a large influence on high school achievement, but primarily indirectly. Lastly, high school motivation has a small effect both directly and indirectly on high school achievement.\n\n\n\nFitting the Path Model with lavaan\nThe {lavaan} package includes the sem() function, which can estimate coefficients in a path model. To use this function, we define the path model by identifying each of the effects on a separate line in a character string. This string takes the model formula from each of the lm() functions we fitted earlier. We can also include the path names as multipliers of the influences in this model formula. Here we use the paths from our hypothesized path model to define the effects in the path model:\n\n# Define path model\npath.model = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n\"\n\nThen we can give this model as input to the sem() function along with the data frame that includes the data to estimate the model. We assign this to an object and use summary() to obtain the results. The rsquare=TRUE argument in summary() print the \\(R^2\\) values, the latter of which we can use to estimate the paths from the unmeasured variables to the respective effects. You can also include other options to output additional estimates (e.g., ci = TRUE for confidence intervals). See here for additional information.\n\n# Fit model\npm.1 = sem(path.model, data = keith)\n\n# Results\nsummary(pm.1, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\n\nWe can also estimate and obtain tests for the indirect effects as well. To do this we need to add model syntax defining each indirect effect we wish to include. This model syntax uses the path names to define how to compute that effect. For example to include the indirect effect of academic ability on high school achievement via high school motivation we would add the following line into our character string:\nindirect_ability_via_motivation := p7*p8\nThe path model would then be defined and fitted:\n\n# Define path model\npath.model.2 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_ability_via_motivation := p7*p8\n\"\n\n# Fit model\npm.2 = sem(path.model.2, data = keith)\n\n# Results\nsummary(pm.2, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indrct_blty_v_    0.002    0.003    0.599    0.549\n\n\nHere the indirect effect of academic ability on high school achievement via high school motivation is not statistically significant. This is likely because the direct effect of high school motivation on high school achievement is also not statistically significant (i.e., if that path is 0 then the product we get when computing the indirect effect will also be 0).\nHere we define and compute the overall indirect effects for each of the hypothesized causes:\n\n# Define path model\npath.model.3 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p8 + p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n\"\n\n# Fit model\npm.3 = sem(path.model.3, data = keith)\n\n# Results\nsummary(pm.2, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indrct_blty_v_    0.002    0.003    0.599    0.549\n\n\nFinally, we can fit a path model and estimate the direct, indirect, and total effects.\n\n# Define path model\npath.model.4 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p8*motivation + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p8 + p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n  total_fam_back := p3 + p1*p5*p10 + p1*p6 + p1*p7*p8 + p1*p7*p9*p10 + p4*p8 + p4*p9*p10 + p2*p10\n  total_ability := p6 + p7*p8 + p7*p9*p10 + p5*p10\n  total_motivation := p8 + p9*p10\n  total_coursework := p10\n\"\n\n# Fit model\npm.4 = sem(path.model.4, data = keith)\n\n# Results\nsummary(pm.4, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        14\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.069    0.022    3.202    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    motivatn  (p8)    0.013    0.021    0.604    0.546\n    courswrk (p10)    0.310    0.024   12.996    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indirct_fm_bck    0.348    0.024   14.751    0.000\n    indirect_ablty    0.131    0.013    9.868    0.000\n    indirect_mtvtn    0.083    0.010    8.003    0.000\n    total_fam_back    0.417    0.029   14.508    0.000\n    total_ability     0.682    0.023   29.460    0.000\n    total_motivatn    0.095    0.021    4.448    0.000\n    total_courswrk    0.310    0.024   12.996    0.000\n\n\n\n\n\nEmpirically Reducing the Model\nThe path between high school motivation and and high school achievement was not statistically significant (p = 0.546). This suggests that the effect we saw in the data may be attributable to sampling variation. One possibility is to reduce the model by removing this path.\n\n\n\n\n\nReduced path model for several predictors of high school achievement. In this model the direct effect between high school motivation and high school achievement was removed (path p8).\n\n\n\n\nNote that removing a path can change both direct and indirect effects. In our path model, we would need to remove any path that traverses through path p8. There are two approaches to doing this:\nOne approach is to remove the path and use the original estimates to re-compute the direct, indirect, and total effects. For example, to compute the indirect effects of academic ability on high school achievement we would use:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effects} &=  (0.374 \\times 0.310)  + (0.152 \\times 0.267 \\times 0.310) \\\\[1em]\n&= 0.128521\n\\end{split}\n\\]\nA second approach is to re-fit the model to the empirical data. The fitted path model is shown below.\n\n# Define path model\npath.model.5 = \"\n  ability ~ p1*fam_back\n  motivation ~ p4*fam_back + p7*ability\n  coursework ~ p2*fam_back + p5*ability + p9*motivation\n  achieve ~ p3*fam_back + p6*ability + p10*coursework\n  indirect_fam_back := p1*p5*p10 + p1*p6  + p1*p7*p9*p10 + p4*p9*p10 + p2*p10\n  indirect_ability := p7*p9*p10 + p5*p10\n  indirect_motivation := p9*p10\n  total_fam_back := p3 + p1*p5*p10 + p1*p6  + p1*p7*p9*p10 + p4*p9*p10 + p2*p10\n  total_ability := p6 + p7*p9*p10 + p5*p10\n  total_coursework := p10\n\"\n\n# Fit model\npm.5 = sem(path.model.5, data = keith)\n\n# Results\nsummary(pm.5, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        13\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.365\n  Degrees of freedom                                 1\n  P-value (Chi-square)                           0.546\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  ability ~                                           \n    fam_back  (p1)    0.417    0.029   14.508    0.000\n  motivation ~                                        \n    fam_back  (p4)    0.127    0.034    3.741    0.000\n    ability   (p7)    0.152    0.034    4.502    0.000\n  coursework ~                                        \n    fam_back  (p2)    0.165    0.028    5.838    0.000\n    ability   (p5)    0.374    0.028   13.194    0.000\n    motivatn  (p9)    0.267    0.026   10.158    0.000\n  achieve ~                                           \n    fam_back  (p3)    0.070    0.022    3.240    0.001\n    ability   (p6)    0.551    0.023   23.758    0.000\n    courswrk (p10)    0.314    0.023   13.841    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .ability           0.825    0.037   22.361    0.000\n   .motivation        0.944    0.042   22.361    0.000\n   .coursework        0.651    0.029   22.361    0.000\n   .achieve           0.371    0.017   22.361    0.000\n\nR-Square:\n                   Estimate\n    ability           0.174\n    motivation        0.055\n    coursework        0.348\n    achieve           0.629\n\nDefined Parameters:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n    indirct_fm_bck    0.347    0.024   14.740    0.000\n    indirect_ablty    0.130    0.013    9.866    0.000\n    indirect_mtvtn    0.084    0.010    8.189    0.000\n    total_fam_back    0.417    0.029   14.508    0.000\n    total_ability     0.682    0.023   29.460    0.000\n    total_courswrk    0.314    0.023   13.841    0.000\n\n\nComputing the indirect effects of academic ability on high school achievement for this re-fitted model we would use:\n\\[\n\\begin{split}\n\\mathrm{Indirect~Effects} &=  (0.374 \\times 0.314)  + (0.152 \\times 0.267 \\times 0.314) \\\\[1em]\n&= 0.1301794\n\\end{split}\n\\]\nOr, we could extract the estimate for indirect_ablty from the output. To get these results to more decimal places, we can use the parameterEstimates() function, which outputs a data frame of the results. We want to extract the est column, which includes the estimates. Note that the indirect effects of academic ability on high school achievement is the 16th element of the est vector.\n\n# Data frame of parameter estimates\nparameterEstimates(pm.5)\n\n                   lhs op                                              rhs\n1              ability  ~                                         fam_back\n2           motivation  ~                                         fam_back\n3           motivation  ~                                          ability\n4           coursework  ~                                         fam_back\n5           coursework  ~                                          ability\n6           coursework  ~                                       motivation\n7              achieve  ~                                         fam_back\n8              achieve  ~                                          ability\n9              achieve  ~                                       coursework\n10             ability ~~                                          ability\n11          motivation ~~                                       motivation\n12          coursework ~~                                       coursework\n13             achieve ~~                                          achieve\n14            fam_back ~~                                         fam_back\n15   indirect_fam_back :=    p1*p5*p10+p1*p6+p1*p7*p9*p10+p4*p9*p10+p2*p10\n16    indirect_ability :=                                 p7*p9*p10+p5*p10\n17 indirect_motivation :=                                           p9*p10\n18      total_fam_back := p3+p1*p5*p10+p1*p6+p1*p7*p9*p10+p4*p9*p10+p2*p10\n19       total_ability :=                              p6+p7*p9*p10+p5*p10\n20    total_coursework :=                                              p10\n                 label   est    se      z pvalue ci.lower ci.upper\n1                   p1 0.417 0.029 14.508  0.000    0.361    0.473\n2                   p4 0.127 0.034  3.741  0.000    0.060    0.193\n3                   p7 0.152 0.034  4.502  0.000    0.086    0.219\n4                   p2 0.165 0.028  5.838  0.000    0.110    0.221\n5                   p5 0.374 0.028 13.194  0.000    0.319    0.430\n6                   p9 0.267 0.026 10.158  0.000    0.215    0.318\n7                   p3 0.070 0.022  3.240  0.001    0.028    0.113\n8                   p6 0.551 0.023 23.758  0.000    0.506    0.597\n9                  p10 0.314 0.023 13.841  0.000    0.270    0.359\n10                     0.825 0.037 22.361  0.000    0.753    0.898\n11                     0.944 0.042 22.361  0.000    0.861    1.027\n12                     0.651 0.029 22.361  0.000    0.594    0.708\n13                     0.371 0.017 22.361  0.000    0.338    0.403\n14                     0.999 0.000     NA     NA    0.999    0.999\n15   indirect_fam_back 0.347 0.024 14.740  0.000    0.301    0.393\n16    indirect_ability 0.130 0.013  9.866  0.000    0.105    0.156\n17 indirect_motivation 0.084 0.010  8.189  0.000    0.064    0.104\n18      total_fam_back 0.417 0.029 14.508  0.000    0.361    0.473\n19       total_ability 0.682 0.023 29.460  0.000    0.636    0.727\n20    total_coursework 0.314 0.023 13.841  0.000    0.270    0.359\n\n# Estimates column\nparameterEstimates(pm.5)$est\n\n [1] 0.41700000 0.12651448 0.15224346 0.16516282 0.37442021 0.26686292\n [7] 0.07021160 0.55114506 0.31441104 0.82528489 0.94380759 0.65137255\n[13] 0.37079389 0.99900000 0.34678840 0.13049578 0.08390465 0.41700000\n[19] 0.68164084 0.31441104\n\n# Indirect effects of academic ability on high school achievement\nparameterEstimates(pm.5)$est[16]\n\n[1] 0.1304958\n\n\nWhether you reduce the model and re-fit it to the data, or use the estimates from the original fitted model and set non-significant effects to zero, the differences are minimal; in this case only one of the path coefficients changed when rounded to three decimal places.\n\n\n\n\n\nReferences\n\nKeith, T. V. (2015). Multiple regression and beyond: An introduction to multiple regression and structural equation modeling (2nd ed.). New York: Routledge."
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html",
    "href": "notes/03-01-regression-diagnostics.html",
    "title": "Regression Diagnostics",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to empirical diagnostics to detect extreme observations. We will use the contraception.csv data to evaluate the effect of female education level on contraception rates.\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Import data\ncontraception = read_csv(file = \"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/contraception.csv\")\n\n# View data\ncontraception\n\n# A tibble: 97 × 5\n   country                region                 contraceptive educ_female gni  \n   &lt;chr&gt;                  &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;\n 1 Algeria                Middle East and North…            57         5.9 High \n 2 Austria                Europe and Central As…            66         8.9 High \n 3 Azerbaijan             Europe and Central As…            55        10.5 High \n 4 Bangladesh             South Asia                        62         4.6 Low  \n 5 Belgium                Europe and Central As…            67        10.5 High \n 6 Belize                 Latin America and the…            51         9.2 High \n 7 Benin                  Sub-Saharan Africa                16         2   Low  \n 8 Bolivia                Latin America and the…            67         8.4 Low  \n 9 Bosnia and Herzegovina Europe and Central As…            46         7.2 High \n10 Botswana               Sub-Saharan Africa                53         8.7 High \n# ℹ 87 more rows"
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html#identifying-high-leverage-observations-in-the-contraception-example",
    "href": "notes/03-01-regression-diagnostics.html#identifying-high-leverage-observations-in-the-contraception-example",
    "title": "Regression Diagnostics",
    "section": "Identifying High Leverage Observations in the Contraception Example",
    "text": "Identifying High Leverage Observations in the Contraception Example\nIn practice, we can use the augment() function from the {broom} package to obtain the leverage values for each observation for a fitted model. These values are provided in the .hat column. Previously, we had assigned the augment output for lm.1 to an object called out_1.\n\n# View augmented data\nout_1\n\n# A tibble: 97 × 9\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\nHere, for example, we can see that the leverage value for the first observation is 0.0875. To determine which observations have high leverage, we will create an index plot of the leverage values.\n\n# Add case number to the augmented data\nout_1 = out_1 |&gt;\n  mutate(\n    case = row_number()\n    )\n\n# View augmented data\nout_1\n\n# A tibble: 97 × 10\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 2 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;\n\n# Create index plot\nggplot(data = out_1, aes(x = case, y = .hat)) +\n  geom_point(size = 4) +\n  theme_bw() +\n  xlab(\"Observation number\") +\n  ylab(\"Leverage value\")\n\n\n\n\n\n\n\nFigure 6: Index plot of the leverage values for Model 1.\n\n\n\n\n\nWhile it seems we may be able to identify observations with high leverage in this plot, sometimes this can be difficult depending on the data. For example, the two highest observations seem like they probably have high leverage relative to the others, but what about the four observations with leverage values between 0.10 and 0.12?\nOne criterion that applied researchers use is that an observation has high leverage if:\n\\[\nh_{ii} &gt; \\frac{2p}{n}\n\\]\nwhere p is the trace of H, which also happens to be the sum of the \\(h_{ii}\\) values. This implies that,\n\\[\n\\begin{split}\nh_{ii} &&gt; \\frac{2p}{n} \\\\[1em]\n&&gt; \\frac{2\\sum h_{ii}}{n} \\\\[1em]\n&&gt; 2 \\bar{h}\n\\end{split}\n\\]\nwhere \\(\\bar{h}\\) is the average leverage value. Thus, we are identifying observations with a leverage value greater than twice the average value, and those are the observations we are saying have high leverage. It is often useful to draw a horizontal line in the index plot at this value. Below we add in this line, and also plot the observations’ case value rather than plotting points. This helps us identify the cases with high leverage.\n\n# Compute cutoff\ncutoff = 2 * mean(out_1$.hat)\ncutoff\n\n[1] 0.08247423\n\n# Create index plot\nggplot(data = out_1, aes(x = case, y = .hat)) +\n  geom_text(aes(label = case)) +\n  geom_hline(yintercept = cutoff, color = \"#cc79a7\") +\n  theme_bw() +\n  xlab(\"Observation number\") +\n  ylab(\"Leverage value\")\n\n\n\n\n\n\n\nFigure 7: Index plot of the leverage values from Model 1. Observations are identified by their case value. The reddish-purple line demarcates observations with a leverage value greater than two times the average leverage value.\n\n\n\n\n\nBased on this criterion, there are several observations that have high leverage values in this model. Since these observations represent countries, and we have country names in the original data, we could also create this index plot using country names to label the observations instead of case numbers.\n\n# Add country names to augmented data\nout_1 = out_1 |&gt;\n  mutate(\n    country = contraception$country\n  )\n\n# View data\nout_1\n\n# A tibble: 97 × 11\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 3 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;\n\n# Create index plot\nggplot(data = out_1, aes(x = case, y = .hat)) +\n  geom_text(aes(label = country)) +\n  geom_hline(yintercept = cutoff, color = \"#cc79a7\") +\n  theme_bw() +\n  xlab(\"Observation number\") +\n  ylab(\"Leverage value\")\n\n\n\n\n\n\n\nFigure 8: Index plot of the leverage values from Model 1. Observations are identified by their country name. The reddish-purple line demarcates observations with a leverage value greater than two times the average leverage value."
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html#statistical-test-for-identifying-observations-with-large-residuals",
    "href": "notes/03-01-regression-diagnostics.html#statistical-test-for-identifying-observations-with-large-residuals",
    "title": "Regression Diagnostics",
    "section": "Statistical Test for Identifying Observations with Large Residuals",
    "text": "Statistical Test for Identifying Observations with Large Residuals\nSome educational scientists evaluate whether a particular observation is a regression outlier, by using a hypothesis test to determine whether its residual differs statistically from 0. To do this, we can use a t-test where we evaluate the standardized residual,\n\\[\ne^*_i = \\frac{e_i}{\\mathrm{SE}(e_i)}\n\\]\nThis is evaluated in a t-distribution with df equivalent to the residual degrees-of-freedom from the model. The t-value here looks a lot like the formula we used to compute the standardized residual.\nUnfortunately, we cannot use the standardized residual here since, if we do, the numerator and denominator are not independent (the term \\(s^2_e\\) in the denominator is a function of \\(e_i\\).). This would mean that the resulting statistic is not t-distributed. To fix this problem, we can compute an estimate of \\(s^2_e\\) that is computed based on the regression deleting the ith observation. That is,\n\\[\nt_i = \\frac{e_i}{\\sqrt{s^2_{e(-i)} (1 - h_{ii})}}\n\\] To differentiate this change in the standard error, sometimes statisticians refer to this value as a studentized residual rather than a standardized residual.4\n\n\nComputing the Studentized Residual\nAll of the components to compute the stuentized residuals are provided in the augment() output.\n\n# View augmented output\nout_1\n\n# A tibble: 97 × 11\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 3 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;\n\n\nThe values in the .sigma column are estimates of \\(s_e\\) computed based on the regression deleting the ith observation. That is, they are \\(s_{e(-i)}\\). To compute the studentized residual for Observation 1,\n\\[\n\\begin{split}\nt_1 &= \\frac{e_i}{s_{e(-i)}\\sqrt{(1 - h_{ii})}} \\\\[1em]\n&= \\frac{1.34}{14.5\\sqrt{1 - .0875}} \\\\[1em]\n&= 0.09674318\n\\end{split}\n\\] We can also use this formula to compute the studentized residual for each observation in the augmented() output.\n\n# Compute studentized residuals\nout_1 = out_1 |&gt;\n  mutate(\n    t_i = .resid / (.sigma * sqrt(1 - .hat))\n  )\n\n# View output\nout_1\n\n# A tibble: 97 × 12\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 4 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;, t_i &lt;dbl&gt;\n\n\nAlternatively, we can also use the rstudent() function to compute the studentized residuals directly. This function takes the model object as its input.\n\n# Compute studentized residuals\nout_1 = out_1 |&gt;\n  mutate(\n    t_i = rstudent(lm.1)\n  )\n\n# View output\nout_1\n\n# A tibble: 97 × 12\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 4 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;, t_i &lt;dbl&gt;\n\n\n\n\n\nTesting Observations for Outlyingness\nNow that we have the t-value for each of the residuals, we can evaluate each residual for outlyingness by evaluating each t-statistics in the t-distribution with df equivalent to the residual degrees-of-freedom from the model. In our case,\n\\[\n\\mathit{df} = 93\n\\] Thus, to evaluate whethe the first observation is a regression outlier, we would evaluate\n\\[\nt(93) = 0.097\n\\] to determine if the residual is statistically different than 0. Below, we compute the p-value associated with the first observation.\n\n# Compute p-value for Obs. 1\n2 * pt(-0.097, df = 93)\n\n[1] 0.9229351\n\n\nBecause we need to carry out this test for each of the 97 observations, we need to adjust each p-value based on the number of comparisons. This is typically done by using a Bonferroni adjustment.\n\n# Bonferroni adjustment for Obs. 1\n0.9229351 * 97\n\n[1] 89.5247\n\n\nSince this results in a value greater than 1, and p-values need to be between 0 and 1, we report this p-value as 1. Since this is not statistically significant (at the \\(\\alpha = 0.05\\) level), Observation 1 is not considered a regression outlier. Below is the syntax I use to compute the Bonferroni adjusted p-values for each of the 97 observations.\n\n# Bonferroni adjustment for all observations\nout_1 = out_1 |&gt;\n  mutate(\n    p = 2 * pt(-abs(t_i), df = 97),\n    p_adj = p.adjust(p, method = \"bonferroni\")\n  )\n\n# Order data by adjusted p-values\narrange(out_1, p_adj)\n\n# A tibble: 97 × 14\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            29        10          0    70.4 -41.4   0.116    13.7 0.307    \n 2            19         5.4        1    54.1 -35.1   0.109    14.0 0.204    \n 3            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 4            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 5            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 6            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 7            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 8            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 9            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n10            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n# ℹ 87 more rows\n# ℹ 6 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;, t_i &lt;dbl&gt;,\n#   p &lt;dbl&gt;, p_adj &lt;dbl&gt;\n\n\nThese results indicate that none of the observations are more extreme than would be expected given the sample size. In other words, the test would not identify any observations as regression outliers."
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html#dfbetas",
    "href": "notes/03-01-regression-diagnostics.html#dfbetas",
    "title": "Regression Diagnostics",
    "section": "DFBETAS",
    "text": "DFBETAS\nThe most direct way to measure the influence of an observation is to drop that observation from the dataset and measure the difference in regression coefficients. We can define this difference as \\(d_{ij}\\) (the difference between the jth regression coefficient when the ith observation is dropped) as:\n\\[\nd_{ij} = \\hat{\\beta}_j - \\hat{\\beta}_{j(-i)}\n\\]\nThese are referred to as DFBETAs following the naming convention set by Belsley, Kuh, & Welsch (1980). The dfbeta() function computes the DFBETA values for each coefficient for all observations. This function takes the fitted model object as its input. (Here we pipe this output into a data frame for better screen printing in the notes.)\n\ndfbeta(lm.1) |&gt;\n  data.frame()\n\n    X.Intercept.   educ_female     high_gni educ_female.high_gni\n1  -1.809840e-16  4.612479e-17  0.305434363        -0.0299422430\n2  -2.140412e-16  5.465651e-17  0.027600534        -0.0012275087\n3   1.467636e-15 -3.054247e-16  0.884314292        -0.1334941558\n4   2.843749e-01  9.973825e-03 -0.284374901        -0.0099738248\n5   4.805681e-17 -4.845179e-19  0.193551196        -0.0292180662\n6  -1.579880e-16  3.410636e-17 -0.290848803        -0.0043401015\n7  -9.890961e-01  1.390126e-01  0.989096139        -0.1390126107\n8  -1.418416e-01  5.117060e-02  0.141841614        -0.0511705953\n9  -6.670330e-16  7.298375e-17 -1.926866474         0.1768438267\n10 -1.706390e-16  2.160100e-17 -0.562197691         0.0338064252\n11  2.860225e-17 -9.408467e-19  0.106970012        -0.0157319062\n12 -9.487940e-02  1.354121e-02  0.094879404        -0.0135412113\n13 -3.119725e-01  4.237847e-02  0.311972499        -0.0423784680\n14  6.099708e-01 -6.209117e-02 -0.609970839         0.0620911695\n15 -1.531780e-01 -3.104119e-02  0.153178047         0.0310411906\n16 -3.899417e-16  1.683153e-17 -1.537721805         0.1904754297\n17 -1.663461e+00  2.719807e-01  1.663460512        -0.2719807369\n18  2.424761e-17 -1.272266e-17 -0.036370422         0.0242160503\n19  1.015791e-15 -1.012343e-16  3.324181371        -0.3096769079\n20 -7.740930e-01  1.070222e-01  0.774093016        -0.1070221810\n21  4.878804e-16 -7.197380e-17  0.949325059        -0.0694909780\n22  1.947189e-17 -1.963193e-19  0.078424013        -0.0118387179\n23 -2.412386e-16 -1.960572e-17 -1.676194531         0.2095356712\n24  3.278130e-16 -3.446449e-17  0.930232256        -0.0809329750\n25  7.423518e-16 -8.063725e-17  2.480347677        -0.2235719332\n26  7.981224e-02  2.795296e-02 -0.079812241        -0.0279529611\n27 -5.838447e-02  1.035264e-01  0.058384473        -0.1035263532\n28  5.448557e-01 -8.289445e-02 -0.544855653         0.0828944480\n29 -1.471893e-16  5.675495e-18 -0.520688515         0.0722713331\n30  5.064284e-03 -9.826809e-02 -0.005064284         0.0982680935\n31 -3.553108e-17 -8.649953e-18 -0.165376727         0.0360679552\n32 -1.439772e+00  2.317999e-01  1.439772232        -0.2317999186\n33 -2.072365e-01  2.109536e-02  0.207236480        -0.0210953616\n34  1.925062e-01  6.742222e-02 -0.192506197        -0.0674222170\n35  3.076175e-16 -2.048721e-17  1.065516502        -0.1422425380\n36  5.356793e-01 -5.452876e-02 -0.535679264         0.0545287576\n37 -6.684626e-02  3.727942e-02  0.066846265        -0.0372794179\n38  2.388282e-16 -1.756613e-17  0.749118332        -0.0758907733\n39  8.690212e-18  9.908841e-19  0.028424337        -0.0097777886\n40  1.188607e-18 -7.306182e-18 -0.078028084         0.0201298504\n41  9.460634e-16 -6.300749e-17  3.276947363        -0.4374604326\n42 -4.658781e-17  1.690082e-17 -0.110348375        -0.0232121148\n43  1.778343e-16  1.287934e-18  0.570817338        -0.0967887948\n44  7.647221e-02  3.593524e-02 -0.076472205        -0.0359352392\n45 -2.383520e-16  7.575492e-18 -0.727680745         0.0982654987\n46  3.363947e-01 -2.209116e-02 -0.336394733         0.0220911563\n47 -3.077206e-16  3.656960e-17 -0.697305200         0.0614717745\n48 -5.313292e-02  3.154562e-02  0.053132921        -0.0315456166\n49 -2.255996e-01  3.007042e-02  0.225599650        -0.0300704163\n50  1.759258e-02  1.469433e-03 -0.017592583        -0.0014694330\n51  6.356182e-01 -5.828158e-02 -0.635618161         0.0582815752\n52 -1.475986e-16  3.186350e-17 -0.271722395        -0.0040546936\n53 -3.159305e-15  3.288994e-16 -9.299314822         0.9242394241\n54 -9.811884e-01  1.492782e-01  0.981188364        -0.1492781940\n55  7.016250e-17 -2.891925e-17  0.029817683         0.0414988376\n56  4.925491e-17 -6.062697e-18  0.146761088        -0.0121279080\n57  1.382758e-16 -1.200440e-17  0.362244594        -0.0292598962\n58  4.786195e-01 -1.520272e-01 -0.478619451         0.1520271750\n59  3.263972e-01 -1.160848e-01 -0.326397236         0.1160848324\n60  1.167158e+00 -1.188093e-01 -1.167157647         0.1188092589\n61 -1.252530e-01  2.016544e-02  0.125253028        -0.0201654408\n62 -5.674762e-17  6.177372e-18 -0.186384854         0.0180129969\n63  8.285264e-01 -1.082017e-01 -0.828526415         0.1082016649\n64  4.953251e-01  4.137238e-02 -0.495325133        -0.0413723826\n65 -1.293715e+00  2.082850e-01  1.293714543        -0.2082849767\n66 -3.735811e-01  1.018922e-02  0.373581124        -0.0101892181\n67 -2.155423e-01  2.089441e-02  0.215542306        -0.0208944072\n68 -1.277315e-17  6.702040e-18  0.019159203        -0.0127565258\n69  3.324209e-16 -3.593700e-17  0.872065197        -0.0777844084\n70  4.678457e-01 -1.602107e-01 -0.467845723         0.1602106589\n71  3.515446e-16 -4.327098e-17  1.047470453        -0.0865599014\n72  4.699954e-19  2.178366e-20  0.002202570        -0.0003430246\n73  5.349811e-01 -5.686828e-02 -0.534981114         0.0568682775\n74 -5.712608e-02  2.771303e-03  0.057126075        -0.0027713033\n75 -4.285702e-01  3.929678e-02  0.428570187        -0.0392967777\n76 -8.526222e-17  1.840636e-17 -0.156963952        -0.0023422461\n77 -6.075648e-01  8.539025e-02  0.607564760        -0.0853902469\n78 -1.915765e-16  3.647690e-18 -0.771513772         0.0993810833\n79 -1.610348e-16  1.185866e-17 -0.623609346         0.0792542730\n80 -3.199816e-18  1.966877e-17  0.210057275        -0.0541910212\n81  1.428380e-17 -5.887430e-18  0.006070336         0.0084484051\n82 -6.831545e-16  7.437966e-17 -1.615708833         0.1470205679\n83 -1.594836e-01  7.157448e-02  0.159483601        -0.0715744793\n84  2.603053e+00 -8.039487e-01 -2.603053159         0.8039486898\n85 -1.422400e-01 -2.290575e-03  0.142240016         0.0022905747\n86  8.761706e-16 -8.731968e-17  2.867272419        -0.2671117961\n87 -7.158340e-01  6.939207e-02  0.715834021        -0.0693920735\n88  6.885061e-02  4.490257e-02 -0.068850611        -0.0449025724\n89 -5.580884e-02  8.171837e-04  0.055808838        -0.0008171837\n90  9.249268e-01 -2.686135e-01 -0.924926763         0.2686135450\n91 -2.990366e-16  1.661098e-17 -1.314549909         0.1598909357\n92  2.262538e-16 -2.864121e-17  0.745429714        -0.0448246484\n93  1.527551e-16 -1.933710e-17  0.503276491        -0.0302633385\n94  2.579832e-01  6.852760e-02 -0.257983213        -0.0685275959\n95  2.316929e-01 -3.599230e-02 -0.231692856         0.0359923019\n96 -2.685758e-03 -1.034350e-02  0.002685758         0.0103434982\n97 -1.326732e-01  8.471798e-02  0.132673221        -0.0847179785\n\n\nPositive DFBETA values indicate that removing the observation results in a lower regression coefficient value, while a negative DFBETA value would indicate that removing the observation results in a higher regression coefficient value. For example, focusing on the interaction term, we see that removing Observation 1 would result in a higher value on the interaction coefficient by .0299 units.\nBecause the DFBETA values are in the same unit as the coefficients, it can sometimes be difficult to assess whether those values are indicative of a small or large amount of influence. To remedy this, the DFBETA values are typically scaled. To do this, the DFBETA value is divided by a scaled RMSE estimate,\n\\[\nd^*_{ij} = \\frac{d_{ij}}{s_{e(-i)}\\sqrt{c_{jj}}}\n\\]\nwhere \\(s_{e(-i)}\\) is the RMSE estimate when the ith observation is deleted from the model, and \\(c_{jj}\\) is the jth diagonal element of the \\((\\mathbf{XX}^\\intercal)^{-1}\\) matrix corresponding to the appropriate regression coefficient. For example, here we compute the \\((\\mathbf{XX}^\\intercal)^{-1}\\) matrix for the fitted model.\n\nX = model.matrix(lm.1)\nsolve(t(X) %*% X)\n\n                     (Intercept)  educ_female    high_gni educ_female:high_gni\n(Intercept)           0.07946790 -0.013509292 -0.07946790          0.013509292\neduc_female          -0.01350929  0.003068899  0.01350929         -0.003068899\nhigh_gni             -0.07946790  0.013509292  0.62476222         -0.070745716\neduc_female:high_gni  0.01350929 -0.003068899 -0.07074572          0.009320611\n\n\nThe \\(c_{jj}\\) value corresponding to the interaction term is 0.009320611.\nAgain, using the .sigma value from the augmented output to obtain \\(s_{e(-i)}\\), we can compute the scaled DFBETA value for the interaction term based on the first observation as:\n\n#Compute scaled DFBETA for interaction term (Obs. 1)\n-0.0299422430 / (14.5 * sqrt(0.009320611))\n\n[1] -0.02138918\n\n\nRemoving Observation 1 would result in a higher value on the interaction coefficient by .0214 standard deviation units. We can use the dfbetas() function to compute all the scaled DFBETA values. (Here we pipe this output into a data frame for better screen printing in the notes.)\n\n# Get scaled DFBETA values\ndfbetas(lm.1) |&gt;\n  data.frame()\n\n    X.Intercept.   educ_female      high_gni educ_female.high_gni\n1  -4.433245e-17  5.749375e-17  0.0266832052        -0.0214160621\n2  -5.242800e-17  6.812593e-17  0.0024111369        -0.0008779386\n3   3.617765e-16 -3.831165e-16  0.0777440666        -0.0960855429\n4   7.013616e-02  1.251748e-02 -0.0250138461        -0.0071826736\n5   1.177459e-17 -6.040952e-19  0.0169131804        -0.0209033588\n6  -3.893665e-17  4.277348e-17 -0.0255646819        -0.0031232603\n7  -2.444463e-01  1.748251e-01  0.0871810332        -0.1003166199\n8  -3.475724e-02  6.380677e-02  0.0123960646        -0.0366130427\n9  -1.642356e-16  9.144304e-17 -0.1692037527         0.1271403667\n10 -4.194639e-17  2.702058e-17 -0.0492883093         0.0242655030\n11  7.006362e-18 -1.172778e-18  0.0093452923        -0.0112524575\n12 -2.324154e-02  1.687933e-02  0.0082890250        -0.0096855481\n13 -7.648975e-02  5.287334e-02  0.0272798340        -0.0303393205\n14  1.504878e-01 -7.795191e-02 -0.0536710212         0.0447296821\n15 -3.772077e-02 -3.889798e-02  0.0134529981         0.0223200986\n16 -9.572597e-17  2.102608e-17 -0.1346314024         0.1365346070\n17 -4.129235e-01  3.435578e-01  0.1472678962        -0.1971373519\n18  5.950651e-18 -1.588833e-17 -0.0031833359         0.0173529221\n19  2.521061e-16 -1.278531e-16  0.2942402553        -0.2244199106\n20 -1.907022e-01  1.341656e-01  0.0680133702        -0.0769857117\n21  1.201607e-16 -9.020448e-17  0.0833877932        -0.0499748092\n22  4.769679e-18 -2.447083e-19  0.0068512297        -0.0084675802\n23 -5.927317e-17 -2.451313e-17 -0.1468839394         0.1503290959\n24  8.045389e-17 -4.304248e-17  0.0814236740        -0.0579989312\n25  1.837374e-16 -1.015613e-16  0.2189469401        -0.1615767196\n26  1.960034e-02  3.493224e-02 -0.0069904006        -0.0200445157\n27 -1.444487e-02  1.303382e-01  0.0051517184        -0.0747895376\n28  1.337243e-01 -1.035282e-01 -0.0476923477         0.0594056661\n29 -3.609193e-17  7.081779e-18 -0.0455355445         0.0517456430\n30  1.254988e-03 -1.239192e-01 -0.0004475875         0.0711062325\n31 -8.716205e-18 -1.079785e-17 -0.0144687824         0.0258353470\n32 -3.564971e-01  2.920654e-01  0.1271435925        -0.1675904048\n33 -5.080248e-02  2.631542e-02  0.0181185487        -0.0151000839\n34  4.788396e-02  8.534006e-02 -0.0170776661        -0.0489690885\n35  7.557911e-17 -2.561405e-17  0.0933661593        -0.1020455315\n36  1.319400e-01 -6.834423e-02 -0.0470559937         0.0392166870\n37 -1.638296e-02  4.649315e-02  0.0058429324        -0.0266782922\n38  5.850681e-17 -2.189785e-17  0.0654500091        -0.0542854509\n39  2.129023e-18  1.235314e-18  0.0024835841        -0.0069946238\n40  2.913210e-19 -9.112314e-18 -0.0068206066         0.0144061454\n41  2.387328e-16 -8.090745e-17  0.2949169340        -0.3223325830\n42 -1.147841e-17  2.118955e-17 -0.0096964748        -0.0166992830\n43  4.380164e-17  1.614258e-18  0.0501430182        -0.0696103125\n44  1.879670e-02  4.494726e-02 -0.0067037871        -0.0257912459\n45 -5.847651e-17  9.457539e-18 -0.0636710533         0.0703942845\n46  8.269692e-02 -2.763524e-02 -0.0294936029         0.0158574107\n47 -7.544835e-17  4.562659e-17 -0.0609753931         0.0440091155\n48 -1.302033e-02  3.933708e-02  0.0046436625        -0.0225720580\n49 -5.528843e-02  3.750074e-02  0.0197184486        -0.0215183490\n50  4.309282e-03  1.831596e-03 -0.0015368923        -0.0010509905\n51  1.571235e-01 -7.331295e-02 -0.0560376114         0.0420677984\n52 -3.634751e-17  3.992920e-17 -0.0238647248        -0.0029155750\n53 -8.030932e-16  4.254437e-16 -0.8430705169         0.6860134435\n54 -2.419044e-01  1.872804e-01  0.0862744784        -0.1074636312\n55  1.734942e-17 -3.638912e-17  0.0026296162         0.0299632966\n56  1.206534e-17 -7.557195e-18  0.0128215221        -0.0086746057\n57  3.388602e-17 -1.496992e-17  0.0316602966        -0.0209373172\n58  1.174812e-01 -1.898908e-01 -0.0418993263         0.1089615083\n59  8.011205e-02 -1.449878e-01 -0.0285717186         0.0831956380\n60  2.936884e-01 -1.521290e-01 -0.1047430530         0.0872933542\n61 -3.068194e-02  2.513662e-02  0.0109426200        -0.0144236771\n62 -1.390009e-17  7.699781e-18 -0.0162824263         0.0128833754\n63  2.045360e-01 -1.359258e-01 -0.0729471366         0.0779957645\n64  1.250201e-01  5.313791e-02 -0.0445880183        -0.0304911303\n65 -3.196590e-01  2.618852e-01  0.1140054030        -0.1502727056\n66 -9.213908e-02  1.278805e-02  0.0328611197        -0.0073379285\n67 -5.284594e-02  2.606837e-02  0.0188473439        -0.0149583218\n68 -3.130323e-18  8.358007e-18  0.0016745844        -0.0091284534\n69  8.153797e-17 -4.485574e-17  0.0762884941        -0.0557105987\n70  1.149812e-01 -2.003642e-01 -0.0410076800         0.1149712169\n71  8.641991e-17 -5.412960e-17  0.0918361608        -0.0621332223\n72  1.151206e-19  2.715157e-20  0.0001924100        -0.0002453343\n73  1.317138e-01 -7.124715e-02 -0.0469753226         0.0408824186\n74 -1.399424e-02  3.454647e-03  0.0049910036        -0.0019823154\n75 -1.054106e-01  4.918399e-02  0.0375943552        -0.0282223264\n76 -2.092145e-17  2.298305e-17 -0.0137364201        -0.0016781909\n77 -1.493172e-01  1.067899e-01  0.0532535315        -0.0612772536\n78 -4.697317e-17  4.551246e-18 -0.0674667726         0.0711516808\n79 -3.946581e-17  1.478909e-17 -0.0545070131         0.0567149614\n80 -7.873710e-19  2.462840e-17  0.0184344676        -0.0389363640\n81  3.500036e-18 -7.341063e-18  0.0005304932         0.0060447316\n82 -1.680025e-16  9.307986e-17 -0.1417093568         0.1055719903\n83 -3.912236e-02  8.934531e-02  0.0139528684        -0.0512673474\n84  6.721937e-01 -1.056439e+00 -0.2397358061         0.6061964429\n85 -3.489167e-02 -2.859230e-03  0.0124440081         0.0016406583\n86  2.167150e-16 -1.099049e-16  0.2529342780        -0.1929154391\n87 -1.772340e-01  8.742771e-02  0.0632099544        -0.0501670071\n88  1.694053e-02  5.622053e-02 -0.0060417866        -0.0322599721\n89 -1.367217e-02  1.018729e-03  0.0048761384        -0.0005845584\n90  2.273715e-01 -3.360170e-01 -0.0810913503         0.1928103220\n91 -7.332983e-17  2.072794e-17 -0.1149666394         0.1144863704\n92  5.576992e-17 -3.592528e-17  0.0655313856        -0.0322622556\n93  3.752339e-17 -2.417142e-17  0.0440911431        -0.0217068465\n94  6.448286e-02  8.716115e-02 -0.0229976142        -0.0500140495\n95  5.676953e-02 -4.487628e-02 -0.0202466773         0.0257505155\n96 -6.579456e-04 -1.289422e-02  0.0002346543         0.0073988499\n97 -3.261470e-02  1.059767e-01  0.0116319340        -0.0608106195\n\n\nAs with our other measures, it is useful to create an index plot of the scaled DFBETA values. Typically you would create separate index plots for each of the coefficients. Below, I include the scaled DFBETA values in a data frame, and create the index plots. Because we are interested in large scaled DFBETA values regardless of sign, I plot the absolute value of the scaled DFBETA values in each case plot. Here I focus on the index plot associated with the interaction term since that is the primary effect of interests in an interaction model.\n\n# Set up data with case number and scaled DFBETA values\nscl_dfbeta = data.frame(\n  case = 1:97\n) |&gt;\n  cbind(dfbetas(lm.1))\n\n\n# Create index plots\nggplot(data = scl_dfbeta, aes(x = case, y = abs(`educ_female:high_gni`))) +\n  geom_text(aes(label = case), size = 3) +\n  xlab(\"Observation number\") +\n  ylab(\"Scaled DFBETA value\") +\n  theme_bw() +\n  geom_hline(yintercept = 2 / sqrt(97), color = \"#cc79a7\")\n\n\n\n\n\n\n\nFigure 10: Index plot of the absolute value of the standardized DFBETA values for the interaction effect between female education level and high GNI.\n\n\n\n\n\nNote that because the variable name included a colon (which is a special character in R syntax), we had to enclose the variable name educ_female:high_gni in back ticks in the ggplot() syntax. We also add a guideline to demarcate observations where \\(\\vert d^*_{ij}\\vert &gt; \\frac{2}{\\sqrt{n}}\\).\nWe want to use the plot to identify cases that have an absolute scaled DFBETA value that is substantially higher than most other observations’ absolute scaled DFBETA values. (The guideline can help us identify these cases.) Observations 53 (Maldives), 84 (Tajikistan), and 41 (Japan) seem to have influence on the interaction coefficient of interest. Observation 19 (Colombia) also may have a modest amount of influence on the interaction term."
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html#cooks-distance",
    "href": "notes/03-01-regression-diagnostics.html#cooks-distance",
    "title": "Regression Diagnostics",
    "section": "Cook’s Distance",
    "text": "Cook’s Distance\nWhile the DFBETA and scaled DFBETA values directly measure the impact of an observation on each of the coefficients, these measures of influence are not commonly used in practice. Instead, Cook’s Distance (Cook, 1977) is a more commonly used measure of influence by applied researchers. This measure is computed as:\n\\[\nD_i = \\frac{(e^*_i)^2}{p + 1} \\times \\frac{h_{ii}}{1 - h_{ii}}\n\\]\nwhere \\(e^*_i\\) is the standardized residual for the ith observation, p is the number of predictors in the fitted model, and \\(h_{ii}\\) is the leverage value for the ith observation. Below we compute Cook’s Distance for Observation 1 using values obtained from the augment() output.\n\n# Compute D_i for Obs. 1\n0.0976^2 / (3 + 1) * 0.0875 / (1 - 0.0875)\n\n[1] 0.0002283573\n\n\nThe Cook’s Distance values are also generated in the .cooksd column of the augment() output.\n\n# View augmented output\nout_1\n\n# A tibble: 97 × 14\n   contraceptive educ_female high_gni .fitted  .resid   .hat .sigma   .cooksd\n           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1            57         5.9        1    55.7   1.34  0.0875   14.5 0.000228 \n 2            66         8.9        1    65.2   0.752 0.0217   14.5 0.0000155\n 3            55        10.5        1    70.4 -15.4   0.0326   14.4 0.00990  \n 4            62         4.6        0    45.9  16.1   0.0201   14.4 0.00653  \n 5            67        10.5        1    70.4  -3.36  0.0326   14.5 0.000474 \n 6            51         9.2        1    66.2 -15.2   0.0213   14.4 0.00619  \n 7            16         2          0    34.1 -18.1   0.0377   14.4 0.0162   \n 8            67         8.4        0    63.1   3.88  0.0691   14.5 0.00145  \n 9            46         7.2        1    59.8 -13.8   0.0452   14.4 0.0114   \n10            53         8.7        1    64.6 -11.6   0.0226   14.4 0.00384  \n# ℹ 87 more rows\n# ℹ 6 more variables: .std.resid &lt;dbl&gt;, case &lt;int&gt;, country &lt;chr&gt;, t_i &lt;dbl&gt;,\n#   p &lt;dbl&gt;, p_adj &lt;dbl&gt;\n\n\nObservations with a Cook’s Distance such that \\(D_i &gt; \\frac{4}{n-p-1}\\) are considered influential. In this case, Observation 1 would not be considered influential as its Cook’s Distance of 0.0002 is not larger than the cutoff of 0.043.\nUnlike the DFBETA measures which computes multiple measures of influence for each observation, Cook’s Distance computes a single measure of influence for each observation. This measures the overall influence across all of the predictors rather than the influence on any one predictor. (It is probably for this reason that Cook’s Distance is favored by applied researchers.) We can, once again, create an index plot to plot the Cook’s Distance values. Within this plot, a guideline can be added to identify cases larger than the cutoff.\n\n# Add case number to data\nout_1 = out_1 |&gt;\n  mutate(case = row_number())\n\n# Create index plot\nggplot(data = out_1, aes(x = case, y = .cooksd)) +\n  geom_text(aes(label = case), size = 3) +\n  xlab(\"Observation number\") +\n  ylab(\"Cook's D value\") +\n  theme_bw() +\n  geom_hline(yintercept = 4 / (97 - 3- 1), color = \"#cc79a7\")\n\n\n\n\n\n\n\nFigure 11: Index plot of Cook’s D values for the 97 observations.\n\n\n\n\n\nBased on the plot, we can identify Observations 84 (Tajikistan), 53 (Maldives), and 41 (Japan) as being influential in the model."
  },
  {
    "objectID": "notes/03-01-regression-diagnostics.html#footnotes",
    "href": "notes/03-01-regression-diagnostics.html#footnotes",
    "title": "Regression Diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen there is more than one predictor, leverage is measure of how far away an observation is from the centroid of the predictor space.↩︎\nBecause we are dividing by the standard error sometimes these are referred to as internally studentized residuals rather than standardized residuals.↩︎\nIf the sample size is large, it is better to use the heuristic that regression outliers have a standardized residual more than 3 SEs from zero.↩︎\nStudentized residuals are also sometimes referred to as: deleted studentized residuals, externally studentized residuals, and in some books and papers, even as standardized residuals. Ugh.↩︎"
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "",
    "text": "In this set of notes, we will use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID) available in slid.csv to explain variation in the hourly wage rate of employed citizens in Ontario."
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "Violating Homoskedasticity",
    "text": "Violating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\n\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\n\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes)."
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nIs there a power transformation that would better “fix” the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis transformation is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\).\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\nslid = slid |&gt;\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n# Examine residual plots\nresidual_plots(lm_bc)\n\n\n\n\n\n\n\nFigure 5: Residual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficient-level output:\n\n# Coeffifient-level output\ntidy(lm_bc, conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   1.04    0.0475        21.9 1.80e-100   0.947     1.13  \n2 age           0.0227  0.000687      33.0 2.85e-211   0.0213    0.0240\n3 education     0.0707  0.00272       26.0 5.14e-138   0.0654    0.0760\n4 male          0.282   0.0164        17.2 4.99e- 64   0.250     0.315 \n\n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y, controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?"
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "Profile Plot for Different Transformations",
    "text": "Profile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\n\n\n\nFigure 6: Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n# Zomm in on confidence limits\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\n\n\n\nFigure 7: Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals."
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "Assume Error Variances are Known",
    "text": "Assume Error Variances are Known\nLet’s assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance–covariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\n\n\n\nTable 1: Example data for weighted least squares.\n\n\n\n\nExample data for weighted least squares.\n\n\nClass Average ACT\nTeacher ACT\nClass SD\n\n\n\n\n17.3\n21\n5.99\n\n\n17.1\n20\n3.94\n\n\n16.4\n19\n1.90\n\n\n16.4\n18\n0.40\n\n\n16.1\n17\n5.65\n\n\n16.2\n16\n2.59\n\n\n\n\n\n\n\n\nSuppose we want to use the teacher’s ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1)\nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n[1] 0.98356794 0.05294073\n\n\nWe could also have used built-in R functions to obtain these values:\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   12.1      0.984      12.3  0.000252   9.36      14.8  \n2 X[, 2]         0.243    0.0529      4.59 0.0101     0.0959     0.390\n\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n[1] 1.17680463 0.06527187\n\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\nTable 2: Coefficients and SEs from the OLS and WLS models.\n\n\n\n\nCoefficients and SEs from the OLS and WLS models.\n\n\n\n\n\n\n\n\n\n\n\nOLS\n\n\nWLS\n\n\n\nCoefficient\nB\nSE\nB\nSE\n\n\n\n\nIntercept\n12.0905\n0.9836\n13.4155\n1.1768\n\n\nEffect of Teacher ACT Score\n0.2429\n0.0529\n0.1658\n0.0653\n\n\n\n\n\n\n\n\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   13.4      1.18       11.4  0.000338  10.1       16.7  \n2 X[, 2]         0.166    0.0653      2.54 0.0639    -0.0154     0.347\n\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input."
  },
  {
    "objectID": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "href": "notes/04-01-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "title": "Tools for Dealing with Heteroskedasticity",
    "section": "What if Error Variances are Unknown?",
    "text": "What if Error Variances are Unknown?\nThe previous example assumed that the variance–covariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\n\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\n\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) |&gt;\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\nBefore examining any output from this model, let’s examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nFYI\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n# Examine residual plots\nresidual_plots(lm_step_5)\n\n\n\n\n\n\n\nFigure 8: Residual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)\n\n# A tibble: 5 × 7\n  term          estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     4.97    0.227        21.9  1.37e-100   4.52      5.41  \n2 age             0.0801  0.00591      13.5  6.95e- 41   0.0685    0.0917\n3 education      -0.201   0.0316       -6.36 2.30e- 10  -0.263    -0.139 \n4 male            2.24    0.166        13.5  1.56e- 40   1.92      2.57  \n5 age:education   0.0185  0.000921     20.1  1.77e- 85   0.0167    0.0203"
  },
  {
    "objectID": "notes/04-02-wls-and-sandwich-estimation.html",
    "href": "notes/04-02-wls-and-sandwich-estimation.html",
    "title": "Weighted Least Squares (WLS) and Sandwich Estimation",
    "section": "",
    "text": "In this set of notes, we will continue to use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID; available in slid.csv) to explain variation in the hourly wage rate of employed citizens in Ontario.\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(educate)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Import data\nslid = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/slid.csv\")\n\n# View data\nslid\n\n# A tibble: 3,997 × 4\n   wages   age education  male\n   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 10.6     40        15     1\n 2 11       19        13     1\n 3 17.8     46        14     1\n 4 14       50        16     0\n 5  8.2     31        15     1\n 6 17.0     30        13     0\n 7  6.7     61        12     0\n 8 14       46        14     0\n 9 19.2     43        18     1\n10  7.25    17        11     1\n# ℹ 3,987 more rows\n\n# Fit model\nlm.1 = lm(wages ~ 1 + age + education + male, data = slid)\nIn the previous set of notes, we found that the homoskedasticity assumption was likely violated; the plot of studentized residuals versus the fitted values showed severe fanning indicating that the variation in residuals seems to increase for higher fitted values. This assumption violation would produce incorrect sampling variances and covariances, and OLS estimates are no longer BLUE. Because of this, the SEs (and resulting t- and p-values) for the coefficients are incorrect.\nPreviously we tried to stabilize the variance by transforming the y-values. We found that even using the the optimal \\(\\lambda\\) value of 0.086 for the Box-Cox transformation the heterogenity issue was still problematic. There are two other solutions to this assumption violation when variance stabilizing transformations do not fix things:"
  },
  {
    "objectID": "notes/04-02-wls-and-sandwich-estimation.html#assume-error-variances-are-known",
    "href": "notes/04-02-wls-and-sandwich-estimation.html#assume-error-variances-are-known",
    "title": "Weighted Least Squares (WLS) and Sandwich Estimation",
    "section": "Assume Error Variances are Known",
    "text": "Assume Error Variances are Known\nLet’s assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance–covariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\n\n\n\nTable 1: Example data for weighted least squares.\n\n\n\n\nExample data for weighted least squares.\n\n\nClass Average ACT\nTeacher ACT\nClass SD\n\n\n\n\n17.3\n21\n5.99\n\n\n17.1\n20\n3.94\n\n\n16.4\n19\n1.90\n\n\n16.4\n18\n0.40\n\n\n16.1\n17\n5.65\n\n\n16.2\n16\n2.59\n\n\n\n\n\n\n\n\nSuppose we want to use the teacher’s ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1)\nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n[1] 0.98356794 0.05294073\n\n\nWe could also have used built-in R functions to obtain these values:\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   12.1      0.984      12.3  0.000252   9.36      14.8  \n2 X[, 2]         0.243    0.0529      4.59 0.0101     0.0959     0.390\n\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n[1] 1.17680463 0.06527187\n\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\nTable 2: Coefficients and SEs from the OLS and WLS models.\n\n\n\n\nCoefficients and SEs from the OLS and WLS models.\n\n\n\n\n\n\n\n\n\n\n\nOLS\n\n\nWLS\n\n\n\nCoefficient\nB\nSE\nB\nSE\n\n\n\n\nIntercept\n12.0905\n0.9836\n13.4155\n1.1768\n\n\nEffect of Teacher ACT Score\n0.2429\n0.0529\n0.1658\n0.0653\n\n\n\n\n\n\n\n\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   13.4      1.18       11.4  0.000338  10.1       16.7  \n2 X[, 2]         0.166    0.0653      2.54 0.0639    -0.0154     0.347\n\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input."
  },
  {
    "objectID": "notes/04-02-wls-and-sandwich-estimation.html#what-if-error-variances-are-unknown",
    "href": "notes/04-02-wls-and-sandwich-estimation.html#what-if-error-variances-are-unknown",
    "title": "Weighted Least Squares (WLS) and Sandwich Estimation",
    "section": "What if Error Variances are Unknown?",
    "text": "What if Error Variances are Unknown?\nThe previous example assumed that the variance–covariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\n\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\n\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) |&gt;\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\nBefore examining any output from this model, let’s examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nFYI\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n# Examine residual plots\nresidual_plots(lm_step_5)\n\n\n\n\n\n\n\nFigure 1: Residual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)\n\n# A tibble: 5 × 7\n  term          estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     4.97    0.227        21.9  1.37e-100   4.52      5.41  \n2 age             0.0801  0.00591      13.5  6.95e- 41   0.0685    0.0917\n3 education      -0.201   0.0316       -6.36 2.30e- 10  -0.263    -0.139 \n4 male            2.24    0.166        13.5  1.56e- 40   1.92      2.57  \n5 age:education   0.0185  0.000921     20.1  1.77e- 85   0.0167    0.0203"
  },
  {
    "objectID": "notes/04-01-heteroskedasticity-and-transformations.html",
    "href": "notes/04-01-heteroskedasticity-and-transformations.html",
    "title": "Heteroskedasticity and Variance Stabilizing Transformations",
    "section": "",
    "text": "In this set of notes, we will use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID; available in slid.csv) to explain variation in the hourly wage rate of employed citizens in Ontario."
  },
  {
    "objectID": "notes/04-01-heteroskedasticity-and-transformations.html#violating-homoskedasticity",
    "href": "notes/04-01-heteroskedasticity-and-transformations.html#violating-homoskedasticity",
    "title": "Heteroskedasticity and Variance Stabilizing Transformations",
    "section": "Violating Homoskedasticity",
    "text": "Violating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\n\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\n\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes)."
  },
  {
    "objectID": "notes/04-01-heteroskedasticity-and-transformations.html#box-cox-transformation",
    "href": "notes/04-01-heteroskedasticity-and-transformations.html#box-cox-transformation",
    "title": "Heteroskedasticity and Variance Stabilizing Transformations",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nIs there a power transformation that would better “fix” the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis transformation is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\).\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\nslid = slid |&gt;\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n# Examine residual plots\nresidual_plots(lm_bc)\n\n\n\n\n\n\n\nFigure 5: Residual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficient-level output:\n\n# Coeffifient-level output\ntidy(lm_bc, conf.int = TRUE)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   1.04    0.0475        21.9 1.80e-100   0.947     1.13  \n2 age           0.0227  0.000687      33.0 2.85e-211   0.0213    0.0240\n3 education     0.0707  0.00272       26.0 5.14e-138   0.0654    0.0760\n4 male          0.282   0.0164        17.2 4.99e- 64   0.250     0.315 \n\n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y, controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?"
  },
  {
    "objectID": "notes/04-01-heteroskedasticity-and-transformations.html#profile-plot-for-different-transformations",
    "href": "notes/04-01-heteroskedasticity-and-transformations.html#profile-plot-for-different-transformations",
    "title": "Heteroskedasticity and Variance Stabilizing Transformations",
    "section": "Profile Plot for Different Transformations",
    "text": "Profile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\n\n\n\nFigure 6: Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n# Zoom in on confidence limits\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\n\n\n\nFigure 7: Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html",
    "href": "notes/05-01-diagnosing-collinearity.html",
    "title": "Diagnosing Collinearity",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to empirical diagnostics to detect collinearity. We will use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(educate)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/equal-education-opportunity.csv\")\n\n# View data\neeo\n\n# A tibble: 70 × 4\n   achievement faculty    peer  school\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      -0.431   0.608  0.0351  0.166 \n 2       0.800   0.794  0.479   0.534 \n 3      -0.925  -0.826 -0.620  -0.786 \n 4      -2.19   -1.25  -1.22   -1.04  \n 5      -2.85    0.174 -0.185   0.142 \n 6      -0.662   0.202  0.128   0.273 \n 7       2.64    0.242 -0.0902  0.0497\n 8       2.36    0.594  0.218   0.519 \n 9      -0.913  -0.616 -0.490  -0.632 \n10       0.594   0.994  0.622   0.934 \n# ℹ 60 more rows"
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#effects-of-collinearity",
    "href": "notes/05-01-diagnosing-collinearity.html#effects-of-collinearity",
    "title": "Diagnosing Collinearity",
    "section": "Effects of Collinearity",
    "text": "Effects of Collinearity\nIf the design matrix is not of full rank, and \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nwhere\n\n\\(R^2_j\\) is the squared multiple correlation for the regression of \\(X_j\\) on the the other predictors;\n\\(S^2_j\\) is the sample variance of predictor \\(X_j\\) defined by \\(S^2_j = \\dfrac{\\sum(X_{ij}-\\bar{X}_j)^2}{n-1}\\);\n\\(\\sigma^2_{\\epsilon}\\) is the variance of the residuals based on regressing \\(Y\\) on all the \\(X\\)’s\n\n\nFYI\nRecall that the multiple correlation is the correlation between the outcome and the predicted values.\n\nThe first term in this product is referred to as the variance inflation factor (VIF). When one of the predictors is perfectly collinear with the others, the value of \\(R^2_j\\) is 1 and the VIF is infinity. Thus the sampling variance of \\(B_j=\\infty\\)."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#perfect-collinearity-in-practice-model-mis-specification",
    "href": "notes/05-01-diagnosing-collinearity.html#perfect-collinearity-in-practice-model-mis-specification",
    "title": "Diagnosing Collinearity",
    "section": "Perfect Collinearity in Practice: Model Mis-specification",
    "text": "Perfect Collinearity in Practice: Model Mis-specification\nIn practice, it is unlikely that you will have exact (or perfect) collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.\n\n# Create design matrix\nX = data.frame(\n  b_0 = rep(1, 5),\n  employed = c(1, 1, 0, 0, 1),\n  not_employed = c(0, 0, 1, 1, 0)\n)\n\n# View design matrix\nX\n\n  b_0 employed not_employed\n1   1        1            0\n2   1        1            0\n3   1        0            1\n4   1        0            1\n5   1        1            0\n\n\nThe columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,\n\\[\nb_0 = 1(\\mathrm{employed}) + 1(\\mathrm{not~employed})\n\\]\nChecking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, X is not full column rank; it is rank deficient.\n\nMatrix::rankMatrix(X)\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 1.110223e-15\n\n\nIncluding all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.\n\nFYI\nIncluding the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.\n\n\n# Create vector of outcomes\nY = c(15, 15, 10, 15, 30)\n\n# Create data frame of Y and X\nmy_data = cbind(Y, X)\nmy_data\n\n   Y b_0 employed not_employed\n1 15   1        1            0\n2 15   1        1            0\n3 10   1        0            1\n4 15   1        0            1\n5 30   1        1            0\n\n# Coefficients (including all three terms)\ncoef(lm(Y ~ 1 + employed + not_employed, data = my_data))\n\n (Intercept)     employed not_employed \n        12.5          7.5           NA \n\n# Coefficients (omitting intercept)\ncoef(lm(Y ~ -1 + employed + not_employed, data = my_data))\n\n    employed not_employed \n        20.0         12.5 \n\n\nIf you overparameterize a model with lm(), one or more of the coefficients will not be estimated (the last parameters entered in the model).\n\nFYI\nConstraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#non-exact-collinearity",
    "href": "notes/05-01-diagnosing-collinearity.html#non-exact-collinearity",
    "title": "Diagnosing Collinearity",
    "section": "Non-Exact Collinearity",
    "text": "Non-Exact Collinearity\nIt is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients’ sampling variances. Again, we look toward how the sampling variances for the coefficent’s are computed:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nWhen the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between \\(X_j\\) and the other \\(X\\)s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model’s residual variance, sample size, and the predictor’s variance—the factors we typically think of affecting the sampling variance of a coefficient.\nIn cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between \\(X_j\\) and the other \\(X\\)s is larger than 0. (Perfect collinearity results in \\(R^2_j=1\\).) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).\nHow much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations (\\(R_j\\)) between \\(X_j\\) and the predicted values from using the other \\(X\\)’s to predict \\(X_j\\).\n\n\n\n\nTable 1: Impact of various \\(R_j\\) values on the VIF and size of the CI for \\(B_j\\).\n\n\n\n\nImpact of various Rj values on the VIF and size of the CI for Bj.\n\n\nRj\nVIF\nCI Factor\n\n\n\n\n0.0\n1.00\n1.00\n\n\n0.1\n1.01\n1.01\n\n\n0.2\n1.04\n1.02\n\n\n0.3\n1.10\n1.05\n\n\n0.4\n1.19\n1.09\n\n\n0.5\n1.33\n1.15\n\n\n0.6\n1.56\n1.25\n\n\n0.7\n1.96\n1.40\n\n\n0.8\n2.78\n1.67\n\n\n0.9\n5.26\n2.29\n\n\n1.0\nInf\nInf\n\n\n\n\n\n\n\n\nFor example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of \\(B_j\\).\nTo sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#collinearity-diagnostics",
    "href": "notes/05-01-diagnosing-collinearity.html#collinearity-diagnostics",
    "title": "Diagnosing Collinearity",
    "section": "Collinearity Diagnostics",
    "text": "Collinearity Diagnostics\nWe can also empirically diagnose problematic collinearity in the data (D. A. Belsley, 1991; D. Belsley, Kuh, & Welsch, 1980). Before we do, however, it is important that the functional form of the model has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated after the model has been satisfactorily specified.\nBelow we will explore some of the diagnostic tools available to an applied researcher."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#high-correlations-among-predictors",
    "href": "notes/05-01-diagnosing-collinearity.html#high-correlations-among-predictors",
    "title": "Diagnosing Collinearity",
    "section": "High Correlations among Predictors",
    "text": "High Correlations among Predictors\nCollinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.\n\n# Correlation between predictors\neeo |&gt;\n  select(faculty, peer, school) |&gt;\n  correlate()\n\n# A tibble: 3 × 4\n  term    faculty   peer school\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 faculty  NA      0.960  0.986\n2 peer      0.960 NA      0.982\n3 school    0.986  0.982 NA    \n\n\nIn this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step)."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#regress-each-predictor-on-the-other-predictors",
    "href": "notes/05-01-diagnosing-collinearity.html#regress-each-predictor-on-the-other-predictors",
    "title": "Diagnosing Collinearity",
    "section": "Regress each Predictor on the Other Predictors",
    "text": "Regress each Predictor on the Other Predictors\nSince collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the \\(R^2\\) value. If all the \\(R^2\\) values are close to zero there is no collinearity problems. If one or more of the \\(R^2\\) values are close to 1, there is a collinearity problem.\n\n# Use faculty as outcome; obtain R2\nsummary(lm(faculty ~ 1 + peer + school, data = eeo))$r.squared\n\n[1] 0.9733906\n\n# Use faculty as outcome; obtain R2\nsummary(lm(peer ~ 1 + faculty + school, data = eeo))$r.squared\n\n[1] 0.9669002\n\n# Use faculty as outcome; obtain R2\nsummary(lm(school ~ 1 + faculty + peer, data = eeo))$r.squared\n\n[1] 0.9879743\n\n\nAll three \\(R^2\\) values are quite high, which is indicative of collinearity.\nOne shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the \\(R^2\\) values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.\nWe will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#variance-inflation-factor-vif",
    "href": "notes/05-01-diagnosing-collinearity.html#variance-inflation-factor-vif",
    "title": "Diagnosing Collinearity",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\nPerhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:\n\\[\n\\mathrm{VIF} = \\frac{1}{1 - R^2_j}\n\\]\nThe VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the vif() function from the car package to compute the variance inflation factors for each coefficient.\n\n# VIF\nvif(lm.1)\n\n faculty     peer   school \n37.58064 30.21166 83.15544 \n\n# Square root of VIF\nsqrt(vif(lm.1))\n\n faculty     peer   school \n6.130305 5.496513 9.118960 \n\n\nThe variances (and hence, the standard errors) for all three coefficients are inflated because of collinearity. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.\nRemember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite \\(\\mathrm{VIF}&gt;10\\) as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite \\(\\mathrm{VIF}&gt;4\\) as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.\nThere are several situations in which high VIF values are expected and not problematic:\n\nThe variables with high VIFs are control variables, and the variables of interest do not have high VIFs. Since we would not be interested in inference around the control variables, high VIF values on those variables would not\nThe high VIFs are caused by the inclusion of powers or products of other variables. The p-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the p-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.\nThe variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. This is especially true when the reference category used has a small proportion of cases. In this case, p-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#eigenvalues-of-the-correlation-matrix",
    "href": "notes/05-01-diagnosing-collinearity.html#eigenvalues-of-the-correlation-matrix",
    "title": "Diagnosing Collinearity",
    "section": "Eigenvalues of the Correlation Matrix",
    "text": "Eigenvalues of the Correlation Matrix\nA second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square (\\(k \\times k\\)) matrix has a set of k scalars, called eigenvalues (denoted \\(\\lambda\\)) associated with it. These eigenvalues can be arranged in descending order such that,\n\\[\n\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_k\n\\]\nBecause any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix.\nAs a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:\n\\[\n\\sum_{i=1}^k \\frac{1}{\\lambda_i}\n\\]\n\nIf the predictors are orthogonal to one another (independent) then \\(\\lambda_i = 1\\) and the sum of the reciprocal values will be equal to the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = k\\).\nIf the predictors are collinear with one another (dependent) then \\(\\lambda_i = 0\\) and the sum of the reciprocal values will be equal to infinity, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = \\infty\\).\nWhen there is nonperfect collinearity then \\(0 &lt; \\lambda_i &lt; 1\\), and the sum of the reciprocal values will be greater than the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} &gt; k\\).\n\n\nMATH NOTE\nIn an orthogonal matrix, the eigenvalues are all \\(\\pm1\\), but since the correlation matrix is positive semidefinite, the eigenvalues are all \\(+1\\).\n\nLarger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.\n\\[\n\\mathrm{IF} \\quad \\sum_{i=1}^k \\frac{1}{\\lambda_i} &gt; 5k \\quad \\mathrm{THEN} \\quad \\mathrm{collnearity~is~a~problem}\n\\]\n\n\nFYI\nIn practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.\n\n\n\nUsing R to Compute the Eigenvalues of the Correlation Matrix\nBecause collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the eigen() function to compute the eigenvalues of a square matrix.\nIn previous classes, I have been using the correlate() function from the {corrr} package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the cor() function, which produces a matrix object, to produce the correlation matrix.\n\n# Correlation matrix of predictors\nr_xx = cor(eeo[c(\"faculty\", \"peer\", \"school\")])\nr_xx\n\n          faculty      peer    school\nfaculty 1.0000000 0.9600806 0.9856837\npeer    0.9600806 1.0000000 0.9821601\nschool  0.9856837 0.9821601 1.0000000\n\n\nOnce we have the correlation matrix, we can use the eigen() function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.\n\n# Compute eigenvalues and eigenvectors\neigen(r_xx)\n\neigen() decomposition\n$values\n[1] 2.951993158 0.040047507 0.007959335\n\n$vectors\n           [,1]        [,2]       [,3]\n[1,] -0.5761385  0.67939712 -0.4544052\n[2,] -0.5754361 -0.73197527 -0.3648089\n[3,] -0.5804634  0.05130072  0.8126687\n\n# Sum of reciprocal of eigenvalues\nsum(1 / eigen(r_xx)$values)\n\n[1] 150.9477\n\n\nWe compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; \\(5 \\times 3 =15\\). Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model."
  },
  {
    "objectID": "notes/05-01-diagnosing-collinearity.html#condition-indices",
    "href": "notes/05-01-diagnosing-collinearity.html#condition-indices",
    "title": "Diagnosing Collinearity",
    "section": "Condition Indices",
    "text": "Condition Indices\nA condition number for a matrix and computational task quantifies how sensitive the result is to perturbations in the input data and to roundoff errors made during the solution process. If minor changes to the matrix elements result in large differences in the computation (say of the inverse), we say that the matrix is “ill-conditioned”. It is important to note that a condition number applies not only to a particular matrix, but also to the particular computation being carried out. That is, a matrix can be ill-conditioned for inversion while the eigenvalue problem is well-conditioned.\nA third common diagnostic measure of collinearity is to compute the condition number of the correlation matrix of the model predictors. This tells us whether small changes in the data will lead to large changes in regression coefficient estimates. To compute the condition number, we need to compute the condition index for each of the eigenvalues of the correlation matrix based on the model predictors. Each eigenvalue has an associated condition index, and the jth eigenvalue’s condition index is denoted \\(\\kappa_j\\), where,\n\\[\n\\kappa_j = \\sqrt{\\frac{\\lambda_{\\mathrm{Max}}}{\\lambda_j}}\n\\]\nand \\(\\lambda_{\\mathrm{Max}}\\) is the largest eigenvalue. The largest eigenvalue will have a condition index of,\n\\[\n\\begin{split}\n\\kappa_{\\lambda_\\mathrm{Max}} &= \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Max}}} \\\\[1ex]\n&= 1\n\\end{split}\n\\]\nThe smallest eigenvalue will have a condition index of\n\\[\n\\kappa_{\\lambda_\\mathrm{Min}} = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}}\n\\]\nThis value will be larger than 1 since \\(\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}\\) will be greater than 1. In general, the largest eigenvalue will have a condition index of 1 and the other condition indices for every other eigenvalue will be larger than one.\nThe condition number of the correlation matrix is equivalent to the condition index for the smallest eigenvalue, that is,\n\\[\n\\kappa = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}}\n\\]\nIf the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.\nFrom empirical work, a condition number between 10 and 30 indicates the presence of multicollinearity. When the condition number is larger than 30, the multicollinearity is regarded as strong and corrective action will almost surely need to be taken. Below we compute the condition indices and the condition number for our empirical example.\n\n# Sort eigenvalues from largest to smallest\nlambda = sort(eigen(r_xx)$values, decreasing = TRUE)\n\n# View eigenvalues\nlambda\n\n[1] 2.951993158 0.040047507 0.007959335\n\n# Compute condition indices\nsqrt(max(lambda) / lambda)\n\n[1]  1.000000  8.585586 19.258359\n\n# Compute condition number directly\nsqrt(max(lambda) / min(lambda))\n\n[1] 19.25836\n\n\nThe condition number of the correlation matrix, \\(\\kappa = 19.26\\), suggests there is collinearity among the predictors."
  },
  {
    "objectID": "notes/05-02-pca-via-spectral-decomposition.html",
    "href": "notes/05-02-pca-via-spectral-decomposition.html",
    "title": "Principal Components Analysis via Spectral Decomposition",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to principal components analysis via spectral decomposition. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n# A tibble: 6 × 4\n  achievement faculty    peer school\n        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\nThe problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. One method to deal with collinearity among a set of predictors is to combine the predictors into a smaller subset of orthogonal measures (called principal components) that can be used instead of the original predictors. This subset of measures will not have the collinearity problems (they are orthogonal to one another), but constitute a slightly smaller amount of “variance accounted for” than the original set of predictors."
  },
  {
    "objectID": "notes/05-02-pca-via-spectral-decomposition.html#matrix-algebra-to-carry-out-the-pca-using-spectral-decomposition",
    "href": "notes/05-02-pca-via-spectral-decomposition.html#matrix-algebra-to-carry-out-the-pca-using-spectral-decomposition",
    "title": "Principal Components Analysis via Spectral Decomposition",
    "section": "Matrix Algebra to Carry Out the PCA using Spectral Decomposition",
    "text": "Matrix Algebra to Carry Out the PCA using Spectral Decomposition\nTo carry out the spectral decomposition in R, we need to create the matrix of the predictors (\\(\\mathbf{X}_p\\)), compute the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, and then use the eigen() function to carry out the spectral decomposition.\n\n# Create predictor matrix\nX_p = eeo |&gt;\n  select(peer, faculty) |&gt;\n  data.matrix()\n\n# Spectral decomposition\nspec_decomp = eigen(t(X_p) %*% X_p)\nspec_decomp\n\neigen() decomposition\n$values\n[1] 137.561000   2.724415\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6469680 -0.7625172\n[2,] 0.7625172  0.6469680\n\n\nThe matrix of eigenvectors, in the $vectors component, compose the P matrix and make up the set of basis vectors for the rotated predictor space. The elements in the $values component are the diagonal elements in D and are the eigenvalues. The decomposition is:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_p\\mathbf{X}_p &= \\mathbf{PDP}^\\intercal \\\\[1em]\n\\begin{bmatrix}59.16 & 66.52 \\\\ 66.62 & 81.12\\end{bmatrix} &= \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix} \\begin{bmatrix}137.561 & 0  \\\\ 0 & 2.724 \\end{bmatrix} \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix}^\\intercal\n\\end{split}\n\\]\nThe span of the basis vectors define the principal components, with the first eigenvector defining the first principal component and the second eigenvector defining the second principal component. (Note: There number of principal components will always be the same as the number of predictors in the \\(\\mathbf{X}_p\\) matrix.)\nWe can post-multiply the matrix of the original predictor values (in \\(\\mathbf{X}_p\\)) by this matrix of basis vectors to obtain the predictor values in the rotated space.\n\n# Matrix of basis vectors for rotated predictor space\nrot_basis = spec_decomp$vectors\n\n# Compute rotated values under the new basis\nrot_pred = X_p %*% rot_basis\nhead(rot_pred)\n\n            [,1]        [,2]\n[1,]  0.48641930  0.36669037\n[2,]  0.91525519  0.14806327\n[3,] -1.03087107 -0.06220262\n[4,] -1.74270855  0.11707722\n[5,]  0.01287131  0.25376126\n[6,]  0.23695822  0.03365744\n\n\nFor example, the first observation, has a value of 0.486 on the first principal component and a value of 0.367 on the second principal component. Similarly, the second observation has a value of 0.915 on the first principal component and a value of 0.148 on the second principal component. Each observation has a set of values on the principal components.\nThe eigenvalues are related to the variances of the principal components. Because we decomposed the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, the variance on each prinicpal component can be computed as:\n\\[\n\\mathrm{Var}(\\mathrm{PC}_i) = \\frac{\\lambda_i}{n-1}\n\\]\nIn our example, the variances can be computed as:\n\n# Compute variances of PCs\nvar_pc = spec_decomp$values / (70 - 1)\nvar_pc\n\n[1] 1.99363769 0.03948427\n\n\nThe first principal component has the largest variance, which will always be the case. Remember, the principal components are selected so the first component maximizes the variation in the predictor space, the second component will maximize the remaining variance (and be orthogonal to the first), etc.\nWe can use these variances to determine the proportion of variation in the predictor space that each principal component accounts for. This is often more useful to the applied data analyst than the actual variance measure itself. Since the principal components are orthogonal, we can sum the variances to obtain a total measure of variation in the original set of predictors accounted for by the principal components. Below, we compute the proportion of variance in the predictor space that each principal component in our example accounts for:\n\n# Compute proportion of variation\nvar_pc / sum(var_pc)\n\n[1] 0.98057949 0.01942051\n\n\nHere the first principal component accounts for 98.1% of the variation in the predictor space, and the second principal component accounts for the remaining 1.9% of the variation."
  },
  {
    "objectID": "notes/05-02-pca-via-spectral-decomposition.html#using-princomp-to-obtain-the-principal-components",
    "href": "notes/05-02-pca-via-spectral-decomposition.html#using-princomp-to-obtain-the-principal-components",
    "title": "Principal Components Analysis via Spectral Decomposition",
    "section": "Using princomp() to Obtain the Principal Components",
    "text": "Using princomp() to Obtain the Principal Components\nWe can also use the R function princomp() to obtain the principal components based on the spectral decomposition. We provide this function with a data frame of the predictors.\n\n# Select predictors\neeo_pred = eeo |&gt;\n  select(faculty, peer)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE)\n\nImportance of components:\n                          Comp.1     Comp.2\nStandard deviation     1.4002088 0.19725327\nProportion of Variance 0.9805406 0.01945935\nCumulative Proportion  0.9805406 1.00000000\n\nLoadings:\n        Comp.1 Comp.2\nfaculty  0.763  0.647\npeer     0.647 -0.763\n\n\nThe values of the principal components, the rotated set of basis vectors, are given in the loadings output. Note that the signs of the principal components are arbitrary. For example, the vector associated with the first principal component could also have been \\((-0.763, -0.647)\\) and that for the second principal component could have been \\((-0.647, 0.763)\\). The variances of each component can be computed by squaring the appropriate standard deviations in the output.\n\n# Compute variance of PC1\n1.4002088 ^ 2\n\n[1] 1.960585\n\n# Compute variance of PC2\n0.19725327 ^ 2\n\n[1] 0.03890885\n\n\nWe can use the variance measures to obtain a total measure of variation in the original set of predictors accounted for by each of the principal components.\n\n# Compute total variation accounted for\ntotal_var = 1.4002088 ^ 2 + 0.19725327 ^ 2\ntotal_var\n\n[1] 1.999494\n\n# Compute variation accounted for by PC1\n(1.4002088 ^ 2) / total_var\n\n[1] 0.9805406\n\n# Compute variation accounted for by PC2\n(0.19725327 ^ 2) / total_var\n\n[1] 0.01945935\n\n\nThis suggests that the first principal component accounts for 98% of the variance in the original set of predictors and that the second principal component accounts for 2% of the variance. (Same as we computed in the matrix algebra.) Note that these values are also given in the summary() output.\nWe can also obtain the principal component scores (the values under the rotation) for each observation by accessing the scores element of the princomp object. (Below we only show the first six scores.)\n\n# Get PC scores\npc_scores = my_pca$scores\n\n# View PC scores\nhead(pc_scores)\n\n          Comp.1      Comp.2\n[1,]  0.41884376  0.37000669\n[2,]  0.84765375  0.15132881\n[3,] -1.09849740 -0.05870659\n[4,] -1.81031365  0.12065755\n[5,] -0.05471761  0.25713367\n[6,]  0.16934323  0.03700331\n\n\nThese are slightly different than the scores we obtained by multiplying the original predictor values by the new basis matrix. For example, the PC scores for the first observation were \\(-0.486\\) and \\(0.367\\). The princomp() function mean centers each variable prior to multiplying by the basis matrix.\n\n# Mimic scores from princomp()\nold = t(c(0.608 - mean(eeo$faculty), 0.0351 - mean(eeo$peer)))\n\n# Compute PC scores\nold %*% basis\n\n          [,1]      [,2]\n[1,] 0.4186997 0.3699581\n\n\n\nFYI\nBecause we want the principal components to be solely functions of the predictors, we mean center them. Otherwise we would have to include a column of ones (intercept) in the \\(\\mathbf{X}_p\\) matrix. Mean centering the predictors makes each predictor orthogonal to the intercept, in which case it can be ignored. (This is similar to how mean centering predictors removes the intercept from the fitted equation.)\n\nSince we only have two principal components, we can visualize the scores using a scatterplot.\n\n# Create data frame of scores\npc_scores = pc_scores |&gt;\n  data.frame()\n\n# Plot the scores\nggplot(data = pc_scores, aes(x = Comp.1, y = Comp.2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"lightgrey\") +\n  geom_vline(xintercept = 0, color = \"lightgrey\") +\n  scale_x_continuous(name = \"Principal Component 1\", limits = c(-4, 4)) +\n  scale_y_continuous(name = \"Principal Component 2\", limits = c(-4, 4)) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\nFigure 3: Rotated predictor space using the principal components as the new basis.\n\n\n\n\n\nConceptually this visualization shows the rotated predictor space after re-orienting the rotated coordinate system. From this visualization, it is also clear that there is much more variation in the values of the first principal component than in the second principal component."
  },
  {
    "objectID": "notes/05-03-pca-via-svd.html",
    "href": "notes/05-03-pca-via-svd.html",
    "title": "Principal Components Analysis via Singular Value Decomposition",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to principal components analysis via singular value decomposition. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n# Load libraries\nlibrary(broom)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/equal-education-opportunity.csv\")\n\n# View data\neeo\n\n# A tibble: 70 × 4\n   achievement faculty    peer  school\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      -0.431   0.608  0.0351  0.166 \n 2       0.800   0.794  0.479   0.534 \n 3      -0.925  -0.826 -0.620  -0.786 \n 4      -2.19   -1.25  -1.22   -1.04  \n 5      -2.85    0.174 -0.185   0.142 \n 6      -0.662   0.202  0.128   0.273 \n 7       2.64    0.242 -0.0902  0.0497\n 8       2.36    0.594  0.218   0.519 \n 9      -0.913  -0.616 -0.490  -0.632 \n10       0.594   0.994  0.622   0.934 \n# ℹ 60 more rows"
  },
  {
    "objectID": "notes/05-03-pca-via-svd.html#pca-using-svd-matrix-algebra",
    "href": "notes/05-03-pca-via-svd.html#pca-using-svd-matrix-algebra",
    "title": "Principal Components Analysis via Singular Value Decomposition",
    "section": "PCA using SVD: Matrix Algebra",
    "text": "PCA using SVD: Matrix Algebra\nTo carry out a principal components analysis, we need to use SVD to decompose the matrix \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\), where \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) is a matrix of the predictors being used in the PCA. Below, we create the \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix we use the svd() function to decompose the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix using singular value decomposition.\n\n# Create matrix of predictors\nX_p = as.matrix(eeo[ , c(\"faculty\", \"peer\", \"school\")])\n\n# SVD decomposition\nsv_decomp = svd(t(X_p) %*% X_p)\n\n# View results\nsv_decomp\n\n$d\n[1] 209.3295610   2.7303093   0.5833274\n\n$u\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n$v\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[1em]\n\\begin{bmatrix}81.123 & 66.518 & 75.512 \\\\ 66.518 & 59.163 & 64.251 \\\\ 75.512 & 64.251 & 72.358\\end{bmatrix} &= \\begin{bmatrix}0.617 & 0.670 & -0.412 \\\\ -0.524 & -0.741 & -0.419 \\\\ -0.586 & -0.042 & 0.809\\end{bmatrix} \\begin{bmatrix}209.330 & 0 & 0 \\\\ 0 & 2.730 & 0 \\\\ 0 & 0 & 0.583 \\end{bmatrix} \\begin{bmatrix}-0.617 & -0.524 & -0.586 \\\\ 0.670 & -0.741 & -0.042 \\\\ -0.412 & -0.419 &  0.809 \\end{bmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/05-03-pca-via-svd.html#understanding-what-the-matrix-algebra-is-doing",
    "href": "notes/05-03-pca-via-svd.html#understanding-what-the-matrix-algebra-is-doing",
    "title": "Principal Components Analysis via Singular Value Decomposition",
    "section": "Understanding What the Matrix Algebra is Doing",
    "text": "Understanding What the Matrix Algebra is Doing\nMathematically, since any matrix can be decomposed using SVD, we can also decompose \\(\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\\). Then we can write the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix, \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), as:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})^{\\intercal} (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})\n\\]\nRe-expressing this we get:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[0.5em]\n\\end{split}\n\\]\nSince D is a diagonal matrix, \\(\\mathbf{D}^{\\intercal}\\mathbf{D} = \\mathbf{D}^2\\), so reducing this expression gives:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^{\\intercal}\n\\]\nThe matrices V and \\(\\mathbf{V}^{\\intercal}\\) are both orthogonal basis matrices that ultimately act to change the coordinate system by rotating the original basis vectors used in the predictor space. The \\(\\mathbf{D}^2\\) matrix is diagonalizing the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix which amounts to finding the the major axes in the data ellipse along which our data varies."
  },
  {
    "objectID": "notes/05-03-pca-via-svd.html#scaling-the-predictors",
    "href": "notes/05-03-pca-via-svd.html#scaling-the-predictors",
    "title": "Principal Components Analysis via Singular Value Decomposition",
    "section": "Scaling the Predictors",
    "text": "Scaling the Predictors\nIn practice, it is important to scale all the predictors used in the PCA. This is especially true when the variables are measured in different metrics or have varying degrees of magnitude. In these cases, not scaling the predictors will often result in results in which variables with large magnitudes of scale dominate the PCA. It also is helpful when the predictors are measured using qualitatively different scales (e.g., one is measured in dollars and another in years of education).\n\nFYI\nRecall, the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix for a set of standardized predictors his the correlation matrix of the predictors. Thus, the SVD is actually being carried out on the correlation matrix rather than the raw \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix.\n\nWhile it isn’t necessary to also center the predictors, it is common to simply standardize the set of predictors being used in the PCA (i.e., convert them to z-scores); thus both centering and scaling them. To do this, we will pipe the selected predictors into the scale() function prior to piping into the prcomp() function.\n\n# Fit the PCA using SVD decomposition on standardized predictors\nsvd_pca_z = eeo |&gt;\n  select(faculty, peer, school) |&gt;\n  scale(center = TRUE, scale = TRUE) |&gt;\n  prcomp()\n\n# View standard deviations and rotation matrix (eigenvector matrix)\nsvd_pca_z\n\nStandard deviations (1, .., p=3):\n[1] 1.71813654 0.20011873 0.08921511\n\nRotation (n x k) = (3 x 3):\n               PC1         PC2        PC3\nfaculty -0.5761385  0.67939712 -0.4544052\npeer    -0.5754361 -0.73197527 -0.3648089\nschool  -0.5804634  0.05130072  0.8126687\n\n# tidy version of the rotation matrix (good for graphing)\nsvd_pca_z |&gt;\n  tidy(matrix = \"rotation\")\n\n# A tibble: 9 × 3\n  column     PC   value\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 faculty     1 -0.576 \n2 faculty     2  0.679 \n3 faculty     3 -0.454 \n4 peer        1 -0.575 \n5 peer        2 -0.732 \n6 peer        3 -0.365 \n7 school      1 -0.580 \n8 school      2  0.0513\n9 school      3  0.813 \n\n# View sds, variance accounted for\nsvd_pca_z |&gt;\n  tidy(matrix = \"eigenvalues\")\n\n# A tibble: 3 × 4\n     PC std.dev percent cumulative\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     1  1.72   0.984        0.984\n2     2  0.200  0.0134       0.997\n3     3  0.0892 0.00265      1    \n\n# Obtain PC scores\npc_scores = augment(svd_pca_z)\npc_scores\n\n# A tibble: 70 × 4\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3\n   &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 1            -0.366      0.366     -0.123 \n 2 2            -0.950      0.149     -0.0847\n 3 3             1.34      -0.0633    -0.0197\n 4 4             2.09       0.129      0.193 \n 5 5             0.0152     0.267      0.127 \n 6 6            -0.269      0.0437     0.0952\n 7 7            -0.0275     0.230     -0.0128\n 8 8            -0.672      0.231      0.0905\n 9 9             1.06      -0.0261    -0.0369\n10 10           -1.37       0.182      0.0925\n# ℹ 60 more rows\n\n\nHere the results from using the standardized predictors are not that different from the previous results since the variables were already reported as z-scores to begin with. The variance accounted for by the principal components is comparable (within rounding); the standardization of the predictors does not change this. The actual principal components in the V (rotation) matrix are different because of the centering and scaling, but the interpretations are the same as when we used the decomposition based on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. Similarly, because of the centering and scaling, the PC scores are different."
  },
  {
    "objectID": "notes/05-03-pca-via-svd.html#behind-the-scenes",
    "href": "notes/05-03-pca-via-svd.html#behind-the-scenes",
    "title": "Principal Components Analysis via Singular Value Decomposition",
    "section": "Behind the Scenes",
    "text": "Behind the Scenes\nBy standardizing the predictors, we are carrying out the SVD on the correlation matrix of the predictors rather than on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. To see this, recall that the correlation matrix of the predictors (\\(\\mathbf{R_X}\\)) is based on the standardized predictors, namely,\n\nFTI\nThis implies that you can also carry out PCA on summaries of the predictors (rather than the raw data) by decomposing either the covariance matrix of the predictors or the correlation matrix of the predictors. This can be useful for example, when trying to reproduce the results of a PCA from a published paper, in which authors will often report summaries of the data used (e.g., correlation matrices) but not the raw data.\n\n\\[\n\\mathbf{R_X} = \\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big)\n\\]\nThe \\(1/(n-1)\\) component is a scalar and is pulled out so that the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\) matrix:\n\\[\n\\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big) = \\frac{1}{n-1}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\n\\]\nSimilarly, we can also decompose the covariance matrix of the predictors (\\(\\boldsymbol\\Sigma_{\\mathbf{X}}\\)), which is based on the centered predictors,\n\\[\n\\boldsymbol\\Sigma_{\\mathbf{X}} = \\frac{1}{n-1} \\big( \\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\big)\n\\]\nIn this case, the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\) matrix. To do this using prcomp() we would change the scale= argument to FALSE in the scale() function, while still leaving center=TRUE."
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html",
    "href": "notes/05-04-biased-estimation-ridge-regression.html",
    "title": "Biased Estimation: Ridge Regression",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to ridge regression. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n# Load libraries\nlibrary(broom)\nlibrary(MASS)\nlibrary(tidyverse)\n\n# Read in data\neeo = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/equal-education-opportunity.csv\")\n\n# View data\neeo\n\n# A tibble: 70 × 4\n   achievement faculty    peer  school\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1      -0.431   0.608  0.0351  0.166 \n 2       0.800   0.794  0.479   0.534 \n 3      -0.925  -0.826 -0.620  -0.786 \n 4      -2.19   -1.25  -1.22   -1.04  \n 5      -2.85    0.174 -0.185   0.142 \n 6      -0.662   0.202  0.128   0.273 \n 7       2.64    0.242 -0.0902  0.0497\n 8       2.36    0.594  0.218   0.519 \n 9      -0.913  -0.616 -0.490  -0.632 \n10       0.594   0.994  0.622   0.934 \n# ℹ 60 more rows"
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#matrix-formulation-of-ridge-regression",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#matrix-formulation-of-ridge-regression",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Matrix Formulation of Ridge Regression",
    "text": "Matrix Formulation of Ridge Regression\nRecall that the OLS estimates are given by:\n\\[\n\\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nUnder collinearity, the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix is ill-conditioned. (A matrix is said to be ill-conditioned if it has a high condition number, meaning that small changes in the data impact the regression results. This ill-conditioning results in inaccuracy when we compute the inverse of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix, which translates into bad estimates of the coefficients and standard errors.\nTo see this, consider our EEO example data. We first standardize the variables (so we can omit the ones column in the design matrix) using the scale() function. Note that the output of the scale() function is a matrix. Then, we will select the predictors using indexing and compute the condition number for the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix.\n\n# Standardize all variables in the eeo data frame\nz_eeo = eeo |&gt; \n  scale()\n\n# Create and view the design matrix\nX = z_eeo[ , c(\"faculty\", \"peer\", \"school\")]\nhead(X)\n\n        faculty        peer     school\n[1,]  0.5158617 -0.01213684  0.1310782\n[2,]  0.6871673  0.46812962  0.4901170\n[3,] -0.8084583 -0.71996624 -0.7994390\n[4,] -1.2024935 -1.36577136 -1.0479983\n[5,]  0.1150408 -0.25030749  0.1078451\n[6,]  0.1413252  0.08793894  0.2356566\n\n# Get eigenvalues\neig_val = eigen(t(X) %*% X)$values\n\n# Compute condition number\nsqrt(max(eig_val) / min(eig_val))\n\n[1] 19.25836\n\n\nWe can inflate the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) to better condition the matrix. This, hopefully, leads to more stability in the inverse matrix and produces better (albeit biased) coefficient estimates. To do this, we can add some constant amount (\\(\\lambda\\)) to each of the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), prior to finding the inverse. This can be expressed as:\n\\[\n\\widetilde{\\mathbf{b}} = (\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{Y}\n\\]\nwhere the tilde over b indicates that the coefficients are biased. To see how increasing the diagonal values leads to better conditioning, we will add some value (here \\(\\lambda=10\\), but it could be any value between 0 and positive infinity) to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix in our example.\n\nFYI\nTechnically this equation is for standardized variables; it assumes that there is no ones column in the X matrix. This is because we only want to add the \\(\\lambda\\) value to the parts of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix associated with the predictors.\n\n\n# Add 50 to each of the diagonal elements of X^T(X)\ninflated = t(X) %*% X + 10*diag(3)\n\n# Get eigenvalues\neig_val_inflated = eigen(inflated)$values\n\n# Compute condition number\nsqrt(max(eig_val_inflated) / min(eig_val_inflated))\n\n[1] 4.500699\n\n\nThe condition number has decreased from 19.26 (problematic collinearity) to 4.5 (non-collinear). Adding 10 to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix resulted in a better conditioned matrix!"
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#using-an-built-in-r-function",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#using-an-built-in-r-function",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Using an Built-In R Function",
    "text": "Using an Built-In R Function\nWe can also use the lm.ridge() function from the {MASS} package to fit a ridge regression. This function uses a formula based on variables from a data frame in the same fashion as the lm() function. It also takes the argument lambda= which specifies the values of \\(\\lambda\\) to use in the penalty term. Because the data= argument has to be a data frame (or tibble), we convert the standardized data (which is a matrix) to a data frame using the data.frame() function.\n\n# Create data frame for use in lm.ridge()\nz_data = z_eeo |&gt;\n  data.frame()\n\n# Fit ridge regression (lambda = 0.1)\nridge_1 = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = 0.1)\n\n# View coefficients\ntidy(ridge_1)\n\n# A tibble: 3 × 5\n  lambda    GCV term    estimate scale\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1    0.1 0.0121 faculty    0.433 0.993\n2    0.1 0.0121 peer       0.850 0.993\n3    0.1 0.0121 school    -0.845 0.993"
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#comparison-to-the-ols-coefficients",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#comparison-to-the-ols-coefficients",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Comparison to the OLS Coefficients",
    "text": "Comparison to the OLS Coefficients\nHow do these biased coefficients from the ridge regression compare to the unbiased coefficients from the OLS estimation? Below, we fit a standardized OLS model to the data, and compare the coefficients to those from the ridge regression with \\(\\lambda=0.1\\).\n\n# Fit standardized OLS model\nlm.1 = lm(achievement ~ faculty + peer + school - 1, data = z_data)\n\n# Obtain coefficients\ncoef(lm.1)\n\n   faculty       peer     school \n 0.5248647  0.9449056 -1.0272986 \n\n\nComparing these coefficients from the different models:\n\n\n\n\nTable 1: Comparison of the coefficients from the OLS (λ=0) and the ridge regression using λ=0.1 based on the standardized data.\n\n\n\n\nComparison of the coefficients from the OLS (λ=0) and the ridge regression using λ=0.1 based on the standardized data.\n\n\nPredictor\nλ=0\nλ=0.1\n\n\n\n\nFaculty\n0.525\n0.436\n\n\nPeer\n0.945\n0.856\n\n\nSchool\n-1.027\n-0.851\n\n\n\n\n\n\n\n\nBased on this comparison, the ridge regression has “shrunk” the estimate of each coefficient toward zero. Remember, the larger the value of \\(\\lambda\\), the more the coefficient estimates will shrink toward 0. The table below shows the coefficient estimates for four different values of \\(\\lambda\\).\n\n\n\n\nTable 2: Comparison of the coefficients from the OLS (λ=0) and three ridge regressions (using λ=0.1, λ=1, and λ=10) based on the standardized data. The condition numbers for the XTX + λI matrix are also provided.\n\n\n\n\nComparison of the coefficients from the OLS (λ=0) and three ridge regressions (using λ=0.1, λ=1, and λ=10) based on the standardized data. The condition numbers for the XTX + λI matrix are also provided.\n\n\nPredictor\n&lambda;=0\n&lambda;=0.1\n&lambda;=1\n&lambda;=10\n\n\n\n\nFaculty\n0.525\n0.436\n0.180\n0.114\n\n\nPeer\n0.945\n0.856\n0.537\n0.227\n\n\nSchool\n-1.027\n-0.851\n-0.283\n0.073\n\n\nCondition Number\n370.884\n313.908\n132.125\n20.256\n\n\n\n\n\n\n\n\nExamining these results we see that increasing the penalty (i.e., higher \\(\\lambda\\) values) better conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but a higher penalty also shrinks the estimates toward zero more (increased bias). This implies that we want a \\(\\lambda\\) that is large enough so that it conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but not too large because we want to introduce the least amount of bias as possible."
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#ridge-trace",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#ridge-trace",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Ridge Trace",
    "text": "Ridge Trace\nA ridge trace computes the ridge regression coefficients for many different values of \\(\\lambda\\). A plot of this trace, can be examined to select the \\(\\lambda\\) value. To do this, we pick the smallest value for \\(\\lambda\\) that produces stable regression coefficients. Here we examine the values of \\(\\lambda\\) where \\(\\lambda = \\{ 0,0.001,0.002,0.003,\\ldots,100\\}\\).\nTo create this plot, we fit a ridge regression that includes a sequence of values in the lambda= argument of the lm.ridge() function. Then we use the tidy() function to summarize the output from this model. This output includes the coefficient estimates for each of the \\(\\lambda\\) values in our sequence. We can then create a line plot of the coefficient values versus the \\(\\lambda\\) values for each predictor.\n\n# Fit ridge model across several lambda values\nridge_models = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, \n                        lambda = seq(from = 0, to = 100, by = 0.01))\n\n# Get tidy() output\nridge_trace = tidy(ridge_models)\nridge_trace\n\n# A tibble: 30,003 × 5\n   lambda    GCV term    estimate scale\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1   0    0.0122 faculty    0.521 0.993\n 2   0    0.0122 peer       0.938 0.993\n 3   0    0.0122 school    -1.02  0.993\n 4   0.01 0.0122 faculty    0.511 0.993\n 5   0.01 0.0122 peer       0.928 0.993\n 6   0.01 0.0122 school    -1.00  0.993\n 7   0.02 0.0122 faculty    0.501 0.993\n 8   0.02 0.0122 peer       0.918 0.993\n 9   0.02 0.0122 school    -0.980 0.993\n10   0.03 0.0122 faculty    0.491 0.993\n# ℹ 29,993 more rows\n\n# Ridge trace\nggplot(data = ridge_trace, aes(x = lambda, y = estimate)) +\n  geom_line(aes(group = term, color = term)) +\n  theme_bw() +\n  xlab(expression(lambda)) +\n  ylab(\"Coefficient estimate\") +\n  ggsci::scale_color_d3(name = \"Predictor\")\n\n\n\n\n\n\n\nFigure 1: Ridge plot showing the size of the standardized regression coefficients for \\(\\lambda\\) values between 0 (OLS) and 100.\n\n\n\n\n\nWe want to find the \\(\\lambda\\) value where the lines begin to flatten out; where the coefficients are no longer changing value. This is difficult to ascertain, but somewhere around \\(\\lambda=50\\), there doesn’t seem to be a lot of change in the coefficients. This suggests that a \\(\\lambda\\) value around 50 would produce stable coefficient estimates."
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#aic-value",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#aic-value",
    "title": "Biased Estimation: Ridge Regression",
    "section": "AIC Value",
    "text": "AIC Value\nIt turns out that not only is it difficult to make a subjective call about where the trace lines begin to flatten, but even when people do make this determination, they often select a \\(\\lambda\\) value that is too high. A better method for obtaining \\(\\lambda\\) is to compute a model-level metric that we can then evaluate across the models produced by the different values of \\(\\lambda\\). One such metric is the Akiake Information Criteria (AIC). We can compute the AIC for a ridge regression as\n\\[\n\\mathrm{AIC} = n \\times \\ln\\big(\\mathbf{e}^{\\intercal}\\mathbf{e}\\big) + 2(\\mathit{df})\n\\]\nwhere n is the sample size, \\(\\mathbf{e}\\) is the vector of residuals from the ridge model, and df is the degrees of freedom associated with the ridge regression model, which we compute by finding the trace of the H matrix, namely,\n\\[\ntr(\\mathbf{H}_{\\mathrm{Ridge}}) =  tr(\\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^{\\intercal})\n\\]\nFor example, to compute the AIC value associated with the ridge regression estimated using a \\(\\lambda\\) value of 0.1 we can use the following syntax.\n\n# Compute coefficients for ridge model\nb = solve(t(X) %*% X + 0.1*diag(3)) %*% t(X) %*% y\n\n# Compute residual vector\ne = y - (X %*% b)\n\n# Compute H matrix\nH = X %*% solve(t(X) %*% X + 0.1*diag(3)) %*% t(X)\n\n# Compute df\ndf = sum(diag(H))\n\n# Compute AIC\naic = 70 * log(t(e) %*% e) + 2 * df\naic\n\n         [,1]\n[1,] 285.8729\n\n\nWe want to compute the AIC value for every single one of the models associated with the \\(\\lambda\\) values from our sequence we used to produce the ridge trace plot. To do this, we will create a function that will compute the AIC from a given \\(\\lambda\\) value.\n\n# Function to compute AIC based on inputted lambda value\nridge_aic = function(lambda){\n  b = solve(t(X) %*% X + lambda*diag(3)) %*% t(X) %*% y\n  e = y - (X %*% b)\n  H = X %*% solve(t(X) %*% X + lambda*diag(3)) %*% t(X)\n  df = sum(diag(H))\n  n = length(y)\n  aic = n * log(t(e) %*% e) + 2 * df\n  return(aic)\n}\n\n# Try function\nridge_aic(lambda = 0.1)\n\n         [,1]\n[1,] 285.8729\n\n\nTo be able to evaluate which \\(\\lambda\\) value is assocuated with the lowest AIC, we need to compute the AIC for many different \\(\\lambda\\) values. To do this, we will create a data frame that has a column that includes the \\(\\lambda\\) values we want to evaluate. Then we use the rowwise() and mutate() functions to apply the ridge_aic() function to each of the lambda values. Finally, we can use filter() to find the \\(\\lambda\\) value associated with the smallest AIC value.\n\n# Create data frame with column of lambda values\n# Create a new column by using the ridge_aic() function for each row\nmy_models = data.frame(\n  Lambda = seq(from = 0, to = 100, by = 0.01)\n  ) |&gt;\n  rowwise() |&gt;\n   mutate(\n    AIC = ridge_aic(Lambda)\n  ) |&gt;\n  ungroup() #Turn off the rowwise() operation\n\n# Find lambda associated with smallest AIC\nmy_models |&gt; \n  filter(AIC == min(AIC))\n\n# A tibble: 1 × 2\n  Lambda AIC[,1]\n   &lt;dbl&gt;   &lt;dbl&gt;\n1   21.8    284.\n\n\nA \\(\\lambda\\) value of 21.77 produces the smallest AIC value, so this is the \\(\\lambda\\) value we will adopt.\n\n# Re-fit ridge regression using lambda = 21.77\nridge_smallest_aic = lm.ridge(achievement ~ -1 + faculty + peer + school, \n                              data = z_data, lambda = 21.77)\n\n# View coefficients\ntidy(ridge_smallest_aic)\n\n# A tibble: 3 × 5\n  lambda    GCV term    estimate scale\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1   21.8 0.0118 faculty   0.115  0.993\n2   21.8 0.0118 peer      0.174  0.993\n3   21.8 0.0118 school    0.0994 0.993\n\n\nBased on using \\(\\lambda=21.77\\), the fitted ridge regression model is:\n\\[\n\\hat{\\mathrm{Achievement}}^{\\star}_i = 0.115(\\mathrm{Faculty}^{\\star}_i) + 0.174(\\mathrm{Peer}^{\\star}_i) + 0.099(\\mathrm{School}^{\\star}_i)\n\\]"
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#estimating-sampling-variance",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#estimating-sampling-variance",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Estimating Sampling Variance",
    "text": "Estimating Sampling Variance\nIn theory it is possible to obtain the sampling variances for the ridge regression coefficients using matrix algebra:\n\\[\n\\sigma^2_{\\mathbf{b}} = \\sigma^2_{e}(\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^\\intercal \\mathbf{X} (\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\n\\]\nwhere \\(\\sigma^2_e\\) is the the error variance estimated from the standardized OLS model.\n\n# Fit standardized model to obtain sigma^2_e\nglance(lm(achievement ~ -1 + faculty + peer + school, data = z_data))\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.206         0.171 0.904      5.80 0.00138     3  -90.7  189.  198.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# Compute sigma^2_epsilon\nresid_var = 0.9041214 ^ 2\n\n# Compute variance-covariance matrix of ridge estimates\nW = solve(t(X) %*% X + 21.77*diag(3))\nvar_b = resid_var * W %*% t(X) %*% X %*% W\n\n# Compute SEs\nsqrt(diag(var_b))\n\n   faculty       peer     school \n0.05482363 0.05670386 0.04133673 \n\n\nComparing these SEs to the SEs from the OLS regression:\n\n\n\n\nTable 3: Comparison of the standard errors from the OLS (λ=0) and ridge regression (λ;=21.77) based on the standardized data.\n\n\n\n\nComparison of the standard errors from the OLS (λ=0) and ridge regression (λ;=21.77) based on the standardized data.\n\n\nPredictor\n&lambda;=0\n&lambda;=21.77\n\n\n\n\nFaculty\n0.667\n0.055\n\n\nPeer\n0.598\n0.057\n\n\nSchool\n0.993\n0.041\n\n\n\n\n\n\n\n\nBased on this comparison, we can see that the standard errors from the ridge regression are quite a bit smaller than those from the OLS. We can then use the estimates and SEs to compute t- and p-values, and confidence intervals. Here we only do it for the school facilities predictor (since it is of primary interest based on the RQ) but one could do it for all the predictors.\n\n# Compute t-value for school predictor\nt = 0.09944444 / 0.04133673 \nt\n\n[1] 2.405716\n\n# Compute df residual\nH = X %*% solve(t(X) %*% X + 21.77*diag(3)) %*% t(X)\ndf_model = sum(diag(H))\ndf_residual = 69 - df_model\n\n# Compute p-value\np = pt(-abs(t), df = df_residual) * 2\np\n\n[1] 0.01886747\n\n# Compute CI\n0.09944444 - qt(p = 0.975, df = df_residual) * 0.04133673 \n\n[1] 0.01695739\n\n0.09944444 + qt(p = 0.975, df = df_residual) * 0.04133673\n\n[1] 0.1819315\n\n\nThis suggests that after controlling for peer influence and faculty credential, there is evidence of an effect of school facilities on student achievement (\\(p=.019\\)). The uncertainty in the 95% CI suggests that the true partial effect of school facilities is between 0.017 and 0.182. The empirical evidence is pointing toward a slight positive effect of school facilities."
  },
  {
    "objectID": "notes/05-04-biased-estimation-ridge-regression.html#footnotes",
    "href": "notes/05-04-biased-estimation-ridge-regression.html#footnotes",
    "title": "Biased Estimation: Ridge Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt least within the class of linear, unbiased estimators.↩︎"
  },
  {
    "objectID": "notes/06-02-cross-validation.html",
    "href": "notes/06-02-cross-validation.html",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "",
    "text": "In a previous activity, we examined predictors of average life expectancy in a state, using data from states-2019.csv. We used several model building strategies and performance metrics to select and evaluate predictors. Unfortunately, we were evaluating the model/predictors using the same data that we used to build the model. This has several problems in practice, especially if you are using inferential metrics (p-values) to select the predictors:\nOne set of methods for dealing with these problems is to use cross-validation. The basic idea of cross-validation is to use one set of data to fit your model(s) and a completely separate set of data to evaluate the models’ performance. We will again use the states-2019.csv data to introduce methods of cross-validation. After importing the data, we will also standardize all the numeric variables.\n# Load libraries\nlibrary(modelr)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidymodels) # Loads broom, rsample, parsnip, recipes, workflow, tune, yardstick, and dials\n\n# Import and view data\nusa = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/states-2019.csv\")\n\n# Create standardized variables after removing state names\nz_usa = usa |&gt;\n  select(-state) |&gt;\n  scale(center = TRUE, scale = TRUE) |&gt;\n  data.frame()\n\n# View data\nhead(z_usa)\n\n  life_expectancy  population     income illiteracy      murder    hs_grad\n1     -1.87539008 -0.20138748 -0.9151339  0.7050017  0.90297951 -0.9556270\n2      0.04786378 -0.77268875  0.8656837 -0.5894276  1.02638276  1.0873231\n3      0.67009298  0.12393888 -0.5179528  0.3120499  0.07000761 -0.6103396\n4     -1.59255863 -0.45958823 -1.0512942  0.4507388  0.71787464 -0.8693051\n5      1.63171991  4.53828023  0.3720793  2.6235307 -0.14594807 -1.7612974\n6      1.00949072 -0.08422076  0.7329133 -0.4276240 -0.36190375  0.7132618\n       frost       area\n1 -1.2033782 -0.2033181\n2  1.7777726  5.8988618\n3 -0.2913627  0.5348718\n4 -0.8290718 -0.1877660\n5 -1.4940832  1.0317286\n6  1.1678622  0.4185305\nFrom our previous notes, there were a several candidate models that we may have been considering based on their AICc values. In this set of notes we will consider the best k-predictor models that had an AICc value within four of the minimum AICc value. These candidate models include:\n\\[\n\\begin{split}\n\\mathbf{M1:~} \\mathrm{Life~Expectancy}_i &= \\beta_1(\\mathrm{Income}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{M2:~} \\mathrm{Life~Expectancy}_i &= \\beta_1(\\mathrm{Income}_i) + \\beta_2(\\mathrm{Population}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{M3:~} \\mathrm{Life~Expectancy}_i &= \\beta_1(\\mathrm{Income}_i) + \\beta_2(\\mathrm{Population}_i) + \\beta_3(\\mathrm{Illiteracy~Rate}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{M4:~} \\mathrm{Life~Expectancy}_i &= \\beta_1(\\mathrm{Income}_i) + \\beta_2(\\mathrm{Population}_i) + \\beta_3(\\mathrm{Illiteracy~Rate}_i) +\\\\\n&~~~~~\\beta_4(\\mathrm{Murder~Rate}_i) + \\epsilon_i\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#cross-validated-mean-square-error-cv-mse",
    "href": "notes/06-02-cross-validation.html#cross-validated-mean-square-error-cv-mse",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Cross-Validated Mean Square Error (CV-MSE)",
    "text": "Cross-Validated Mean Square Error (CV-MSE)\nOne very common metric used is the residual variance (a.k.a., mean square error). Because this is computed on the validation data rather than on the data used to fit the model, we refer to it as the Cross-Validated Mean Square Error (CV-MSE). The CV-MSE is:\n\\[\n\\mathrm{CV\\mbox{-}MSE} = \\frac{1}{n} \\sum_{i=1}^n e_i^2\n\\]\nwhere n is the number of observations in the validation set, and \\(e_i\\) are the residuals computed on the validation set. This value summarizes the model-data misfit in the validation data. Note that because this is a residual-based measure, lower values of CV-MSE correspond to better fitting models. Because this metric is computed on the validation data (data not used to initially fit the model), it is a better measure of how the model might perform in future samples."
  },
  {
    "objectID": "notes/06-02-cross-validation.html#divide-the-sample-data-into-a-training-and-validation-set",
    "href": "notes/06-02-cross-validation.html#divide-the-sample-data-into-a-training-and-validation-set",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Divide the Sample Data into a Training and Validation Set",
    "text": "Divide the Sample Data into a Training and Validation Set\nThere are many ways to do this, but here we will use the sample() function to randomly sample 35 cases (rows) from the z_usa data (about 2/3 of the data). These 35 cases will make up the training data. Then we will use the filter() function to select those cases. The remaining cases (those 17 observations not sampled) are put into a validation set of data.\n\n# Make the random sampling replicable\nset.seed(42)\n\n# Select the cases to be in the training set\ntraining_cases = sample(1:nrow(z_usa), size = 35, replace = FALSE)\n\n# Create training data from the sampled cases\ntrain = z_usa |&gt;\n  filter(row_number() %in% training_cases)\n\n# Create validation data from the remaining cases\nvalidate = z_usa |&gt;\n  filter(!row_number() %in% training_cases)"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#fit-the-candidate-models-to-the-training-data",
    "href": "notes/06-02-cross-validation.html#fit-the-candidate-models-to-the-training-data",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Fit the Candidate Models to the Training Data",
    "text": "Fit the Candidate Models to the Training Data\nWe now fit the four candidate models to the observations in the training data.\n\nlm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)\nlm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)\nlm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)\nlm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#use-the-coefficients-from-the-fitted-models-and-the-validation-data-to-obtain-fitted-values-residuals-and-cv-mses",
    "href": "notes/06-02-cross-validation.html#use-the-coefficients-from-the-fitted-models-and-the-validation-data-to-obtain-fitted-values-residuals-and-cv-mses",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Use the Coefficients from the Fitted Models and the Validation Data to Obtain Fitted Values, Residuals, and CV-MSEs",
    "text": "Use the Coefficients from the Fitted Models and the Validation Data to Obtain Fitted Values, Residuals, and CV-MSEs\nNow we can obtain the predicted life expectancy for the observations in the validation data based on the coefficients from the models fitted to the training data. These vectors of fitted values can be used to compute the residuals for the validation observations, which in turn can be used to compute the CV-MSE.\n\n# Get the predicted values for the validation data\nyhat_1 = predict(lm.1, newdata = validate)\nyhat_2 = predict(lm.2, newdata = validate)\nyhat_3 = predict(lm.3, newdata = validate)\nyhat_4 = predict(lm.4, newdata = validate)\n\n# Compute residuals and CV-MSE\nsum((validate$life_expectancy - yhat_1) ^ 2) / nrow(validate)\n\n[1] 0.7349516\n\nsum((validate$life_expectancy - yhat_2) ^ 2) / nrow(validate)\n\n[1] 0.7483797\n\nsum((validate$life_expectancy - yhat_3) ^ 2) / nrow(validate)\n\n[1] 0.8547222\n\nsum((validate$life_expectancy - yhat_4) ^ 2) / nrow(validate)\n\n[1] 0.8968336"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#select-candidate-model-with-the-lowest-cv-mse",
    "href": "notes/06-02-cross-validation.html#select-candidate-model-with-the-lowest-cv-mse",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Select Candidate Model with the Lowest CV-MSE",
    "text": "Select Candidate Model with the Lowest CV-MSE\nSince the CV-MSE is a measure of model misfit, the candidate model having the smallest CV-MSE is the model that should be adopted. In this case, the values for the CV-MSE suggest adopting the single predictor model.\n\n\n\n\nTable 1: CV-MSE for four potential models based on simple cross-validation.\n\n\n\n\nCV-MSE for four potential models based on simple cross-validation.\n\n\nModel\nCV-MSE\n\n\n\n\n1-Predictor Model\n0.735\n\n\n2-Predictor Model\n0.748\n\n\n3-Predictor Model\n0.855\n\n\n4-Predictor Model\n0.897"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#function-to-carry-out-loocv",
    "href": "notes/06-02-cross-validation.html#function-to-carry-out-loocv",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Function to Carry Out LOOCV",
    "text": "Function to Carry Out LOOCV\nTo carry out LOOCV, we will write a function that computes \\(\\widehat{\\mathrm{CV\\mbox{-}MSE}}_i\\) for each model given a particular case.\n\n# Function to compute CV-MSE for LOOCV\ncv_mse_i = function(case_index){\n\n  # Create training and validation data sets\n  train = z_usa |&gt; filter(row_number() != case_index)\n  validate = z_usa |&gt; filter(row_number() == case_index)\n\n  # Fit models to training data\n  lm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)\n  lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)\n  lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)\n  lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)\n\n  # Compute fitted value for validation data\n  yhat_1 = predict(lm.1, newdata = validate)\n  yhat_2 = predict(lm.2, newdata = validate)\n  yhat_3 = predict(lm.3, newdata = validate)\n  yhat_4 = predict(lm.4, newdata = validate)\n\n  # Compute CV-MSE_i for each model\n  cv_mse_1 = (validate$life_expectancy - yhat_1) ^ 2\n  cv_mse_2 = (validate$life_expectancy - yhat_2) ^ 2\n  cv_mse_3 = (validate$life_expectancy - yhat_3) ^ 2\n  cv_mse_4 = (validate$life_expectancy - yhat_4) ^ 2\n\n  # Output a data frame\n  return(data.frame(cv_mse_1, cv_mse_2, cv_mse_3, cv_mse_4))\n}\n\n# Test function on Case 1\ncv_mse_i(1)\n\n  cv_mse_1 cv_mse_2 cv_mse_3 cv_mse_4\n1  2.08481 1.988258 1.383972 1.297868\n\n\nThen we can use this function to compute the four \\(\\widehat{\\mathrm{CV\\mbox{-}MSE}}_i\\) values for each case in the data. We set up a data frame that includes a column called case that has each case number in the data. Then we use rowwise() to operate on each row separately and mutate() a new column by applying our cv_mse_i() function using map(). This creates a list column where the contents of each cell in the column is actually a list; in our case a data frame of four columns. Finally, we use unnest() to take the contents of the list column and spread them out into separate columns.\n\n# Apply cv_mse_i() function to all cases\nmy_cv_mse = data.frame(case = 1:52) |&gt;\n  rowwise() |&gt;\n  mutate(\n    cv_mse = map(case, cv_mse_i) #New list column that includes the data frame of output\n  ) |&gt;\n  unnest(cols = cv_mse) #Turn list column into multiple columns\n\n# View output\nmy_cv_mse\n\n# A tibble: 52 × 5\n    case cv_mse_1 cv_mse_2 cv_mse_3 cv_mse_4\n   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1     1   2.08     1.99     1.38    1.30   \n 2     2   0.151    0.0504   0.0681  0.00113\n 3     3   0.870    0.816    0.956   0.889  \n 4     4   1.20     1.02     0.629   0.616  \n 5     5   2.10     0.773    0.657   0.562  \n 6     6   0.425    0.456    0.315   0.331  \n 7     7   0.0544   0.114    0.0310  0.0537 \n 8     8   0.136    0.0471   0.0230  0.0143 \n 9     9   4.62     3.83     1.80    1.24   \n10    10   0.820    0.275    0.531   0.450  \n# ℹ 42 more rows"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#alternative-programming-use-a-for-loop",
    "href": "notes/06-02-cross-validation.html#alternative-programming-use-a-for-loop",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Alternative Programming: Use a for() Loop",
    "text": "Alternative Programming: Use a for() Loop\nAlternatively, we could also have used a for() loop to carry out the LOOCV. Here is some example syntax (not run):\n\n# Set up empty vector to store results\nmse_1 = rep(NA, 52)\nmse_2 = rep(NA, 52)\nmse_3 = rep(NA, 52)\nmse_4 = rep(NA, 52)\nmse_5 = rep(NA, 52)\n\n# Loop through the cross-validation\nfor(i in 1:nrow(z_usa)){\n  train = z_usa |&gt; filter(row_number() != i)\n  validate = z_usa |&gt; filter(row_number() == i)\n\n  lm.1 = lm(life_expectancy ~ -1 + income,                                    data = train)\n  lm.2 = lm(life_expectancy ~ -1 + income + population,                       data = train)\n  lm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy,          data = train)\n  lm.4 = lm(life_expectancy ~ -1 + income + population + illiteracy + murder, data = train)\n\n  yhat_1 = predict(lm.1, newdata = validate)\n  yhat_2 = predict(lm.2, newdata = validate)\n  yhat_3 = predict(lm.3, newdata = validate)\n  yhat_4 = predict(lm.4, newdata = validate)\n\n  mse_1[i] = (validate$life_expectancy - yhat_1) ^ 2\n  mse_2[i] = (validate$life_expectancy - yhat_2) ^ 2\n  mse_3[i] = (validate$life_expectancy - yhat_3) ^ 2\n  mse_4[i] = (validate$life_expectancy - yhat_4) ^ 2\n\n} \n\n# Create data frame of results\nmy_cv_mse = data.frame(\n  case = 1:52,\n  cv_mse_1 = mse_1, \n  cv_mse_2 = mse_2,\n  cv_mse_3 = mse_3,\n  cv_mse_4 = mse_4\n  )"
  },
  {
    "objectID": "notes/06-02-cross-validation.html#select-model-with-lowest-cv-mse",
    "href": "notes/06-02-cross-validation.html#select-model-with-lowest-cv-mse",
    "title": "Cross-Validation Methods for Model Selection",
    "section": "Select Model with Lowest CV-MSE",
    "text": "Select Model with Lowest CV-MSE\nWe can now compute the average CV-MSE for each of the candidate models and select the candidate model with the lowest average CV-MSE.\n\n# Compute average CV-MSE\nmy_cv_mse |&gt;\n  select(-case) |&gt;\n  summarize_all(mean)\n\n# A tibble: 1 × 4\n  cv_mse_1 cv_mse_2 cv_mse_3 cv_mse_4\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    0.832    0.807    0.769     1.12\n\n\nThe LOOCV results suggest that we adopt the three-predictor model; it has the smallest average CV-MSE. Since LOOCV method is less biased than simple cross-validation (it trains the models on a much larger set of observations) its results are more believable than those from the simple cross-validation. Another advantage of LOOCV is that it will always produce the same results as opposed to simple cross-validation) since there is no randomness in producing the training and validation datasets.\n\n\n\n\nTable 2: CV-MSE for four potential models based on simple cross-validation and Leave-One-Out Cross-Validation (LOOCV).\n\n\n\n\nCV-MSE for four potential models based on simple cross-validation and Leave-One-Out Cross-Validation (LOOCV).\n\n\n\n\n\n\n\n\n\nCV-MSE\n\n\n\nModel\nSimple CV\nLOOCV\n\n\n\n\n1-Predictor Model\n0.735\n0.832\n\n\n2-Predictor Model\n0.748\n0.807\n\n\n3-Predictor Model\n0.855\n0.769\n\n\n4-Predictor Model\n0.897\n1.124\n\n\n\n\n\n\n\n\n\n\nComputing the CV-MSE under LOOCV: A Shortcut for OLS Models\nLOOCV can be computationally expensive when you sample is very large; we have to fit the candidate models n times. It turns out, however, that models fit with OLS, can give us the LOOCV results using the following formula:\n\\[\n\\mathrm{CV\\mbox{-}MSE} = \\frac{1}{n} \\sum_{i=1}^n \\bigg(\\frac{e_i}{1-h_{ii}}\\bigg)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the ith fitted value from the original least squares fit, and \\(h_{ii}\\) is the leverage value. This is similar to the model (biased) MSE, except the ith residual is divided by \\(1 − h_{ii}\\). Below, we compute the LOOCV CV-MSE for the three-predictor candidate model\n\n# Compute CV-MSE for Candidate Model 3\nlm.3 = lm(life_expectancy ~ -1 + income + population + illiteracy, data = z_usa)\n\n# Augment the model to get e_i and h_ii\nout.3 = augment(lm.3)\n\n# Compute CV-MSE for best three-predictor model\n1 / 52 * sum((out.3$.resid / (1 - out.3$.hat))^2)\n\n[1] 0.7686342"
  },
  {
    "objectID": "notes/06-01-vintage-methods.html",
    "href": "notes/06-01-vintage-methods.html",
    "title": "Vintage Methods for Model Selection",
    "section": "",
    "text": "In a previous activity, we examined predictors of average life expectancy in a state, using data from states-2019.csv. In this set of notes, we will examine several “vintage” methods for model selection. These methods are often used when researchers have no a priori hypotheses about the data.\n# Load libraries\nlibrary(modelr)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidymodels) # Loads broom, rsample, parsnip, recipes, workflow, tune, yardstick, and dials\n\n# Import and view data\nusa = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/states-2019.csv\")\n\n# View data\nusa\n\n# A tibble: 52 × 9\n   state       life_expectancy population income illiteracy murder hs_grad frost\n   &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama                75.4      4.90    23.6       14.8    7.8    85.3  42.8\n 2 Alaska                 78.8      0.732   33.1        9.2    8.2    92.4 200. \n 3 Arizona                79.9      7.28    25.7       13.1    5.1    86.5  90.8\n 4 Arkansas               75.9      3.02    22.9       13.7    7.2    85.6  62.5\n 5 California             81.6     39.5     30.4       23.1    4.4    82.5  27.5\n 6 Colorado               80.5      5.76    32.4        9.9    3.7    91.1 168. \n 7 Connecticut            80.9      3.57    39.4        8.6    2.3    90.2  88.5\n 8 Delaware               78.4      0.974   30.5       10.7    5      89.3 114  \n 9 District o…            78.6      0.706   45.9       19      5.9    90.3  97  \n10 Florida                80       21.5     26.6       19.7    5.2    87.6   7.1\n# ℹ 42 more rows\n# ℹ 1 more variable: area &lt;dbl&gt;\nIn all of our previous courses and notes, predictors have been selected a priori, based on substantive literature and theory. Subsequently, most of our analytic work has focused on:\nWhen theory does not specify the predictor set, variable (model) selection becomes an important analytical problem! To consider this, let’s examine the how we might make decisions about potential predictors:"
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#purpose-of-the-model-and-analytic-goal",
    "href": "notes/06-01-vintage-methods.html#purpose-of-the-model-and-analytic-goal",
    "title": "Vintage Methods for Model Selection",
    "section": "Purpose of the Model and Analytic Goal",
    "text": "Purpose of the Model and Analytic Goal\nNote that there are many decisions to be made in this process. One thing that can help guide some of these decisions is the purpose for modeling and the related analytic goal. For example:\n\nDescription\n\nPurpose of the model is to describe the data, or to understand a complex system.\nAnalytic goal could be to choose the smallest number of predictors that account for a substantial amount of variation in the outcome.\nNote that this analytic goal may hve competing requirements since explaining more variation in the outcome generally requires more predictors\n\nInference/Prediction\n\nPurpose of the model is to predict the outcome/mean outcome for new cases or make inferences about the eﬀects of predictors\nHere the analytic goal is prediction/inferential accuracy.\nBecause of this, performance in our sample is not as important as performance in future (out-of-sample) cases\n\n\n\n\nModel Evaluation Criteria\nThe model’s purpose determines how we will measure and evaluate “model success”. Each purpose points to a diﬀerent criteria to use in the model evaluation process.\nThere are several criteria that have been proposed to evaluate model performance when the purpose is description.\n\nSum of Squared Residuals: \\(\\mathrm{SSE} = \\sum(Y_i - \\hat{Y}_i)^2\\)\nResidual Mean Square: \\(\\mathrm{RMS} = \\frac{\\mathrm{SSE}}{\\mathrm{df}_{\\mathrm{Residual}}}\\)\n\nWhen using criteria that measure the residual error (SSE, RMS), we want to select a model that minimizes these values.\n\n\\(\\mathbf{R}^2\\): \\(R^2 = \\frac{\\mathrm{SST}-\\mathrm{SSE}}{\\mathrm{SST}}\\)\nAdjusted \\(\\mathbf{R}^2\\): \\(R^2_{Adj} = \\frac{\\mathrm{SST}-\\mathrm{SSE}}{\\mathrm{SST}} \\times \\frac{\\mathrm{df}_{\\mathrm{Total}}}{\\mathrm{df}_{\\mathrm{Residual}}}\\)\n\nWhen using an \\(R^2\\) value to evaluate model performance, we want to select a model that maximizes these values. The adjusted \\(R^2\\) value penalizes the \\(R^2\\) value for model complexity, so when the number of predictors varies across the models, this is a better criterion.\nThe criteria that have been proposed to evaluate model performance when the purpose is prediction/inference focus on measuring out-of-sample performance. (Note that in each of the following formulae, k is the total number of parameters being estimated, including the residual variance.)\n\nt-value/p-value\n\nNote that these criteria are at the predictor-level; not the model-level. Using the maximum p-value or minimum t-value can make this a model-level metric.\n\nMallow’s Cp: \\(C_p=\\frac{\\mathrm{SSE}}{\\hat{\\sigma^2_{\\mathrm{Residual (Full)}}}}+2k-n\\)\n\nMallow’s Cp is an estimate of the average mean squared error of prediction. The Cp value should be similar to the number of predictors in the model.\n\nAIC: \\(\\mathrm{AIC} = n \\times \\ln(\\frac{\\mathrm{SSE}}{n}) + 2k\\)\nCorrected AIC: \\(\\mathrm{AICc} = \\mathrm{AIC} + \\frac{2(k+2)(k+3)}{n-k-3}\\)\n\nAIC has a penalty for model complexity. It must be computed on the same set of observations; no missing data. Corrected AIC has been found to perform better than AIC.\n\nBIC: \\(\\mathrm{BIC} = n \\times \\ln(\\frac{\\mathrm{SSE}}{n}) + k \\bigg(\\ln(n)\\bigg)\\)\n\nBIC has a larger penalty term than AIC, which is based on sample size and model complexity. It performs best when the “true” model is among the candidate models."
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#back-to-example",
    "href": "notes/06-01-vintage-methods.html#back-to-example",
    "title": "Vintage Methods for Model Selection",
    "section": "Back to Example",
    "text": "Back to Example\nOur modeling goal is to explore the predictors of life expectancy. Because we have no a priori hypotheses about which predictors should be included in the model nor about the importance of these predictors, one method is to put all of the predictors in the model immediately. Before we do this, we will create a data frame of the usa data that will be useful in our modeling by removing state name—this leaves only the outcome and predictors in the data.\n\n# Create data frame that includes all rows/columns except the state names\nusa2 = usa |&gt;\n  select(-state)\n\n# View data\nhead(usa2)\n\n# A tibble: 6 × 8\n  life_expectancy population income illiteracy murder hs_grad frost  area\n            &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1            75.4      4.90    23.6       14.8    7.8    85.3  42.8  5.08\n2            78.8      0.732   33.1        9.2    8.2    92.4 200.  57.1 \n3            79.9      7.28    25.7       13.1    5.1    86.5  90.8 11.4 \n4            75.9      3.02    22.9       13.7    7.2    85.6  62.5  5.21\n5            81.6     39.5     30.4       23.1    4.4    82.5  27.5 15.6 \n6            80.5      5.76    32.4        9.9    3.7    91.1 168.  10.4 \n\n# Use all variables as predictors\nlm.all = lm(life_expectancy ~ ., data = usa2)\n\n# Examine output\ntidy(lm.all)\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept) 79.9      12.1         6.61  0.0000000426\n2 population   0.0806    0.0391      2.06  0.0451      \n3 income       0.160     0.0562      2.85  0.00661     \n4 illiteracy  -0.158     0.0829     -1.90  0.0639      \n5 murder      -0.130     0.106      -1.23  0.226       \n6 hs_grad     -0.0364    0.137      -0.265 0.792       \n7 frost       -0.00622   0.00694    -0.897 0.375       \n8 area         0.0189    0.0290      0.653 0.517       \n\n\nBased on the coefficient-level output, we find that:\n\nPopulation, area, and income are positively related to life expectancy;\nIlliteracy rate, murder rate, days with a temperature below freezing, and graduation rate are negatively related to life expectancy;\n\nAlso,\n\nOnly population, income, and maybe illiteracy rate are statistically significant at the .05-level.\n\n\n\nComputing Model Evaluation Criteria\nHere we compute the different model evaluation criteria as an example. In practice, you would decide on the criterion you will use prior to any analysis being undertaken.\n\n# Descriptive: Compute SSE and RMS\nanova(lm.all)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npopulation  1  7.892   7.892  3.5087 0.0677015 .  \nincome      1 38.172  38.172 16.9703 0.0001648 ***\nilliteracy  1  9.635   9.635  4.2833 0.0443921 *  \nmurder      1  2.478   2.478  1.1015 0.2996718    \nhs_grad     1  0.208   0.208  0.0923 0.7626738    \nfrost       1  1.072   1.072  0.4765 0.4936514    \narea        1  0.960   0.960  0.4268 0.5169551    \nResiduals  44 98.972   2.249                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSSE = 31.668\nRMS = 0.7197\n\n# Descriptive: Compute R2 and adj. R2\nglance(lm.all)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.379         0.280  1.50      3.84 0.00245     7  -90.5  199.  217.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nR2 = 0.379\nAdjR2 = 0.280\n\n\n# Assign values for k and n\nk = 8 \nn = 52\n\n# Inference: Compute maximum t-value\nmax(tidy(lm.all)$statistic)\n\n[1] 6.610782\n\n# Inference: Compute minimum p-value\nmin(tidy(lm.all)$p.value)\n\n[1] 0.00000004262328\n\n# Inference: Compute Mallow's Cp\nSSE / RMS + 2 * k - n\n\n[1] 8.001667\n\n# Inference: AIC\naic_mod = n * log(SSE / n) + 2 * k\naic_mod\n\n[1] -9.788725\n\n# Inference: AICc\naic_mod + (2 * (k + 2) * (k + 3)) / (n - k - 3)\n\n[1] -4.422871\n\n# Inference: BIC\nn * log(SSE / n) + k * log(n)\n\n[1] 5.821225\n\n\nAnother consideration is the usefulness of these criteria. For example the maximum t-value, minimum p-value, SSE, RMS, \\(R^2\\), and adjusted \\(R^2\\) values are useful for both summarizing a single model and for comparing models. Whereas, Mallow’s Cp, and the information criteria metrics are only useful for comparing models; not for summarizing a single model."
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#model-building-strategy",
    "href": "notes/06-01-vintage-methods.html#model-building-strategy",
    "title": "Vintage Methods for Model Selection",
    "section": "Model Building Strategy",
    "text": "Model Building Strategy\nOnce you determine the criteria/metric you will use to measure the performance of each model, you need to outline your model building strategy. Three common strategies for model building are: forward selection, backward elimination, and stepwise. Note that again, we assume that there is no a priori hypotheses about the importance of the predictors.\nBefore we begin, we will standardize all the variables in the analysis.\n\n# Create standardized variables after removing state names\nz_usa = usa |&gt;\n  select(-state) |&gt;\n  scale(center = TRUE, scale = TRUE) |&gt;\n  data.frame()\n\n# View data\nhead(z_usa)\n\n  life_expectancy  population     income illiteracy      murder    hs_grad\n1     -1.87539008 -0.20138748 -0.9151339  0.7050017  0.90297951 -0.9556270\n2      0.04786378 -0.77268875  0.8656837 -0.5894276  1.02638276  1.0873231\n3      0.67009298  0.12393888 -0.5179528  0.3120499  0.07000761 -0.6103396\n4     -1.59255863 -0.45958823 -1.0512942  0.4507388  0.71787464 -0.8693051\n5      1.63171991  4.53828023  0.3720793  2.6235307 -0.14594807 -1.7612974\n6      1.00949072 -0.08422076  0.7329133 -0.4276240 -0.36190375  0.7132618\n       frost       area\n1 -1.2033782 -0.2033181\n2  1.7777726  5.8988618\n3 -0.2913627  0.5348718\n4 -0.8290718 -0.1877660\n5 -1.4940832  1.0317286\n6  1.1678622  0.4185305\n\n\n\n\nForward Selection\n\nStep 1: We fit each of the one-predictor models and measure the performance of each model using the criteria/metric chosen. The predictor from the model that has the best performance is retained.\nStep 2: We then fit each of the two-predictor models that can be fitted with the predictor retained in Step 1. The predictors from the model that has the best performance are retained.\n\nWe continue this process until we have either (a) fitted a model with all the predictors, or (b) hit some stopping/selection criteria that we have identified (e.g., stop once one of the p-values is greater than 0.05).\n\nIMPORTANT\nOnce a predictor is retained in forward selection, it is always included in all later stages.\n\nIn our example, we will employ forward selection to adopt a model using the following performance metric and selection criteria:\n\nMetric of Performance: Select the predictor with the highest t-value (absolute value).\nSelection Criterion: All t-values for predictors in the model need to be greater than 1.\n\n\n# Step 1: Fit all one-predictor models\n# tidy(lm(life_expectancy ~ -1 + population, data = z_usa)) #t = 1.63\n# tidy(lm(life_expectancy ~ -1 + income,     data = z_usa)) #t = 4.09\n# tidy(lm(life_expectancy ~ -1 + illiteracy, data = z_usa)) #t = -0.45\n# tidy(lm(life_expectancy ~ -1 + murder,     data = z_usa)) #t = -2.93\n# tidy(lm(life_expectancy ~ -1 + hs_grad,    data = z_usa)) #t = 2.47\n# tidy(lm(life_expectancy ~ -1 + frost,      data = z_usa)) #t = 1.19\n# tidy(lm(life_expectancy ~ -1 + area,       data = z_usa)) #t = 0.38\n\nThe best one-predictor model under this criterion includes income.\n\n# Step 2: Fit all two-predictor models that include income\n# tidy(lm(life_expectancy ~ -1 + income + population, data = z_usa)) #t = 1.71\n# tidy(lm(life_expectancy ~ -1 + income + illiteracy, data = z_usa)) #t = -0.58\n# tidy(lm(life_expectancy ~ -1 + income + murder,     data = z_usa)) #t = -1.37\n# tidy(lm(life_expectancy ~ -1 + income + hs_grad,    data = z_usa)) #t = 0.46\n# tidy(lm(life_expectancy ~ -1 + income + frost,      data = z_usa)) #t = 0.03\n# tidy(lm(life_expectancy ~ -1 + income + area,       data = z_usa)) #t = 0.51\n\nThe best two-predictor model under this criterion includes income and population.\n\n# Step 3: Fit all three-predictor models that include income and population\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy, data = z_usa)) #t = -2.13\n# tidy(lm(life_expectancy ~ -1 + income + population + murder,     data = z_usa)) #t = -1.54\n# tidy(lm(life_expectancy ~ -1 + income + population + hs_grad,    data = z_usa)) #t = 1.50\n# tidy(lm(life_expectancy ~ -1 + income + population + frost,      data = z_usa)) #t = 0.90\n# tidy(lm(life_expectancy ~ -1 + income + population + area,       data = z_usa)) #t = 0.25\n\nThe best three-predictor model under this criterion includes income, population, and illiteracy.\n\n# Step 4: Fit all four-predictor models that include income, population, and illiteracy\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder,  data = z_usa)) #t = -1.08\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + hs_grad, data = z_usa)) #t = 0.45\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + frost,   data = z_usa)) #t = -0.35\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + area,    data = z_usa)) #t = 0.13\n\nThe best four-predictor model under this criterion includes income, population, illiteracy, and murder rate. Continue this process to determine the best four-, five-, six- and seven-predictor models\n\n# Step 5: Fit all five-predictor models that include income, population, illiteracy, and murder_rate\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + hs_grad, data = z_usa)) #t = -0.31\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost,   data = z_usa)) #t = -0.77\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + area,    data = z_usa)) #t = 0.21\n\n# Step 6: Fit all six-predictor models that include income, population, illiteracy, murder_rate, and frost\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + hs_grad, data = z_usa)) #t = -0.12\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + area,    data = z_usa)) #t = 0.62\n\n# Step 7: Fit all seven-predictor models that include income, population, illiteracy, murder_rate, frost, and area\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + area + hs_grad, data = z_usa)) #t = -0.27\n\nAt each stage we could also check to see that all the predictors in the selected model meet our selection criterion that the t-value for all predictors is greater than 1. With this criteria we could have stopped after Stage 4 since not all of the t-values of the best model in Stage 5 were above 1. Based on the forward selection process and the criteria we adopted, we would adopt the best performing model from Stage 4.\n\n# Model adopted from forward selection\ntidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder,  data = z_usa))\n\n# A tibble: 4 × 5\n  term       estimate std.error statistic p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 income        0.418     0.132      3.18 0.00259\n2 population    0.378     0.146      2.60 0.0123 \n3 illiteracy   -0.270     0.149     -1.80 0.0775 \n4 murder       -0.146     0.135     -1.08 0.284  \n\n\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Life~Expectancy}_i} = &0.42(\\mathrm{Income}_i) + 0.38(\\mathrm{Population}_i) - 0.27(\\mathrm{Illiteracy~Rate}_i)\\\\\n&- 0.15(\\mathrm{Murder~Rate}_i)\n\\end{split}\n\\]\nwhere all the variables in the equation are standardized. Note that the p-values are irrelevant as that did not factor into our selection criteria. (In fact, I would not even report them if I was using this selection criterion.)\n\n\n\nBackward Elimination\n\nStep 1: We begin with a model that includes all of the predictors.\nStep 2: We then fit each of the models that include all of the predictors except one, and measure the performance. The predictor that decreases the performance the least is removed from the model.\n\nWe continue this process, at each stage removing the predictor that has the least impact on performance until we get down to an intercept-only model.\n\nIMPORTANT\nOnce a predictor is removed, it is removed in all later stages.\n\nIn our example, we will employ backward elimination to adopt a model using the following performance metric and selection criteria:\n\nMetric of Performance: Select the model with the highest \\(R^2\\) value.\nSelection Criterion: The total \\(R^2\\) value needs to be greater than 0.3.\n\n\n# Step 0: Fit model with all predictors\n# glance(lm(life_expectancy ~ . - 1, data = z_usa))$r.squared #R2 = 0.379\n\n# Step 1: Fit all models with one predictor removed\n# glance(lm(life_expectancy ~ . -1 - population, data = z_usa))$r.squared #R2 = 0.319\n# glance(lm(life_expectancy ~ . -1 - income,     data = z_usa))$r.squared #R2 = 0.264\n# glance(lm(life_expectancy ~ . -1 - illiteracy, data = z_usa))$r.squared #R2 = 0.328\n# glance(lm(life_expectancy ~ . -1 - murder,     data = z_usa))$r.squared #R2 = 0.357\n# glance(lm(life_expectancy ~ . -1 - hs_grad,    data = z_usa))$r.squared #R2 = 0.378\n# glance(lm(life_expectancy ~ . -1 - frost,      data = z_usa))$r.squared #R2 = 0.367\n# glance(lm(life_expectancy ~ . -1 - area,       data = z_usa))$r.squared #R2 = 0.373\n\nAfter Step 1, the model with the highest \\(R^2\\) removes high school graduation rate. Since the \\(R^2\\) value is above our threshold of 0.3, we continue to Step 2.\n\n# Step 2: Fit all models with hs_grad and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - population, data = z_usa))$r.squared #R2 = 0.309\n# glance(lm(life_expectancy ~ . -1 - hs_grad - income,     data = z_usa))$r.squared #R2 = 0.232\n# glance(lm(life_expectancy ~ . -1 - hs_grad - illiteracy, data = z_usa))$r.squared #R2 = 0.324\n# glance(lm(life_expectancy ~ . -1 - hs_grad - murder,     data = z_usa))$r.squared #R2 = 0.352\n# glance(lm(life_expectancy ~ . -1 - hs_grad - frost,      data = z_usa))$r.squared #R2 = 0.366\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area,       data = z_usa))$r.squared #R2 = 0.373\n\nAfter Step 2, the model with the highest \\(R^2\\) removes high school graduation rate and area. Since the \\(R^2\\) value is above our threshold of 0.3, we continue to Step 3. We will continue this process to determine the best four-, three-, two- and one-predictor models. At each stage we should also check to see that the selected model does not decrease the criteria beyond our pre-identified threshold of 0.3.\n\n# Step 3: Fit all models with hs_grad, area, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - population, data = z_usa))$r.squared #R2 = 0.289\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - income,     data = z_usa))$r.squared #R2 = 0.232\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - illiteracy, data = z_usa))$r.squared #R2 = 0.323\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - murder,     data = z_usa))$r.squared #R2 = 0.351\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost,      data = z_usa))$r.squared #R2 = 0.365\n\n# Step 4: Fit all models with hs_grad, area, frost, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - population, data = z_usa))$r.squared #R2 = 0.276\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - income,     data = z_usa))$r.squared #R2 = 0.231\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - illiteracy, data = z_usa))$r.squared #R2 = 0.322\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder,     data = z_usa))$r.squared #R2 = 0.350\n\n# Step 5: Fit all models with hs_grad, area, frost, murder, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - population, data = z_usa))$r.squared #R2 = 0.253\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - income,     data = z_usa))$r.squared #R2 = 0.112\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - illiteracy, data = z_usa))$r.squared #R2 = 0.289\n\nWith this criteria we stop after Step 4 since the \\(R^2\\) value of the best model in Step 5 has an \\(R^2\\) value that is less than 0.3.\n\n# Adopted backward elimination model\ntidy(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder, data = z_usa))\n\n# A tibble: 3 × 5\n  term       estimate std.error statistic  p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 population    0.392     0.145      2.70 0.00944 \n2 income        0.488     0.115      4.23 0.000102\n3 illiteracy   -0.310     0.145     -2.13 0.0379  \n\n\n\\[\n\\widehat{\\mathrm{Life~Expectancy}_i} = 0.39(\\mathrm{Population}_i) + 0.92(\\mathrm{Income}_i) - 0.32(\\mathrm{Illiteracy~Rate}_i)\n\\]\nwhere all the variables in the equation are standardized. Again, the p-values are irrelevant as that did not factor into our selection criteria and should not be reported.\n\n\n\nStepwise Regression\nStepwise regression is based on backward elimination (starting with all predictors and eliminating them one at a time), but the predictors that were removed in earlier steps can be considered for re-entry into the model at later stages. Using the following criteria:\n\nMetric of Performance: Select the predictor with the highest p-value for removal.\nSelection Criterion: Predictors with p-values greater than 0.3 are candidates for elimination; and those with p-values less than 0.1 are candidates for re-entry.\n\nBased on the results of this analysis (not shown), the adopted model is:\n\\[\n\\widehat{\\mathrm{Life~Expectancy}_i} = 0.39(\\mathrm{Population}_i) + 0.49(\\mathrm{Income}_i) - 0.31(\\mathrm{Illiteracy~Rate}_i)\n\\]"
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#using-r-for-automated-model-building",
    "href": "notes/06-01-vintage-methods.html#using-r-for-automated-model-building",
    "title": "Vintage Methods for Model Selection",
    "section": "Using R for Automated Model Building",
    "text": "Using R for Automated Model Building\nFunctions from the {olsrr} package perform automated forward selection, backward elimination, and stepwise regression. The evaluation metrics that these functions use include the p-value, \\(R^2\\), Adjusted \\(R^2\\), AIC, BIC, and SBIC. All of the functions require a lm() object that includes all possible predictors.\nWhile we can’t mimic our forward selection from earlier (the t-value is not a metric we can use with this package), we demonstrate forward selction using the AIC. Variables are added one-at-a-time based on their AIC values. The process terminates when either all the variables have been added or the addition of another variable increases the AIC value.\n\nlibrary(olsrr)\n\n# Forward selection using AIC\nfs_output = ols_step_forward_aic(lm.all, details = TRUE)\n\nForward Selection Method \n------------------------\n\nCandidate Terms: \n\n1. population \n2. income \n3. illiteracy \n4. murder \n5. hs_grad \n6. frost \n7. area \n\n\nStep     =&gt; 0 \nModel    =&gt; life_expectancy ~ 1 \nAIC      =&gt; 209.8146 \n\nInitiating stepwise selection... \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2  \n-----------------------------------------------------------------------\nincome         1    197.033    202.886    49.362    0.24743     0.23238 \nmurder         1    203.710    209.564    55.531    0.14431     0.12720 \nhs_grad        1    205.929    211.783    57.583    0.10702     0.08916 \npopulation     1    209.174    215.028    60.589    0.04952     0.03051 \nfrost          1    210.386    216.240    61.713    0.02710     0.00764 \nilliteracy     1    211.607    217.460    62.846    0.00399    -0.01593 \narea           1    211.668    217.522    62.903    0.00282    -0.01713 \n-----------------------------------------------------------------------\n\nStep     =&gt; 1 \nAdded    =&gt; income \nModel    =&gt; life_expectancy ~ income \nAIC      =&gt; 197.0327 \n\n                     Table: Adding New Variables                       \n----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2 \n----------------------------------------------------------------------\npopulation     1    196.078    203.883    48.699    0.28901    0.25999 \nmurder         1    197.128    204.933    49.627    0.27450    0.24489 \nilliteracy     1    198.680    206.485    51.000    0.25252    0.22201 \narea           1    198.762    206.567    51.073    0.25134    0.22078 \nhs_grad        1    198.813    206.618    51.118    0.25060    0.22002 \nfrost          1    199.032    206.837    51.312    0.24745    0.21673 \n----------------------------------------------------------------------\n\nStep     =&gt; 2 \nAdded    =&gt; population \nModel    =&gt; life_expectancy ~ income + population \nAIC      =&gt; 196.0776 \n\n                     Table: Adding New Variables                       \n----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2 \n----------------------------------------------------------------------\nilliteracy     1    193.457    203.214    46.879    0.34946    0.30880 \nmurder         1    195.610    205.367    48.686    0.32196    0.27958 \nhs_grad        1    195.747    205.504    48.801    0.32017    0.27768 \nfrost          1    197.233    206.989    50.051    0.30046    0.25674 \narea           1    198.012    207.768    50.708    0.28990    0.24552 \n----------------------------------------------------------------------\n\nStep     =&gt; 3 \nAdded    =&gt; illiteracy \nModel    =&gt; life_expectancy ~ income + population + illiteracy \nAIC      =&gt; 193.4573 \n\n                     Table: Adding New Variables                      \n---------------------------------------------------------------------\nPredictor    DF      AIC        SBC       SBIC       R2       Adj. R2 \n---------------------------------------------------------------------\nmurder        1    194.200    205.907    48.138    0.36500    0.31096 \nhs_grad       1    195.238    206.946    48.962    0.35219    0.29706 \nfrost         1    195.324    207.032    49.030    0.35112    0.29589 \narea          1    195.439    207.147    49.121    0.34968    0.29434 \n---------------------------------------------------------------------\n\n\nNo more variables to be added.\n\nVariables Selected: \n\n=&gt; income \n=&gt; population \n=&gt; illiteracy \n\n# View results from adopted model\nfs_output\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    209.815    213.717    61.571    0.00000    0.00000 \n 1      income        197.033    202.886    49.362    0.24743    0.23238 \n 2      population    196.078    203.883    48.699    0.28901    0.25999 \n 3      illiteracy    193.457    203.214    46.879    0.34946    0.30880 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.591       RMSE                 1.412 \nR-Squared               0.349       MSE                  1.994 \nAdj. R-Squared          0.309       Coef. Var            1.867 \nPred R-Squared          0.181       AIC                193.457 \nMAE                     1.120       SBC                203.214 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                              ANOVA                                \n------------------------------------------------------------------\n               Sum of                                             \n              Squares        DF    Mean Square      F        Sig. \n------------------------------------------------------------------\nRegression     55.699         3         18.566    8.595    0.0001 \nResidual      103.689        48          2.160                    \nTotal         159.388        51                                   \n------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    74.975         1.269                 59.104    0.000    72.424    77.525 \n     income     0.162         0.039        0.488     4.186    0.000     0.084     0.240 \n population     0.095         0.036        0.392     2.674    0.010     0.024     0.166 \n illiteracy    -0.127         0.060       -0.310    -2.112    0.040    -0.247    -0.006 \n----------------------------------------------------------------------------------------\n\n\nWe can also plot the AIC values versus each step of the selction process.\n\nplot(fs_output)\n\n\n\n\n\n\n\nFigure 1: Plot of the AIC versus each step in the forward selection process.\n\n\n\n\n\nBackward elimination and stepwise regression operate in a similar manner. For backward elimination the function using AIC would be ols_step_backward_aic(). For stepwise regression the function would be ols_step_both_aic()."
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#some-considerations-in-model-selection",
    "href": "notes/06-01-vintage-methods.html#some-considerations-in-model-selection",
    "title": "Vintage Methods for Model Selection",
    "section": "Some Considerations in Model Selection",
    "text": "Some Considerations in Model Selection\nDiﬀerent model strategies, metrics of model performance, and criteria for model selection lead to diﬀerent “final” models.\nMany statistical programs have functionality that can automate these fitting strategies. Before automating the selection process, it is important to understand the purpose of your model (is it to describe the data? make predictions? inference?). This often guides the choice of performance metrics and model building strategy.\nAlthough these packages can select a model based on some performance metric, there are several problems that automation does not solve:\n\nIt does not address the functional form of the predictors.\nIt does not address interactions.\nIt does not address outliers.\nIt does not address collinearity problems.\n\nMost software will require that you deal with these problems sequentially; first selecting the variables for the model and then determining their functional form, interactons, etc."
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#all-subsets-regression",
    "href": "notes/06-01-vintage-methods.html#all-subsets-regression",
    "title": "Vintage Methods for Model Selection",
    "section": "All Subsets Regression",
    "text": "All Subsets Regression\nAll subsets regression fits all possible k-predictor models. The number of k-predictor models is:\n\\[\n\\mathrm{Number~of~Models} = 2^p - 1\n\\]\nwhere p is the number of predictors in the candidate set.\nThe ols_step_all_possible() function from the {olsrr} package can be used to exhaustively fit a set of models. The function takes an lm() object that includes all potential predictors.\n\n# Fit all subsets of predictors\nall_output = ols_step_all_possible(lm.all)\n\n# Output\nall_output\n\n\n\n   mindex n         predictors     rsquare         adjr     rmse      predrsq\n2       1 1             income 0.247434318  0.232383004 1.518791  0.115564209\n4       2 1             murder 0.144310731  0.127196945 1.619510 -0.158791388\n5       3 1            hs_grad 0.107015741  0.089156056 1.654427 -0.052459371\n1       4 1         population 0.049515821  0.030506137 1.706861 -0.016974597\n6       5 1              frost 0.027098563  0.007640534 1.726872 -0.064805099\n3       6 1         illiteracy 0.003990589 -0.015929599 1.747260 -0.095006514\n7       7 1               area 0.002815354 -0.017128339 1.748290 -0.060143568\n8       8 2  population income 0.289008906  0.259988862 1.476243  0.141506182\n15      9 2      income murder 0.274498977  0.244886691 1.491230 -0.197068157\n14     10 2  income illiteracy 0.252519569  0.222010163 1.513651  0.069707266\n18     11 2        income area 0.251337502  0.220779849 1.514847 -0.024356505\n16     12 2     income hs_grad 0.250603731  0.220016128 1.515589 -0.009461673\n17     13 2       income frost 0.247447777  0.216731360 1.518777  0.062237367\n11     14 2 population hs_grad 0.246290930  0.215527295 1.519944 -0.107847338\n10     15 2  population murder 0.203374938  0.170859629 1.562618 -0.169538055\n23     16 2     murder hs_grad 0.150824219  0.116163983 1.613335 -0.254046736\n25     17 2        murder area 0.150525243  0.115852804 1.613619 -0.213700252\n24     18 2       murder frost 0.144894337  0.109992065 1.618958 -0.207731316\n19     19 2  illiteracy murder 0.144568416  0.109652842 1.619267 -0.327331485\n12     20 2   population frost 0.129021244  0.093471091 1.633915 -0.016957037\n          cp      aic     sbic      sbc     msep      fpe       apc        hsp\n2   5.326316 197.0327 49.36238 202.8864 124.7515 2.491263 0.8127709 0.04895906\n4  12.633586 203.7105 55.53078 209.5642 141.8461 2.832639 0.9241444 0.05566789\n5  15.276285 205.9289 57.58295 211.7826 148.0284 2.956099 0.9644230 0.05809416\n1  19.350692 209.1738 60.58864 215.0276 157.5601 3.146445 1.0265229 0.06183489\n6  20.939164 210.3860 61.71282 216.2398 161.2761 3.220654 1.0507336 0.06329327\n3  22.576580 211.6067 62.84565 217.4604 165.1067 3.297149 1.0756902 0.06479659\n7  22.659856 211.6680 62.90258 217.5217 165.3015 3.301040 1.0769594 0.06487304\n8   4.380367 196.0776 48.69885 203.8826 120.3151 2.446145 0.7980512 0.04818165\n15  5.408531 197.1282 49.62732 204.9331 122.7705 2.496066 0.8143379 0.04916494\n14  6.965978 198.6801 51.00044 206.4851 126.4899 2.571686 0.8390086 0.05065441\n18  7.049738 198.7623 51.07319 206.5673 126.6899 2.575752 0.8403355 0.05073452\n16  7.101733 198.8132 51.11829 206.6182 126.8141 2.578277 0.8411591 0.05078424\n17  7.325362 199.0318 51.31181 206.8368 127.3482 2.589135 0.8447015 0.05099811\n11  7.407335 199.1117 51.38255 206.9166 127.5439 2.593115 0.8460000 0.05107651\n10 10.448335 201.9913 53.93644 209.7963 134.8063 2.740766 0.8941710 0.05398479\n23 14.172044 205.3132 56.89179 213.1181 143.6990 2.921566 0.9531565 0.05754599\n25 14.193230 205.3315 56.90811 213.1364 143.7496 2.922594 0.9534921 0.05756625\n24 14.592232 205.6750 57.21437 213.4800 144.7024 2.941967 0.9598125 0.05794784\n19 14.615326 205.6948 57.23204 213.4998 144.7576 2.943089 0.9601783 0.05796993\n12 15.716989 206.6314 58.06758 214.4364 147.3885 2.996578 0.9776292 0.05902351\n\n\nHere, although there are \\(2^7-1 = 127\\) different models included in the output, we only show the first 20. The actual output is in an object called result. (Calling the names() function on the all_output object returns the associated objects in the output.) We can operate on this to sort/arrange the models based on any of the included metrics, or to add new metrics.\n\n# Add AICc to results\nmodels = all_output$result |&gt;\n  mutate(\n    aic_c = aic + (2 * (n + 2) * (n + 3)) / (nrow(z_usa) - n - 3)\n    )\n\n\n# Order from smallest to largest AICc metric\n# Only show the best 20 models\nmodels |&gt;\n  arrange(aic_c) |&gt;\n  head(20)\n\n   mindex n                                  predictors   rsquare      adjr\n1      29 3                population income illiteracy 0.3494565 0.3087975\n2      64 4         population income illiteracy murder 0.3650011 0.3109586\n3      30 3                    population income murder 0.3219562 0.2795785\n4       8 2                           population income 0.2890089 0.2599889\n5      31 3                   population income hs_grad 0.3201656 0.2776759\n6      65 4        population income illiteracy hs_grad 0.3521897 0.2970569\n7      66 4          population income illiteracy frost 0.3511185 0.2958945\n8      67 4           population income illiteracy area 0.3496834 0.2943373\n9       1 1                                      income 0.2474343 0.2323830\n10      9 2                               income murder 0.2744990 0.2448867\n11     99 5   population income illiteracy murder frost 0.3728339 0.3046637\n12     32 3                     population income frost 0.3004649 0.2567440\n13    100 5 population income illiteracy murder hs_grad 0.3663040 0.2974240\n14    101 5    population income illiteracy murder area 0.3655789 0.2966201\n15     68 4            population income murder hs_grad 0.3277327 0.2705185\n16     33 3                      population income area 0.2899048 0.2455238\n17     69 4               population income murder area 0.3235424 0.2659715\n18     70 4              population income murder frost 0.3232112 0.2656121\n19    102 5  population income illiteracy hs_grad frost 0.3567520 0.2868337\n20     10 2                           income illiteracy 0.2525196 0.2220102\n       rmse     predrsq       cp      aic     sbic      sbc     msep      fpe\n1  1.412095  0.18140210 2.097091 193.4573 46.87937 203.2136 112.4283 2.326347\n2  1.395122 -0.21878421 2.995610 194.1997 48.13834 205.9072 112.1276 2.360486\n3  1.441633 -0.20799197 4.045741 195.6103 48.68559 205.3666 117.1810 2.424689\n4  1.476243  0.14150618 4.380367 196.0776 48.69885 203.8826 120.3151 2.446145\n5  1.443535 -0.08731959 4.172624 195.7475 48.80085 205.5037 117.4905 2.431092\n6  1.409125 -0.32689734 3.903416 195.2384 48.96165 206.9459 114.3898 2.408110\n7  1.410290  0.09436707 3.979323 195.3243 49.02986 207.0318 114.5790 2.412092\n8  1.411849  0.12287005 4.081014 195.4392 49.12107 207.1467 114.8324 2.417427\n9  1.518791  0.11556421 5.326316 197.0327 49.36238 202.8864 124.7515 2.491263\n10 1.491230 -0.19706816 5.408531 197.1282 49.62732 204.9331 122.7705 2.496066\n11 1.386491 -0.29086986 4.440581 195.5543 49.96805 209.2130 113.2055 2.423841\n12 1.464301  0.06437984 5.568600 197.2329 50.05093 206.9892 120.8952 2.501542\n13 1.393690 -0.63087784 4.903285 196.0929 50.36989 209.7516 114.3841 2.449078\n14 1.394487 -0.31988118 4.954666 196.1524 50.41431 209.8111 114.5150 2.451880\n15 1.435479 -0.32449344 5.636424 197.1654 50.49506 208.8729 118.7084 2.499024\n16 1.475312  0.06738309 6.316887 198.0121 50.70783 207.7683 122.7202 2.539305\n17 1.439945 -0.28300400 5.933347 197.4885 50.75294 209.1960 119.4483 2.514601\n18 1.440298 -0.28188046 5.956817 197.5140 50.77326 209.2215 119.5068 2.515832\n19 1.404155 -0.44044201 5.580135 196.8709 50.95178 210.5296 116.1083 2.485994\n20 1.513651  0.06970727 6.965978 198.6801 51.00044 206.4851 126.4899 2.571686\n         apc        hsp    aic_c\n1  0.7589674 0.04596127 194.7617\n2  0.7701051 0.04681360 196.0664\n3  0.7910511 0.04790418 196.9147\n4  0.7980512 0.04818165 196.9287\n5  0.7931401 0.04803069 197.0518\n6  0.7856423 0.04775809 197.1051\n7  0.7869414 0.04783706 197.1910\n8  0.7886819 0.04794286 197.3059\n9  0.8127709 0.04895906 197.5327\n10 0.8143379 0.04916494 197.9792\n11 0.7907746 0.04829109 198.0998\n12 0.8161242 0.04942255 198.5373\n13 0.7990080 0.04879388 198.6384\n14 0.7999222 0.04884972 198.6978\n15 0.8153029 0.04956112 199.0321\n16 0.8284444 0.05016863 199.3164\n17 0.8203848 0.04987004 199.3552\n18 0.8207864 0.04989445 199.3807\n19 0.8110518 0.04952938 199.4163\n20 0.8390086 0.05065441 199.5312\n\n\nThe model with the smallest AICc includes the predictors population, income, and illiteracy rate. However, there are several models that have an AICc value within 4 of the minimum AICc. All of these models are also plausible given the data and candidate set of 127 models.\n\n# Get models within 4 of minimum AICc\nmodels |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  filter(aic_c - min(aic_c) &lt; 4) |&gt;\n  arrange(aic_c)\n\n   mindex n                                  predictors    aic_c\n1      29 3                population income illiteracy 194.7617\n2      64 4         population income illiteracy murder 196.0664\n3      30 3                    population income murder 196.9147\n4       8 2                           population income 196.9287\n5      31 3                   population income hs_grad 197.0518\n6      65 4        population income illiteracy hs_grad 197.1051\n7      66 4          population income illiteracy frost 197.1910\n8      67 4           population income illiteracy area 197.3059\n9       1 1                                      income 197.5327\n10      9 2                               income murder 197.9792\n11     99 5   population income illiteracy murder frost 198.0998\n12     32 3                     population income frost 198.5373\n13    100 5 population income illiteracy murder hs_grad 198.6384\n14    101 5    population income illiteracy murder area 198.6978\n\n\nYou could also examine the best 1-predictor, 2-predictor, 3-predictor, etc. models.\n\n# Get best k-predictor models\nmodels |&gt;\n  group_by(n) |&gt;\n  filter(aic_c == min(aic_c)) |&gt;\n  ungroup() |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  arrange(aic_c)\n\n# A tibble: 7 × 4\n  mindex     n predictors                                             aic_c\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n1     29     3 population income illiteracy                            195.\n2     64     4 population income illiteracy murder                     196.\n3      8     2 population income                                       197.\n4      1     1 income                                                  198.\n5     99     5 population income illiteracy murder frost               198.\n6    120     6 population income illiteracy murder frost area          200.\n7    127     7 population income illiteracy murder hs_grad frost area  203.\n\n\nIt can be useful to examine the predictors from the best models. (Here I do that in a plot, but it could also be done in a table.) This can help identify substantively important predictors.\n\n# Get models within 4 of minimum AICc\nplausible = models |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  filter(aic_c - min(aic_c) &lt; 4) |&gt;\n  arrange(aic_c)\n\n# Load library for labeling\nlibrary(ggrepel)\n\n# Plot the models\nggplot(data = plausible, aes(x = as.numeric(rownames(plausible)), y = aic_c)) +\n  geom_line(group = 1) +\n  geom_point() +\n  geom_label_repel(aes(label = predictors), size = 3) +\n  theme_bw() +\n  scale_x_continuous(name = \"Ten Best Models\", breaks = 1:14) +\n  ylab(\"AICc\")\n\n\n\n\n\n\n\nFigure 2: Fourteen plausible models and their predictors based on their AICc values.\n\n\n\n\n\nThe models with the lowest AICc values all seem to include population and income. Many also include illiteracy rate."
  },
  {
    "objectID": "notes/06-01-vintage-methods.html#problems",
    "href": "notes/06-01-vintage-methods.html#problems",
    "title": "Vintage Methods for Model Selection",
    "section": "99 Problems…",
    "text": "99 Problems…\nAutomated methods of model selection has a host of critics and issues. In simulation studies, even when the true set of predictors are included in the subset of regression models, automated strategies may not identify these true predictors in the best models (Miller, 2002). The essential problems with automated selection methods have been summarized by Harrell (2001):\n\n\\(R^2\\) values are biased high;\nThe F-statistics are not F-distributed;\nThe standard errors of the parameter estimates are too small;\n\nConsequently, the confidence intervals around the parameter estimates are too narrow.\n\np-values are too low, due to multiple comparisons, and are diﬃcult to correct;\nParameter estimates are biased away from 0; and\nCollinearity problems are exacerbated.\n\nIn sum, the parameter estimates are likely to be too far away from zero; the variance estimates for those parameter estimates are not correct; which implies that the confidence intervals and hypothesis tests will be wrong; and there are no reasonable ways of correcting these problems! In general the evidence around using automated selection methods is that these methods are subpar. If you need to use these methods backward elimination seems to be the best method to use. (Stepwise regression consistently performs the worst.) Also, information criteria (AIC, BIC) seems to be the best criterion when using these methods.\n\nSUMMATION OF AUTOMATED METHODS\nFlom (2018) writes, “Most devastatingly, it allows the analyst not to think. Put in another way, for a data analyst to use stepwise methods is equivalent to telling his or her boss that his or her salary should be cut.”"
  },
  {
    "objectID": "notes/06-01-automated-methods.html",
    "href": "notes/06-01-automated-methods.html",
    "title": "Automated Methods for Model Selection",
    "section": "",
    "text": "In a previous activity, we examined predictors of average life expectancy in a state, using data from states-2019.csv. In this set of notes, we will examine several “vintage” methods for model selection. These methods are often used when researchers have no a priori hypotheses about the data.\n# Load libraries\nlibrary(modelr)\nlibrary(patchwork)\nlibrary(tidyverse)\nlibrary(tidymodels) # Loads broom, rsample, parsnip, recipes, workflow, tune, yardstick, and dials\n\n# Import and view data\nusa = read_csv(\"https://raw.githubusercontent.com/zief0002/symmetrical-fiesta/main/data/states-2019.csv\")\n\n# View data\nusa\n\n# A tibble: 52 × 9\n   state       life_expectancy population income illiteracy murder hs_grad frost\n   &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama                75.4      4.90    23.6       14.8    7.8    85.3  42.8\n 2 Alaska                 78.8      0.732   33.1        9.2    8.2    92.4 200. \n 3 Arizona                79.9      7.28    25.7       13.1    5.1    86.5  90.8\n 4 Arkansas               75.9      3.02    22.9       13.7    7.2    85.6  62.5\n 5 California             81.6     39.5     30.4       23.1    4.4    82.5  27.5\n 6 Colorado               80.5      5.76    32.4        9.9    3.7    91.1 168. \n 7 Connecticut            80.9      3.57    39.4        8.6    2.3    90.2  88.5\n 8 Delaware               78.4      0.974   30.5       10.7    5      89.3 114  \n 9 District o…            78.6      0.706   45.9       19      5.9    90.3  97  \n10 Florida                80       21.5     26.6       19.7    5.2    87.6   7.1\n# ℹ 42 more rows\n# ℹ 1 more variable: area &lt;dbl&gt;\nIn all of our previous courses and notes, predictors have been selected a priori, based on substantive literature and theory. Subsequently, most of our analytic work has focused on:\nWhen theory does not specify the predictor set, variable (model) selection becomes an important analytical problem! To consider this, let’s examine the how we might make decisions about potential predictors:"
  },
  {
    "objectID": "notes/06-01-automated-methods.html#purpose-of-the-model-and-analytic-goal",
    "href": "notes/06-01-automated-methods.html#purpose-of-the-model-and-analytic-goal",
    "title": "Automated Methods for Model Selection",
    "section": "Purpose of the Model and Analytic Goal",
    "text": "Purpose of the Model and Analytic Goal\nNote that there are many decisions to be made in this process. One thing that can help guide some of these decisions is the purpose for modeling and the related analytic goal. For example:\n\nDescription\n\nPurpose of the model is to describe the data, or to understand a complex system.\nAnalytic goal could be to choose the smallest number of predictors that account for a substantial amount of variation in the outcome.\nNote that this analytic goal may hve competing requirements since explaining more variation in the outcome generally requires more predictors\n\nInference/Prediction\n\nPurpose of the model is to predict the outcome/mean outcome for new cases or make inferences about the eﬀects of predictors\nHere the analytic goal is prediction/inferential accuracy.\nBecause of this, performance in our sample is not as important as performance in future (out-of-sample) cases\n\n\n\n\nModel Evaluation Criteria\nThe model’s purpose determines how we will measure and evaluate “model success”. Each purpose points to a diﬀerent criteria to use in the model evaluation process.\nThere are several criteria that have been proposed to evaluate model performance when the purpose is description.\n\nSum of Squared Residuals: \\(\\mathrm{SSE} = \\sum(Y_i - \\hat{Y}_i)^2\\)\nResidual Mean Square: \\(\\mathrm{RMS} = \\frac{\\mathrm{SSE}}{\\mathrm{df}_{\\mathrm{Residual}}}\\)\n\nWhen using criteria that measure the residual error (SSE, RMS), we want to select a model that minimizes these values.\n\n\\(\\mathbf{R}^2\\): \\(R^2 = \\frac{\\mathrm{SST}-\\mathrm{SSE}}{\\mathrm{SST}}\\)\nAdjusted \\(\\mathbf{R}^2\\): \\(R^2_{Adj} = \\frac{\\mathrm{SST}-\\mathrm{SSE}}{\\mathrm{SST}} \\times \\frac{\\mathrm{df}_{\\mathrm{Total}}}{\\mathrm{df}_{\\mathrm{Residual}}}\\)\n\nWhen using an \\(R^2\\) value to evaluate model performance, we want to select a model that maximizes these values. The adjusted \\(R^2\\) value penalizes the \\(R^2\\) value for model complexity, so when the number of predictors varies across the models, this is a better criterion.\nThe criteria that have been proposed to evaluate model performance when the purpose is prediction/inference focus on measuring out-of-sample performance. (Note that in each of the following formulae, k is the total number of parameters being estimated, including the residual variance.)\n\nt-value/p-value\n\nNote that these criteria are at the predictor-level; not the model-level. Using the maximum p-value or minimum t-value can make this a model-level metric.\n\nMallow’s Cp: \\(C_p=\\frac{\\mathrm{SSE}}{\\hat{\\sigma^2_{\\mathrm{Residual (Full)}}}}+2k-n\\)\n\nMallow’s Cp is an estimate of the average mean squared error of prediction. The Cp value should be similar to the number of predictors in the model.\n\nAIC: \\(\\mathrm{AIC} = n \\times \\ln(\\frac{\\mathrm{SSE}}{n}) + 2k\\)\nCorrected AIC: \\(\\mathrm{AICc} = \\mathrm{AIC} + \\frac{2(k+2)(k+3)}{n-k-3}\\)\n\nAIC has a penalty for model complexity. It must be computed on the same set of observations; no missing data. Corrected AIC has been found to perform better than AIC.\n\nBIC: \\(\\mathrm{BIC} = n \\times \\ln(\\frac{\\mathrm{SSE}}{n}) + k \\bigg(\\ln(n)\\bigg)\\)\n\nBIC has a larger penalty term than AIC, which is based on sample size and model complexity. It performs best when the “true” model is among the candidate models."
  },
  {
    "objectID": "notes/06-01-automated-methods.html#back-to-example",
    "href": "notes/06-01-automated-methods.html#back-to-example",
    "title": "Automated Methods for Model Selection",
    "section": "Back to Example",
    "text": "Back to Example\nOur modeling goal is to explore the predictors of life expectancy. Because we have no a priori hypotheses about which predictors should be included in the model nor about the importance of these predictors, one method is to put all of the predictors in the model immediately. Before we do this, we will create a data frame of the usa data that will be useful in our modeling by removing state name—this leaves only the outcome and predictors in the data.\n\n# Create data frame that includes all rows/columns except the state names\nusa2 = usa |&gt;\n  select(-state)\n\n# View data\nhead(usa2)\n\n# A tibble: 6 × 8\n  life_expectancy population income illiteracy murder hs_grad frost  area\n            &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1            75.4      4.90    23.6       14.8    7.8    85.3  42.8  5.08\n2            78.8      0.732   33.1        9.2    8.2    92.4 200.  57.1 \n3            79.9      7.28    25.7       13.1    5.1    86.5  90.8 11.4 \n4            75.9      3.02    22.9       13.7    7.2    85.6  62.5  5.21\n5            81.6     39.5     30.4       23.1    4.4    82.5  27.5 15.6 \n6            80.5      5.76    32.4        9.9    3.7    91.1 168.  10.4 \n\n# Use all variables as predictors\nlm.all = lm(life_expectancy ~ ., data = usa2)\n\n# Examine output\ntidy(lm.all)\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept) 79.9      12.1         6.61  0.0000000426\n2 population   0.0806    0.0391      2.06  0.0451      \n3 income       0.160     0.0562      2.85  0.00661     \n4 illiteracy  -0.158     0.0829     -1.90  0.0639      \n5 murder      -0.130     0.106      -1.23  0.226       \n6 hs_grad     -0.0364    0.137      -0.265 0.792       \n7 frost       -0.00622   0.00694    -0.897 0.375       \n8 area         0.0189    0.0290      0.653 0.517       \n\n\nBased on the coefficient-level output, we find that:\n\nPopulation, area, and income are positively related to life expectancy;\nIlliteracy rate, murder rate, days with a temperature below freezing, and graduation rate are negatively related to life expectancy;\n\nAlso,\n\nOnly population, income, and maybe illiteracy rate are statistically significant at the .05-level.\n\n\n\nComputing Model Evaluation Criteria\nHere we compute the different model evaluation criteria as an example. In practice, you would decide on the criterion you will use prior to any analysis being undertaken.\n\n# Descriptive: Compute SSE and RMS\nanova(lm.all)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npopulation  1  7.892   7.892  3.5087 0.0677015 .  \nincome      1 38.172  38.172 16.9703 0.0001648 ***\nilliteracy  1  9.635   9.635  4.2833 0.0443921 *  \nmurder      1  2.478   2.478  1.1015 0.2996718    \nhs_grad     1  0.208   0.208  0.0923 0.7626738    \nfrost       1  1.072   1.072  0.4765 0.4936514    \narea        1  0.960   0.960  0.4268 0.5169551    \nResiduals  44 98.972   2.249                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSSE = 31.668\nRMS = 0.7197\n\n# Descriptive: Compute R2 and adj. R2\nglance(lm.all)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.379         0.280  1.50      3.84 0.00245     7  -90.5  199.  217.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nR2 = 0.379\nAdjR2 = 0.280\n\n\n# Assign values for k and n\nk = 8 \nn = 52\n\n# Inference: Compute maximum t-value\nmax(tidy(lm.all)$statistic)\n\n[1] 6.610782\n\n# Inference: Compute minimum p-value\nmin(tidy(lm.all)$p.value)\n\n[1] 0.00000004262328\n\n# Inference: Compute Mallow's Cp\nSSE / RMS + 2 * k - n\n\n[1] 8.001667\n\n# Inference: AIC\naic_mod = n * log(SSE / n) + 2 * k\naic_mod\n\n[1] -9.788725\n\n# Inference: AICc\naic_mod + (2 * (k + 2) * (k + 3)) / (n - k - 3)\n\n[1] -4.422871\n\n# Inference: BIC\nn * log(SSE / n) + k * log(n)\n\n[1] 5.821225\n\n\nAnother consideration is the usefulness of these criteria. For example the maximum t-value, minimum p-value, SSE, RMS, \\(R^2\\), and adjusted \\(R^2\\) values are useful for both summarizing a single model and for comparing models. Whereas, Mallow’s Cp, and the information criteria metrics are only useful for comparing models; not for summarizing a single model."
  },
  {
    "objectID": "notes/06-01-automated-methods.html#model-building-strategy",
    "href": "notes/06-01-automated-methods.html#model-building-strategy",
    "title": "Automated Methods for Model Selection",
    "section": "Model Building Strategy",
    "text": "Model Building Strategy\nOnce you determine the criteria/metric you will use to measure the performance of each model, you need to outline your model building strategy. Three common strategies for model building are: forward selection, backward elimination, and stepwise. Note that again, we assume that there is no a priori hypotheses about the importance of the predictors.\nBefore we begin, we will standardize all the variables in the analysis.\n\n# Create standardized variables after removing state names\nz_usa = usa |&gt;\n  select(-state) |&gt;\n  scale(center = TRUE, scale = TRUE) |&gt;\n  data.frame()\n\n# View data\nhead(z_usa)\n\n  life_expectancy  population     income illiteracy      murder    hs_grad\n1     -1.87539008 -0.20138748 -0.9151339  0.7050017  0.90297951 -0.9556270\n2      0.04786378 -0.77268875  0.8656837 -0.5894276  1.02638276  1.0873231\n3      0.67009298  0.12393888 -0.5179528  0.3120499  0.07000761 -0.6103396\n4     -1.59255863 -0.45958823 -1.0512942  0.4507388  0.71787464 -0.8693051\n5      1.63171991  4.53828023  0.3720793  2.6235307 -0.14594807 -1.7612974\n6      1.00949072 -0.08422076  0.7329133 -0.4276240 -0.36190375  0.7132618\n       frost       area\n1 -1.2033782 -0.2033181\n2  1.7777726  5.8988618\n3 -0.2913627  0.5348718\n4 -0.8290718 -0.1877660\n5 -1.4940832  1.0317286\n6  1.1678622  0.4185305\n\n\n\n\nForward Selection\n\nStep 1: We fit each of the one-predictor models and measure the performance of each model using the criteria/metric chosen. The predictor from the model that has the best performance is retained.\nStep 2: We then fit each of the two-predictor models that can be fitted with the predictor retained in Step 1. The predictors from the model that has the best performance are retained.\n\nWe continue this process until we have either (a) fitted a model with all the predictors, or (b) hit some stopping/selection criteria that we have identified (e.g., stop once one of the p-values is greater than 0.05).\n\nIMPORTANT\nOnce a predictor is retained in forward selection, it is always included in all later stages.\n\nIn our example, we will employ forward selection to adopt a model using the following performance metric and selection criteria:\n\nMetric of Performance: Select the predictor with the highest t-value (absolute value).\nSelection Criterion: All t-values for predictors in the model need to be greater than 1.\n\n\n# Step 1: Fit all one-predictor models\n# tidy(lm(life_expectancy ~ -1 + population, data = z_usa)) #t = 1.63\n# tidy(lm(life_expectancy ~ -1 + income,     data = z_usa)) #t = 4.09\n# tidy(lm(life_expectancy ~ -1 + illiteracy, data = z_usa)) #t = -0.45\n# tidy(lm(life_expectancy ~ -1 + murder,     data = z_usa)) #t = -2.93\n# tidy(lm(life_expectancy ~ -1 + hs_grad,    data = z_usa)) #t = 2.47\n# tidy(lm(life_expectancy ~ -1 + frost,      data = z_usa)) #t = 1.19\n# tidy(lm(life_expectancy ~ -1 + area,       data = z_usa)) #t = 0.38\n\nThe best one-predictor model under this criterion includes income.\n\n# Step 2: Fit all two-predictor models that include income\n# tidy(lm(life_expectancy ~ -1 + income + population, data = z_usa)) #t = 1.71\n# tidy(lm(life_expectancy ~ -1 + income + illiteracy, data = z_usa)) #t = -0.58\n# tidy(lm(life_expectancy ~ -1 + income + murder,     data = z_usa)) #t = -1.37\n# tidy(lm(life_expectancy ~ -1 + income + hs_grad,    data = z_usa)) #t = 0.46\n# tidy(lm(life_expectancy ~ -1 + income + frost,      data = z_usa)) #t = 0.03\n# tidy(lm(life_expectancy ~ -1 + income + area,       data = z_usa)) #t = 0.51\n\nThe best two-predictor model under this criterion includes income and population.\n\n# Step 3: Fit all three-predictor models that include income and population\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy, data = z_usa)) #t = -2.13\n# tidy(lm(life_expectancy ~ -1 + income + population + murder,     data = z_usa)) #t = -1.54\n# tidy(lm(life_expectancy ~ -1 + income + population + hs_grad,    data = z_usa)) #t = 1.50\n# tidy(lm(life_expectancy ~ -1 + income + population + frost,      data = z_usa)) #t = 0.90\n# tidy(lm(life_expectancy ~ -1 + income + population + area,       data = z_usa)) #t = 0.25\n\nThe best three-predictor model under this criterion includes income, population, and illiteracy.\n\n# Step 4: Fit all four-predictor models that include income, population, and illiteracy\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder,  data = z_usa)) #t = -1.08\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + hs_grad, data = z_usa)) #t = 0.45\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + frost,   data = z_usa)) #t = -0.35\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + area,    data = z_usa)) #t = 0.13\n\nThe best four-predictor model under this criterion includes income, population, illiteracy, and murder rate. Continue this process to determine the best four-, five-, six- and seven-predictor models\n\n# Step 5: Fit all five-predictor models that include income, population, illiteracy, and murder_rate\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + hs_grad, data = z_usa)) #t = -0.31\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost,   data = z_usa)) #t = -0.77\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + area,    data = z_usa)) #t = 0.21\n\n# Step 6: Fit all six-predictor models that include income, population, illiteracy, murder_rate, and frost\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + hs_grad, data = z_usa)) #t = -0.12\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + area,    data = z_usa)) #t = 0.62\n\n# Step 7: Fit all seven-predictor models that include income, population, illiteracy, murder_rate, frost, and area\n# tidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder + frost + area + hs_grad, data = z_usa)) #t = -0.27\n\nAt each stage we could also check to see that all the predictors in the selected model meet our selection criterion that the t-value for all predictors is greater than 1. With this criteria we could have stopped after Stage 4 since not all of the t-values of the best model in Stage 5 were above 1. Based on the forward selection process and the criteria we adopted, we would adopt the best performing model from Stage 4.\n\n# Model adopted from forward selection\ntidy(lm(life_expectancy ~ -1 + income + population + illiteracy + murder,  data = z_usa))\n\n# A tibble: 4 × 5\n  term       estimate std.error statistic p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 income        0.418     0.132      3.18 0.00259\n2 population    0.378     0.146      2.60 0.0123 \n3 illiteracy   -0.270     0.149     -1.80 0.0775 \n4 murder       -0.146     0.135     -1.08 0.284  \n\n\n\\[\n\\begin{split}\n\\widehat{\\mathrm{Life~Expectancy}_i} = &0.42(\\mathrm{Income}_i) + 0.38(\\mathrm{Population}_i) - 0.27(\\mathrm{Illiteracy~Rate}_i)\\\\\n&- 0.15(\\mathrm{Murder~Rate}_i)\n\\end{split}\n\\]\nwhere all the variables in the equation are standardized. Note that the p-values are irrelevant as that did not factor into our selection criteria. (In fact, I would not even report them if I was using this selection criterion.)\n\n\n\nBackward Elimination\n\nStep 1: We begin with a model that includes all of the predictors.\nStep 2: We then fit each of the models that include all of the predictors except one, and measure the performance. The predictor that decreases the performance the least is removed from the model.\n\nWe continue this process, at each stage removing the predictor that has the least impact on performance until we get down to an intercept-only model.\n\nIMPORTANT\nOnce a predictor is removed, it is removed in all later stages.\n\nIn our example, we will employ backward elimination to adopt a model using the following performance metric and selection criteria:\n\nMetric of Performance: Select the model with the highest \\(R^2\\) value.\nSelection Criterion: The total \\(R^2\\) value needs to be greater than 0.3.\n\n\n# Step 0: Fit model with all predictors\n# glance(lm(life_expectancy ~ . - 1, data = z_usa))$r.squared #R2 = 0.379\n\n# Step 1: Fit all models with one predictor removed\n# glance(lm(life_expectancy ~ . -1 - population, data = z_usa))$r.squared #R2 = 0.319\n# glance(lm(life_expectancy ~ . -1 - income,     data = z_usa))$r.squared #R2 = 0.264\n# glance(lm(life_expectancy ~ . -1 - illiteracy, data = z_usa))$r.squared #R2 = 0.328\n# glance(lm(life_expectancy ~ . -1 - murder,     data = z_usa))$r.squared #R2 = 0.357\n# glance(lm(life_expectancy ~ . -1 - hs_grad,    data = z_usa))$r.squared #R2 = 0.378\n# glance(lm(life_expectancy ~ . -1 - frost,      data = z_usa))$r.squared #R2 = 0.367\n# glance(lm(life_expectancy ~ . -1 - area,       data = z_usa))$r.squared #R2 = 0.373\n\nAfter Step 1, the model with the highest \\(R^2\\) removes high school graduation rate. Since the \\(R^2\\) value is above our threshold of 0.3, we continue to Step 2.\n\n# Step 2: Fit all models with hs_grad and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - population, data = z_usa))$r.squared #R2 = 0.309\n# glance(lm(life_expectancy ~ . -1 - hs_grad - income,     data = z_usa))$r.squared #R2 = 0.232\n# glance(lm(life_expectancy ~ . -1 - hs_grad - illiteracy, data = z_usa))$r.squared #R2 = 0.324\n# glance(lm(life_expectancy ~ . -1 - hs_grad - murder,     data = z_usa))$r.squared #R2 = 0.352\n# glance(lm(life_expectancy ~ . -1 - hs_grad - frost,      data = z_usa))$r.squared #R2 = 0.366\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area,       data = z_usa))$r.squared #R2 = 0.373\n\nAfter Step 2, the model with the highest \\(R^2\\) removes high school graduation rate and area. Since the \\(R^2\\) value is above our threshold of 0.3, we continue to Step 3. We will continue this process to determine the best four-, three-, two- and one-predictor models. At each stage we should also check to see that the selected model does not decrease the criteria beyond our pre-identified threshold of 0.3.\n\n# Step 3: Fit all models with hs_grad, area, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - population, data = z_usa))$r.squared #R2 = 0.289\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - income,     data = z_usa))$r.squared #R2 = 0.232\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - illiteracy, data = z_usa))$r.squared #R2 = 0.323\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - murder,     data = z_usa))$r.squared #R2 = 0.351\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost,      data = z_usa))$r.squared #R2 = 0.365\n\n# Step 4: Fit all models with hs_grad, area, frost, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - population, data = z_usa))$r.squared #R2 = 0.276\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - income,     data = z_usa))$r.squared #R2 = 0.231\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - illiteracy, data = z_usa))$r.squared #R2 = 0.322\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder,     data = z_usa))$r.squared #R2 = 0.350\n\n# Step 5: Fit all models with hs_grad, area, frost, murder, and one other predictor removed\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - population, data = z_usa))$r.squared #R2 = 0.253\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - income,     data = z_usa))$r.squared #R2 = 0.112\n# glance(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder - illiteracy, data = z_usa))$r.squared #R2 = 0.289\n\nWith this criteria we stop after Step 4 since the \\(R^2\\) value of the best model in Step 5 has an \\(R^2\\) value that is less than 0.3.\n\n# Adopted backward elimination model\ntidy(lm(life_expectancy ~ . -1 - hs_grad - area - frost - murder, data = z_usa))\n\n# A tibble: 3 × 5\n  term       estimate std.error statistic  p.value\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 population    0.392     0.145      2.70 0.00944 \n2 income        0.488     0.115      4.23 0.000102\n3 illiteracy   -0.310     0.145     -2.13 0.0379  \n\n\n\\[\n\\widehat{\\mathrm{Life~Expectancy}_i} = 0.39(\\mathrm{Population}_i) + 0.92(\\mathrm{Income}_i) - 0.32(\\mathrm{Illiteracy~Rate}_i)\n\\]\nwhere all the variables in the equation are standardized. Again, the p-values are irrelevant as that did not factor into our selection criteria and should not be reported.\n\n\n\nStepwise Regression\nStepwise regression is based on backward elimination (starting with all predictors and eliminating them one at a time), but the predictors that were removed in earlier steps can be considered for re-entry into the model at later stages. Using the following criteria:\n\nMetric of Performance: Select the predictor with the highest p-value for removal.\nSelection Criterion: Predictors with p-values greater than 0.3 are candidates for elimination; and those with p-values less than 0.1 are candidates for re-entry.\n\nBased on the results of this analysis (not shown), the adopted model is:\n\\[\n\\widehat{\\mathrm{Life~Expectancy}_i} = 0.39(\\mathrm{Population}_i) + 0.49(\\mathrm{Income}_i) - 0.31(\\mathrm{Illiteracy~Rate}_i)\n\\]"
  },
  {
    "objectID": "notes/06-01-automated-methods.html#using-r-for-automated-model-building",
    "href": "notes/06-01-automated-methods.html#using-r-for-automated-model-building",
    "title": "Automated Methods for Model Selection",
    "section": "Using R for Automated Model Building",
    "text": "Using R for Automated Model Building\nFunctions from the {olsrr} package perform automated forward selection, backward elimination, and stepwise regression. The evaluation metrics that these functions use include the p-value, \\(R^2\\), Adjusted \\(R^2\\), AIC, BIC, and SBIC. All of the functions require a lm() object that includes all possible predictors.\nWhile we can’t mimic our forward selection from earlier (the t-value is not a metric we can use with this package), we demonstrate forward selction using the AIC. Variables are added one-at-a-time based on their AIC values. The process terminates when either all the variables have been added or the addition of another variable increases the AIC value.\n\nlibrary(olsrr)\n\n# Forward selection using AIC\nfs_output = ols_step_forward_aic(lm.all, details = TRUE)\n\nForward Selection Method \n------------------------\n\nCandidate Terms: \n\n1. population \n2. income \n3. illiteracy \n4. murder \n5. hs_grad \n6. frost \n7. area \n\n\nStep     =&gt; 0 \nModel    =&gt; life_expectancy ~ 1 \nAIC      =&gt; 209.8146 \n\nInitiating stepwise selection... \n\n                      Table: Adding New Variables                       \n-----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2  \n-----------------------------------------------------------------------\nincome         1    197.033    202.886    49.362    0.24743     0.23238 \nmurder         1    203.710    209.564    55.531    0.14431     0.12720 \nhs_grad        1    205.929    211.783    57.583    0.10702     0.08916 \npopulation     1    209.174    215.028    60.589    0.04952     0.03051 \nfrost          1    210.386    216.240    61.713    0.02710     0.00764 \nilliteracy     1    211.607    217.460    62.846    0.00399    -0.01593 \narea           1    211.668    217.522    62.903    0.00282    -0.01713 \n-----------------------------------------------------------------------\n\nStep     =&gt; 1 \nAdded    =&gt; income \nModel    =&gt; life_expectancy ~ income \nAIC      =&gt; 197.0327 \n\n                     Table: Adding New Variables                       \n----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2 \n----------------------------------------------------------------------\npopulation     1    196.078    203.883    48.699    0.28901    0.25999 \nmurder         1    197.128    204.933    49.627    0.27450    0.24489 \nilliteracy     1    198.680    206.485    51.000    0.25252    0.22201 \narea           1    198.762    206.567    51.073    0.25134    0.22078 \nhs_grad        1    198.813    206.618    51.118    0.25060    0.22002 \nfrost          1    199.032    206.837    51.312    0.24745    0.21673 \n----------------------------------------------------------------------\n\nStep     =&gt; 2 \nAdded    =&gt; population \nModel    =&gt; life_expectancy ~ income + population \nAIC      =&gt; 196.0776 \n\n                     Table: Adding New Variables                       \n----------------------------------------------------------------------\nPredictor     DF      AIC        SBC       SBIC       R2       Adj. R2 \n----------------------------------------------------------------------\nilliteracy     1    193.457    203.214    46.879    0.34946    0.30880 \nmurder         1    195.610    205.367    48.686    0.32196    0.27958 \nhs_grad        1    195.747    205.504    48.801    0.32017    0.27768 \nfrost          1    197.233    206.989    50.051    0.30046    0.25674 \narea           1    198.012    207.768    50.708    0.28990    0.24552 \n----------------------------------------------------------------------\n\nStep     =&gt; 3 \nAdded    =&gt; illiteracy \nModel    =&gt; life_expectancy ~ income + population + illiteracy \nAIC      =&gt; 193.4573 \n\n                     Table: Adding New Variables                      \n---------------------------------------------------------------------\nPredictor    DF      AIC        SBC       SBIC       R2       Adj. R2 \n---------------------------------------------------------------------\nmurder        1    194.200    205.907    48.138    0.36500    0.31096 \nhs_grad       1    195.238    206.946    48.962    0.35219    0.29706 \nfrost         1    195.324    207.032    49.030    0.35112    0.29589 \narea          1    195.439    207.147    49.121    0.34968    0.29434 \n---------------------------------------------------------------------\n\n\nNo more variables to be added.\n\nVariables Selected: \n\n=&gt; income \n=&gt; population \n=&gt; illiteracy \n\n# View results from adopted model\nfs_output\n\n\n                             Stepwise Summary                             \n------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n------------------------------------------------------------------------\n 0      Base Model    209.815    213.717    61.571    0.00000    0.00000 \n 1      income        197.033    202.886    49.362    0.24743    0.23238 \n 2      population    196.078    203.883    48.699    0.28901    0.25999 \n 3      illiteracy    193.457    203.214    46.879    0.34946    0.30880 \n------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.591       RMSE                 1.412 \nR-Squared               0.349       MSE                  1.994 \nAdj. R-Squared          0.309       Coef. Var            1.867 \nPred R-Squared          0.181       AIC                193.457 \nMAE                     1.120       SBC                203.214 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                              ANOVA                                \n------------------------------------------------------------------\n               Sum of                                             \n              Squares        DF    Mean Square      F        Sig. \n------------------------------------------------------------------\nRegression     55.699         3         18.566    8.595    0.0001 \nResidual      103.689        48          2.160                    \nTotal         159.388        51                                   \n------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)    74.975         1.269                 59.104    0.000    72.424    77.525 \n     income     0.162         0.039        0.488     4.186    0.000     0.084     0.240 \n population     0.095         0.036        0.392     2.674    0.010     0.024     0.166 \n illiteracy    -0.127         0.060       -0.310    -2.112    0.040    -0.247    -0.006 \n----------------------------------------------------------------------------------------\n\n\nWe can also plot the AIC values versus each step of the selction process.\n\nplot(fs_output)\n\n\n\n\n\n\n\nFigure 1: Plot of the AIC versus each step in the forward selection process.\n\n\n\n\n\nBackward elimination and stepwise regression operate in a similar manner. For backward elimination the function using AIC would be ols_step_backward_aic(). For stepwise regression the function would be ols_step_both_aic()."
  },
  {
    "objectID": "notes/06-01-automated-methods.html#all-subsets-regression",
    "href": "notes/06-01-automated-methods.html#all-subsets-regression",
    "title": "Automated Methods for Model Selection",
    "section": "All Subsets Regression",
    "text": "All Subsets Regression\nAll subsets regression fits all possible k-predictor models. The number of k-predictor models is:\n\\[\n\\mathrm{Number~of~Models} = 2^p - 1\n\\]\nwhere p is the number of predictors in the candidate set.\nThe ols_step_all_possible() function from the {olsrr} package can be used to exhaustively fit a set of models. The function takes an lm() object that includes all potential predictors.\n\n# Fit all subsets of predictors\nall_output = ols_step_all_possible(lm.all)\n\n# Output\nall_output\n\n\n\n   mindex n         predictors     rsquare         adjr     rmse      predrsq\n2       1 1             income 0.247434318  0.232383004 1.518791  0.115564209\n4       2 1             murder 0.144310731  0.127196945 1.619510 -0.158791388\n5       3 1            hs_grad 0.107015741  0.089156056 1.654427 -0.052459371\n1       4 1         population 0.049515821  0.030506137 1.706861 -0.016974597\n6       5 1              frost 0.027098563  0.007640534 1.726872 -0.064805099\n3       6 1         illiteracy 0.003990589 -0.015929599 1.747260 -0.095006514\n7       7 1               area 0.002815354 -0.017128339 1.748290 -0.060143568\n8       8 2  population income 0.289008906  0.259988862 1.476243  0.141506182\n15      9 2      income murder 0.274498977  0.244886691 1.491230 -0.197068157\n14     10 2  income illiteracy 0.252519569  0.222010163 1.513651  0.069707266\n18     11 2        income area 0.251337502  0.220779849 1.514847 -0.024356505\n16     12 2     income hs_grad 0.250603731  0.220016128 1.515589 -0.009461673\n17     13 2       income frost 0.247447777  0.216731360 1.518777  0.062237367\n11     14 2 population hs_grad 0.246290930  0.215527295 1.519944 -0.107847338\n10     15 2  population murder 0.203374938  0.170859629 1.562618 -0.169538055\n23     16 2     murder hs_grad 0.150824219  0.116163983 1.613335 -0.254046736\n25     17 2        murder area 0.150525243  0.115852804 1.613619 -0.213700252\n24     18 2       murder frost 0.144894337  0.109992065 1.618958 -0.207731316\n19     19 2  illiteracy murder 0.144568416  0.109652842 1.619267 -0.327331485\n12     20 2   population frost 0.129021244  0.093471091 1.633915 -0.016957037\n          cp      aic     sbic      sbc     msep      fpe       apc        hsp\n2   5.326316 197.0327 49.36238 202.8864 124.7515 2.491263 0.8127709 0.04895906\n4  12.633586 203.7105 55.53078 209.5642 141.8461 2.832639 0.9241444 0.05566789\n5  15.276285 205.9289 57.58295 211.7826 148.0284 2.956099 0.9644230 0.05809416\n1  19.350692 209.1738 60.58864 215.0276 157.5601 3.146445 1.0265229 0.06183489\n6  20.939164 210.3860 61.71282 216.2398 161.2761 3.220654 1.0507336 0.06329327\n3  22.576580 211.6067 62.84565 217.4604 165.1067 3.297149 1.0756902 0.06479659\n7  22.659856 211.6680 62.90258 217.5217 165.3015 3.301040 1.0769594 0.06487304\n8   4.380367 196.0776 48.69885 203.8826 120.3151 2.446145 0.7980512 0.04818165\n15  5.408531 197.1282 49.62732 204.9331 122.7705 2.496066 0.8143379 0.04916494\n14  6.965978 198.6801 51.00044 206.4851 126.4899 2.571686 0.8390086 0.05065441\n18  7.049738 198.7623 51.07319 206.5673 126.6899 2.575752 0.8403355 0.05073452\n16  7.101733 198.8132 51.11829 206.6182 126.8141 2.578277 0.8411591 0.05078424\n17  7.325362 199.0318 51.31181 206.8368 127.3482 2.589135 0.8447015 0.05099811\n11  7.407335 199.1117 51.38255 206.9166 127.5439 2.593115 0.8460000 0.05107651\n10 10.448335 201.9913 53.93644 209.7963 134.8063 2.740766 0.8941710 0.05398479\n23 14.172044 205.3132 56.89179 213.1181 143.6990 2.921566 0.9531565 0.05754599\n25 14.193230 205.3315 56.90811 213.1364 143.7496 2.922594 0.9534921 0.05756625\n24 14.592232 205.6750 57.21437 213.4800 144.7024 2.941967 0.9598125 0.05794784\n19 14.615326 205.6948 57.23204 213.4998 144.7576 2.943089 0.9601783 0.05796993\n12 15.716989 206.6314 58.06758 214.4364 147.3885 2.996578 0.9776292 0.05902351\n\n\nHere, although there are \\(2^7-1 = 127\\) different models included in the output, we only show the first 20. The actual output is in an object called result. (Calling the names() function on the all_output object returns the associated objects in the output.) We can operate on this to sort/arrange the models based on any of the included metrics, or to add new metrics.\n\n# Add AICc to results\nmodels = all_output$result |&gt;\n  mutate(\n    aic_c = aic + (2 * (n + 2) * (n + 3)) / (nrow(z_usa) - n - 3)\n    )\n\n\n# Order from smallest to largest AICc metric\n# Only show the best 20 models\nmodels |&gt;\n  arrange(aic_c) |&gt;\n  head(20)\n\n   mindex n                                  predictors   rsquare      adjr\n1      29 3                population income illiteracy 0.3494565 0.3087975\n2      64 4         population income illiteracy murder 0.3650011 0.3109586\n3      30 3                    population income murder 0.3219562 0.2795785\n4       8 2                           population income 0.2890089 0.2599889\n5      31 3                   population income hs_grad 0.3201656 0.2776759\n6      65 4        population income illiteracy hs_grad 0.3521897 0.2970569\n7      66 4          population income illiteracy frost 0.3511185 0.2958945\n8      67 4           population income illiteracy area 0.3496834 0.2943373\n9       1 1                                      income 0.2474343 0.2323830\n10      9 2                               income murder 0.2744990 0.2448867\n11     99 5   population income illiteracy murder frost 0.3728339 0.3046637\n12     32 3                     population income frost 0.3004649 0.2567440\n13    100 5 population income illiteracy murder hs_grad 0.3663040 0.2974240\n14    101 5    population income illiteracy murder area 0.3655789 0.2966201\n15     68 4            population income murder hs_grad 0.3277327 0.2705185\n16     33 3                      population income area 0.2899048 0.2455238\n17     69 4               population income murder area 0.3235424 0.2659715\n18     70 4              population income murder frost 0.3232112 0.2656121\n19    102 5  population income illiteracy hs_grad frost 0.3567520 0.2868337\n20     10 2                           income illiteracy 0.2525196 0.2220102\n       rmse     predrsq       cp      aic     sbic      sbc     msep      fpe\n1  1.412095  0.18140210 2.097091 193.4573 46.87937 203.2136 112.4283 2.326347\n2  1.395122 -0.21878421 2.995610 194.1997 48.13834 205.9072 112.1276 2.360486\n3  1.441633 -0.20799197 4.045741 195.6103 48.68559 205.3666 117.1810 2.424689\n4  1.476243  0.14150618 4.380367 196.0776 48.69885 203.8826 120.3151 2.446145\n5  1.443535 -0.08731959 4.172624 195.7475 48.80085 205.5037 117.4905 2.431092\n6  1.409125 -0.32689734 3.903416 195.2384 48.96165 206.9459 114.3898 2.408110\n7  1.410290  0.09436707 3.979323 195.3243 49.02986 207.0318 114.5790 2.412092\n8  1.411849  0.12287005 4.081014 195.4392 49.12107 207.1467 114.8324 2.417427\n9  1.518791  0.11556421 5.326316 197.0327 49.36238 202.8864 124.7515 2.491263\n10 1.491230 -0.19706816 5.408531 197.1282 49.62732 204.9331 122.7705 2.496066\n11 1.386491 -0.29086986 4.440581 195.5543 49.96805 209.2130 113.2055 2.423841\n12 1.464301  0.06437984 5.568600 197.2329 50.05093 206.9892 120.8952 2.501542\n13 1.393690 -0.63087784 4.903285 196.0929 50.36989 209.7516 114.3841 2.449078\n14 1.394487 -0.31988118 4.954666 196.1524 50.41431 209.8111 114.5150 2.451880\n15 1.435479 -0.32449344 5.636424 197.1654 50.49506 208.8729 118.7084 2.499024\n16 1.475312  0.06738309 6.316887 198.0121 50.70783 207.7683 122.7202 2.539305\n17 1.439945 -0.28300400 5.933347 197.4885 50.75294 209.1960 119.4483 2.514601\n18 1.440298 -0.28188046 5.956817 197.5140 50.77326 209.2215 119.5068 2.515832\n19 1.404155 -0.44044201 5.580135 196.8709 50.95178 210.5296 116.1083 2.485994\n20 1.513651  0.06970727 6.965978 198.6801 51.00044 206.4851 126.4899 2.571686\n         apc        hsp    aic_c\n1  0.7589674 0.04596127 194.7617\n2  0.7701051 0.04681360 196.0664\n3  0.7910511 0.04790418 196.9147\n4  0.7980512 0.04818165 196.9287\n5  0.7931401 0.04803069 197.0518\n6  0.7856423 0.04775809 197.1051\n7  0.7869414 0.04783706 197.1910\n8  0.7886819 0.04794286 197.3059\n9  0.8127709 0.04895906 197.5327\n10 0.8143379 0.04916494 197.9792\n11 0.7907746 0.04829109 198.0998\n12 0.8161242 0.04942255 198.5373\n13 0.7990080 0.04879388 198.6384\n14 0.7999222 0.04884972 198.6978\n15 0.8153029 0.04956112 199.0321\n16 0.8284444 0.05016863 199.3164\n17 0.8203848 0.04987004 199.3552\n18 0.8207864 0.04989445 199.3807\n19 0.8110518 0.04952938 199.4163\n20 0.8390086 0.05065441 199.5312\n\n\nThe model with the smallest AICc includes the predictors population, income, and illiteracy rate. However, there are several models that have an AICc value within 4 of the minimum AICc. All of these models are also plausible given the data and candidate set of 127 models.\n\n# Get models within 4 of minimum AICc\nmodels |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  filter(aic_c - min(aic_c) &lt; 4) |&gt;\n  arrange(aic_c)\n\n   mindex n                                  predictors    aic_c\n1      29 3                population income illiteracy 194.7617\n2      64 4         population income illiteracy murder 196.0664\n3      30 3                    population income murder 196.9147\n4       8 2                           population income 196.9287\n5      31 3                   population income hs_grad 197.0518\n6      65 4        population income illiteracy hs_grad 197.1051\n7      66 4          population income illiteracy frost 197.1910\n8      67 4           population income illiteracy area 197.3059\n9       1 1                                      income 197.5327\n10      9 2                               income murder 197.9792\n11     99 5   population income illiteracy murder frost 198.0998\n12     32 3                     population income frost 198.5373\n13    100 5 population income illiteracy murder hs_grad 198.6384\n14    101 5    population income illiteracy murder area 198.6978\n\n\nYou could also examine the best 1-predictor, 2-predictor, 3-predictor, etc. models.\n\n# Get best k-predictor models\nmodels |&gt;\n  group_by(n) |&gt;\n  filter(aic_c == min(aic_c)) |&gt;\n  ungroup() |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  arrange(aic_c)\n\n# A tibble: 7 × 4\n  mindex     n predictors                                             aic_c\n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n1     29     3 population income illiteracy                            195.\n2     64     4 population income illiteracy murder                     196.\n3      8     2 population income                                       197.\n4      1     1 income                                                  198.\n5     99     5 population income illiteracy murder frost               198.\n6    120     6 population income illiteracy murder frost area          200.\n7    127     7 population income illiteracy murder hs_grad frost area  203.\n\n\nIt can be useful to examine the predictors from the best models. (Here I do that in a plot, but it could also be done in a table.) This can help identify substantively important predictors.\n\n# Get models within 4 of minimum AICc\nplausible = models |&gt;\n  select(mindex, n, predictors, aic_c) |&gt;\n  filter(aic_c - min(aic_c) &lt; 4) |&gt;\n  arrange(aic_c)\n\n# Load library for labeling\nlibrary(ggrepel)\n\n# Plot the models\nggplot(data = plausible, aes(x = as.numeric(rownames(plausible)), y = aic_c)) +\n  geom_line(group = 1) +\n  geom_point() +\n  geom_label_repel(aes(label = predictors), size = 3) +\n  theme_bw() +\n  scale_x_continuous(name = \"Ten Best Models\", breaks = 1:14) +\n  ylab(\"AICc\")\n\n\n\n\n\n\n\nFigure 2: Fourteen plausible models and their predictors based on their AICc values.\n\n\n\n\n\nThe models with the lowest AICc values all seem to include population and income. Many also include illiteracy rate."
  },
  {
    "objectID": "notes/06-01-automated-methods.html#problems",
    "href": "notes/06-01-automated-methods.html#problems",
    "title": "Automated Methods for Model Selection",
    "section": "99 Problems…",
    "text": "99 Problems…\nAutomated methods of model selection has a host of critics and issues. In simulation studies, even when the true set of predictors are included in the subset of regression models, automated strategies may not identify these true predictors in the best models (Miller, 2002). The essential problems with automated selection methods have been summarized by Harrell (2001):\n\n\\(R^2\\) values are biased high;\nThe F-statistics are not F-distributed;\nThe standard errors of the parameter estimates are too small;\n\nConsequently, the confidence intervals around the parameter estimates are too narrow.\n\np-values are too low, due to multiple comparisons, and are diﬃcult to correct;\nParameter estimates are biased away from 0; and\nCollinearity problems are exacerbated.\n\nIn sum, the parameter estimates are likely to be too far away from zero; the variance estimates for those parameter estimates are not correct; which implies that the confidence intervals and hypothesis tests will be wrong; and there are no reasonable ways of correcting these problems! In general the evidence around using automated selection methods is that these methods are subpar. If you need to use these methods backward elimination seems to be the best method to use. (Stepwise regression consistently performs the worst.) Also, information criteria (AIC, BIC) seems to be the best criterion when using these methods.\n\nSUMMATION OF AUTOMATED METHODS\nFlom (2018) writes, “Most devastatingly, it allows the analyst not to think. Put in another way, for a data analyst to use stepwise methods is equivalent to telling his or her boss that his or her salary should be cut.”"
  },
  {
    "objectID": "notes/06-01-automated-methods.html#some-considerations-in-model-selection",
    "href": "notes/06-01-automated-methods.html#some-considerations-in-model-selection",
    "title": "Automated Methods for Model Selection",
    "section": "Some Considerations in Model Selection",
    "text": "Some Considerations in Model Selection\nDiﬀerent model strategies, metrics of model performance, and criteria for model selection lead to diﬀerent “final” models.\nMany statistical programs have functionality that can automate these fitting strategies. Before automating the selection process, it is important to understand the purpose of your model (is it to describe the data? make predictions? inference?). This often guides the choice of performance metrics and model building strategy.\nAlthough these packages can select a model based on some performance metric, there are several problems that automation does not solve:\n\nIt does not address the functional form of the predictors.\nIt does not address interactions.\nIt does not address outliers.\nIt does not address collinearity problems.\n\nMost software will require that you deal with these problems sequentially; first selecting the variables for the model and then determining their functional form, interactons, etc."
  }
]